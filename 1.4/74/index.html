
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
        <link rel="canonical" href="https://pytorch.apachecn.org/1.4/74/">
      
      
        <link rel="prev" href="../72/">
      
      
        <link rel="next" href="../75/">
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.5.3, mkdocs-material-9.5.0">
    
    
      
        <title>torch - 【布客】PyTorch 中文翻译</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.45e1311d.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#torch" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="【布客】PyTorch 中文翻译" class="md-header__button md-logo" aria-label="【布客】PyTorch 中文翻译" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            【布客】PyTorch 中文翻译
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              torch
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var media,input,key,value,palette=__md_get("__palette");if(palette&&palette.color){"(prefers-color-scheme)"===palette.color.media&&(media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']"),palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent"));for([key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/apachecn/pytorch-doc-zh" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    apachecn/pytorch-doc-zh
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="【布客】PyTorch 中文翻译" class="md-nav__button md-logo" aria-label="【布客】PyTorch 中文翻译" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    【布客】PyTorch 中文翻译
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/apachecn/pytorch-doc-zh" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    apachecn/pytorch-doc-zh
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    PyTorch 中文文档 & 教程
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    PyTorch 新特性
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            PyTorch 新特性
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../LatestChanges/PyTorch_V2.0/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    V2.0
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../LatestChanges/PyTorch_V1.13/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    V1.13
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../LatestChanges/PyTorch_V1.12/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    V1.12
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../LatestChanges/PyTorch_V1.11/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    V1.11
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../LatestChanges/PyTorch_V1.10/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    V1.10
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../LatestChanges/PyTorch_V1.9/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    V1.9
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../LatestChanges/PyTorch_V1.8/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    V1.8
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../LatestChanges/PyTorch_V1.7/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    V1.7
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../LatestChanges/PyTorch_V1.6/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    V1.6
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../LatestChanges/PyTorch_V1.5/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    V1.5
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../LatestChanges/PyTorch_V1.4/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    V1.4
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../LatestChanges/PyTorch_V1.3/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    V1.3
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../LatestChanges/PyTorch_V1.2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    V1.2
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    PyTorch 2.x 中文文档 & 教程
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            PyTorch 2.x 中文文档 & 教程
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1" >
        
          
          <label class="md-nav__link" for="__nav_3_1" id="__nav_3_1_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    中文教程
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_1">
            <span class="md-nav__icon md-icon"></span>
            中文教程
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1_1" >
        
          
          <label class="md-nav__link" for="__nav_3_1_1" id="__nav_3_1_1_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    PyTorch Recipes
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_1_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_1_1">
            <span class="md-nav__icon md-icon"></span>
            PyTorch Recipes
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/recipes/recipes_index/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    See All Recipes
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/prototype/prototype_index/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    See All Prototype Recipes
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1_2" >
        
          
          <label class="md-nav__link" for="__nav_3_1_2" id="__nav_3_1_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Introduction to PyTorch
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_1_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_1_2">
            <span class="md-nav__icon md-icon"></span>
            Introduction to PyTorch
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/beginner/basics/intro/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Learn the Basics
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/beginner/basics/quickstart_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Quickstart
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/beginner/basics/tensorqs_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Tensors
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/beginner/basics/data_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Datasets & DataLoaders
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/beginner/basics/transforms_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Transforms
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/beginner/basics/buildmodel_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Build the Neural Network
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/beginner/basics/autogradqs_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Automatic Differentiation with torch.autograd
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/beginner/basics/optimization_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Optimizing Model Parameters
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/beginner/basics/saveloadrun_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Save and Load the Model
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1_3" >
        
          
          <label class="md-nav__link" for="__nav_3_1_3" id="__nav_3_1_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Introduction to PyTorch on YouTube
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_1_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_1_3">
            <span class="md-nav__icon md-icon"></span>
            Introduction to PyTorch on YouTube
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/beginner/introyt/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Introduction to PyTorch - YouTube Series
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/beginner/introyt/introyt1_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Introduction to PyTorch
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/beginner/introyt/tensors_deeper_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Introduction to PyTorch Tensors
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/beginner/introyt/autogradyt_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    The Fundamentals of Autograd
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/beginner/introyt/modelsyt_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Building Models with PyTorch
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/beginner/introyt/tensorboardyt_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    PyTorch TensorBoard Support
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/beginner/introyt/trainingyt/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Training with PyTorch
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/beginner/introyt/captumyt/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Model Understanding with Captum
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1_4" >
        
          
          <label class="md-nav__link" for="__nav_3_1_4" id="__nav_3_1_4_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Learning PyTorch
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_1_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_1_4">
            <span class="md-nav__icon md-icon"></span>
            Learning PyTorch
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/beginner/deep_learning_60min_blitz/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Deep Learning with PyTorch: A 60 Minute Blitz
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/beginner/pytorch_with_examples/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Learning PyTorch with Examples
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/beginner/nn_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    What is torch.nn really?
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/intermediate/tensorboard_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Visualizing Models, Data, and Training with TensorBoard
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1_5" >
        
          
          <label class="md-nav__link" for="__nav_3_1_5" id="__nav_3_1_5_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Image and Video
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_1_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_1_5">
            <span class="md-nav__icon md-icon"></span>
            Image and Video
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/intermediate/torchvision_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    TorchVision Object Detection Finetuning Tutorial
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/beginner/transfer_learning_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Transfer Learning for Computer Vision Tutorial
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/beginner/fgsm_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Adversarial Example Generation
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/beginner/dcgan_faces_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    DCGAN Tutorial
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/intermediate/spatial_transformer_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Spatial Transformer Networks Tutorial
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/beginner/vt_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Optimizing Vision Transformer Model for Deployment
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1_6" >
        
          
          <label class="md-nav__link" for="__nav_3_1_6" id="__nav_3_1_6_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Audio
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_1_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_1_6">
            <span class="md-nav__icon md-icon"></span>
            Audio
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/beginner/audio_io_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Audio I/O
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/beginner/audio_resampling_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Audio Resampling
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/beginner/audio_data_augmentation_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Audio Data Augmentation
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/beginner/audio_feature_extractions_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Audio Feature Extractions
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/beginner/audio_feature_augmentation_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Audio Feature Augmentation
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/beginner/audio_datasets_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Audio Datasets
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/intermediate/speech_recognition_pipeline_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Speech Recognition with Wav2Vec2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/intermediate/text_to_speech_with_torchaudio/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Text-to-speech with Tacotron2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/intermediate/forced_alignment_with_torchaudio_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Forced Alignment with Wav2Vec2
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1_7" >
        
          
          <label class="md-nav__link" for="__nav_3_1_7" id="__nav_3_1_7_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Text
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_1_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_1_7">
            <span class="md-nav__icon md-icon"></span>
            Text
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/beginner/transformer_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Language Modeling with nn.Transformer and torchtext
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/beginner/bettertransformer_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Fast Transformer Inference with Better Transformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/intermediate/char_rnn_classification_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    NLP From Scratch: Classifying Names with a Character-Level RNN
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/intermediate/char_rnn_generation_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    NLP From Scratch: Generating Names with a Character-Level RNN
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/intermediate/seq2seq_translation_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    NLP From Scratch: Translation with a Sequence to Sequence Network and Attention
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/beginner/text_sentiment_ngrams_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Text classification with the torchtext library
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/beginner/translation_transformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Language Translation with nn.Transformer and torchtext
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/beginner/torchtext_custom_dataset_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Preprocess custom text dataset using Torchtext
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1_8" >
        
          
          <label class="md-nav__link" for="__nav_3_1_8" id="__nav_3_1_8_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Backends
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_1_8_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_1_8">
            <span class="md-nav__icon md-icon"></span>
            Backends
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/beginner/onnx/intro_onnx/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Introduction to ONNX
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1_9" >
        
          
          <label class="md-nav__link" for="__nav_3_1_9" id="__nav_3_1_9_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Reinforcement Learning
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_1_9_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_1_9">
            <span class="md-nav__icon md-icon"></span>
            Reinforcement Learning
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/intermediate/reinforcement_q_learning/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Reinforcement Learning (DQN) Tutorial
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/intermediate/reinforcement_ppo/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Reinforcement Learning (PPO) with TorchRL Tutorial
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/intermediate/mario_rl_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Train a Mario-playing RL Agent
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1_10" >
        
          
          <label class="md-nav__link" for="__nav_3_1_10" id="__nav_3_1_10_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Deploying PyTorch Models in Production
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_1_10_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_1_10">
            <span class="md-nav__icon md-icon"></span>
            Deploying PyTorch Models in Production
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/beginner/onnx/intro_onnx/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Introduction to ONNX
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/intermediate/flask_rest_api_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Deploying PyTorch in Python via a REST API with Flask
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/beginner/Intro_to_TorchScript_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Introduction to TorchScript
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/advanced/cpp_export/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Loading a TorchScript Model in C++
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/advanced/super_resolution_with_onnxruntime/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    (optional) Exporting a Model from PyTorch to ONNX and Running it using ONNX Runtime
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/intermediate/realtime_rpi/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Real Time Inference on Raspberry Pi 4 (30 fps!)
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1_11" >
        
          
          <label class="md-nav__link" for="__nav_3_1_11" id="__nav_3_1_11_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Code Transforms with FX
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_1_11_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_1_11">
            <span class="md-nav__icon md-icon"></span>
            Code Transforms with FX
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/intermediate/fx_conv_bn_fuser/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    (beta) Building a Convolution/Batch Norm fuser in FX
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/intermediate/fx_profiling_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    (beta) Building a Simple CPU Performance Profiler with FX
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1_12" >
        
          
          <label class="md-nav__link" for="__nav_3_1_12" id="__nav_3_1_12_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Frontend APIs
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_1_12_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_1_12">
            <span class="md-nav__icon md-icon"></span>
            Frontend APIs
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/intermediate/memory_format_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    (beta) Channels Last Memory Format in PyTorch
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/intermediate/forward_ad_usage/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Forward-mode Automatic Differentiation (Beta)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/intermediate/jacobians_hessians/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Jacobians, Hessians, hvp, vhp, and more: composing function transforms
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/intermediate/ensembling/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Model ensembling
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/intermediate/per_sample_grads/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Per-sample-gradients
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/advanced/cpp_frontend/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Using the PyTorch C++ Frontend
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/advanced/torch-script-parallelism/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Dynamic Parallelism in TorchScript
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/advanced/cpp_autograd/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Autograd in C++ Frontend
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1_13" >
        
          
          <label class="md-nav__link" for="__nav_3_1_13" id="__nav_3_1_13_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Extending PyTorch
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_1_13_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_1_13">
            <span class="md-nav__icon md-icon"></span>
            Extending PyTorch
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/intermediate/custom_function_double_backward_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Double Backward with Custom Functions
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/intermediate/custom_function_conv_bn_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Fusing Convolution and Batch Norm using Custom Function
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/advanced/cpp_extension/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Custom C++ and CUDA Extensions
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/advanced/torch_script_custom_ops/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Extending TorchScript with Custom C++ Operators
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/advanced/torch_script_custom_classes/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Extending TorchScript with Custom C++ Classes
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/advanced/dispatcher/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Registering a Dispatched Operator in C++
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/advanced/extend_dispatcher/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Extending dispatcher for a new backend in C++
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/advanced/privateuseone/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Facilitating New Backend Integration by PrivateUse1
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1_14" >
        
          
          <label class="md-nav__link" for="__nav_3_1_14" id="__nav_3_1_14_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Model Optimization
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_1_14_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_1_14">
            <span class="md-nav__icon md-icon"></span>
            Model Optimization
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/beginner/profiler/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Profiling your PyTorch Module
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/intermediate/tensorboard_profiler_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    PyTorch Profiler With TensorBoard
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/beginner/hyperparameter_tuning_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Hyperparameter tuning with Ray Tune
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/beginner/vt_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Optimizing Vision Transformer Model for Deployment
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/intermediate/parametrizations/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Parametrizations Tutorial
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/intermediate/pruning_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Pruning Tutorial
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/advanced/dynamic_quantization_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    (beta) Dynamic Quantization on an LSTM Word Language Model
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/intermediate/dynamic_quantization_bert_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    (beta) Dynamic Quantization on BERT
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/intermediate/quantized_transfer_learning_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    (beta) Quantized Transfer Learning for Computer Vision Tutorial
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/advanced/static_quantization_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    (beta) Static Quantization with Eager Mode in PyTorch
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/intermediate/torchserve_with_ipex/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Grokking PyTorch Intel CPU performance from first principles
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/intermediate/torchserve_with_ipex_2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Grokking PyTorch Intel CPU performance from first principles (Part 2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/intermediate/nvfuser_intro_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Getting Started - Accelerate Your Scripts with nvFuser
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/intermediate/ax_multiobjective_nas_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Multi-Objective NAS with Ax
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/intermediate/torch_compile_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    torch.compile Tutorial
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/intermediate/inductor_debug_cpu/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Inductor CPU backend debugging and profiling
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/intermediate/scaled_dot_product_attention_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    (Beta) Implementing High-Performance Transformers with Scaled Dot Product Attention (SDPA)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/intermediate/scaled_dot_product_attention_tutorial%23using-sdpa-with-torch-compile/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Using SDPA with torch.compile
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/intermediate/scaled_dot_product_attention_tutorial%23conclusion/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Conclusion
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/beginner/knowledge_distillation_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Knowledge Distillation Tutorial
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1_15" >
        
          
          <label class="md-nav__link" for="__nav_3_1_15" id="__nav_3_1_15_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Parallel and Distributed Training
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_1_15_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_1_15">
            <span class="md-nav__icon md-icon"></span>
            Parallel and Distributed Training
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/distributed/home/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Distributed and Parallel Training Tutorials
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/beginner/dist_overview/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    PyTorch Distributed Overview
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/beginner/ddp_series_intro/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Distributed Data Parallel in PyTorch - Video Tutorials
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/intermediate/model_parallel_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Single-Machine Model Parallel Best Practices
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/intermediate/ddp_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Getting Started with Distributed Data Parallel
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/intermediate/dist_tuto/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Writing Distributed Applications with PyTorch
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/intermediate/FSDP_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Getting Started with Fully Sharded Data Parallel(FSDP)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/intermediate/FSDP_adavnced_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Advanced Model Training with Fully Sharded Data Parallel (FSDP)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/intermediate/process_group_cpp_extension_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Customize Process Group Backends Using Cpp Extensions
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/intermediate/rpc_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Getting Started with Distributed RPC Framework
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/intermediate/rpc_param_server_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Implementing a Parameter Server Using Distributed RPC Framework
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/intermediate/dist_pipeline_parallel_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Distributed Pipeline Parallelism Using RPC
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/intermediate/rpc_async_execution/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Implementing Batch RPC Processing Using Asynchronous Executions
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/advanced/rpc_ddp_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Combining Distributed DataParallel with Distributed RPC Framework
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/intermediate/pipeline_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Training Transformer models using Pipeline Parallelism
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/advanced/ddp_pipeline/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Training Transformer models using Distributed Data Parallel and Pipeline Parallelism
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/advanced/generic_join/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Distributed Training with Uneven Inputs Using the Join Context Manager
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1_16" >
        
          
          <label class="md-nav__link" for="__nav_3_1_16" id="__nav_3_1_16_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Mobile
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_1_16_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_1_16">
            <span class="md-nav__icon md-icon"></span>
            Mobile
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/beginner/deeplabv3_on_ios/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Image Segmentation DeepLabV3 on iOS
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/beginner/deeplabv3_on_android/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Image Segmentation DeepLabV3 on Android
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1_17" >
        
          
          <label class="md-nav__link" for="__nav_3_1_17" id="__nav_3_1_17_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Recommendation Systems
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_1_17_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_1_17">
            <span class="md-nav__icon md-icon"></span>
            Recommendation Systems
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/intermediate/torchrec_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Introduction to TorchRec
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/advanced/sharding/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Exploring TorchRec sharding
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1_18" >
        
          
          <label class="md-nav__link" for="__nav_3_1_18" id="__nav_3_1_18_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Multimodality
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_1_18_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_1_18">
            <span class="md-nav__icon md-icon"></span>
            Multimodality
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/beginner/flava_finetuning_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    TorchMultimodal Tutorial: Finetuning FLAVA
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_2" >
        
          
          <label class="md-nav__link" for="__nav_3_2" id="__nav_3_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    中文文档
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_2">
            <span class="md-nav__icon md-icon"></span>
            中文文档
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/docs/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    介绍
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/docs/cuda/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    torch.cuda
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    PyTorch 1.7 中文文档
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            PyTorch 1.7 中文文档
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_1" >
        
          
          <label class="md-nav__link" for="__nav_4_1" id="__nav_4_1_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    学习 PyTorch
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4_1">
            <span class="md-nav__icon md-icon"></span>
            学习 PyTorch
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_1_1" >
        
          
          <label class="md-nav__link" for="__nav_4_1_1" id="__nav_4_1_1_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    PyTorch 深度学习：60 分钟的突击
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_4_1_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4_1_1">
            <span class="md-nav__icon md-icon"></span>
            PyTorch 深度学习：60 分钟的突击
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/02/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    介绍
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/03/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    张量
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/04/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    torch.autograd的简要介绍
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/05/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    神经网络
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/06/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    训练分类器
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_1_2" >
        
          
          <label class="md-nav__link" for="__nav_4_1_2" id="__nav_4_1_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    通过示例学习 PyTorch
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_4_1_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4_1_2">
            <span class="md-nav__icon md-icon"></span>
            通过示例学习 PyTorch
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/07/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    介绍
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/08/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    热身：NumPy
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/09/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    PyTorch：张量
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/10/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    PyTorch：张量和 Autograd
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/11/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    PyTorch：定义新的 Autograd 函数
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/12/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    PyTorch：nn
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/13/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    PyTorch：optim
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/14/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    PyTorch：自定义nn模块
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/15/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    PyTorch：控制流 - 权重共享
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/16/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    torch.nn到底是什么？
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/17/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    使用 TensorBoard 可视化模型，数据和训练
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_2" >
        
          
          <label class="md-nav__link" for="__nav_4_2" id="__nav_4_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    图片/视频
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4_2">
            <span class="md-nav__icon md-icon"></span>
            图片/视频
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/19/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    torchvision对象检测微调教程
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/20/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    计算机视觉的迁移学习教程
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/21/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    对抗示例生成
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/22/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    DCGAN 教程
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_3" >
        
          
          <label class="md-nav__link" for="__nav_4_3" id="__nav_4_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    音频
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4_3">
            <span class="md-nav__icon md-icon"></span>
            音频
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/24/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    音频 I/O 和torchaudio的预处理
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/25/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    使用torchaudio的语音命令识别
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_4" >
        
          
          <label class="md-nav__link" for="__nav_4_4" id="__nav_4_4_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    文本
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4_4">
            <span class="md-nav__icon md-icon"></span>
            文本
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/27/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    使用nn.Transformer和torchtext的序列到序列建模
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/28/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    从零开始的 NLP：使用字符级 RNN 分类名称
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/29/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    从零开始的 NLP：使用字符级 RNN 生成名称
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/30/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    从零开始的 NLP：使用序列到序列网络和注意力的翻译
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/31/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    使用torchtext的文本分类
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/32/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    torchtext语言翻译
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_5" >
        
          
          <label class="md-nav__link" for="__nav_4_5" id="__nav_4_5_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    强化学习
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4_5">
            <span class="md-nav__icon md-icon"></span>
            强化学习
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/34/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    强化学习（DQN）教程
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/35/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    训练玩马里奥的 RL 智能体
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_6" >
        
          
          <label class="md-nav__link" for="__nav_4_6" id="__nav_4_6_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    在生产中部署 PyTorch 模型
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4_6">
            <span class="md-nav__icon md-icon"></span>
            在生产中部署 PyTorch 模型
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/37/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    通过使用 Flask 的 REST API 在 Python 中部署 PyTorch
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/38/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    TorchScript 简介
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/39/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    在 C++ 中加载 TorchScript 模型
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/40/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    将模型从 PyTorch 导出到 ONNX 并使用 ONNX 运行时运行它（可选）
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_7" >
        
          
          <label class="md-nav__link" for="__nav_4_7" id="__nav_4_7_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    前端 API
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4_7">
            <span class="md-nav__icon md-icon"></span>
            前端 API
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/42/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    PyTorch 中的命名张量简介（原型）
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/43/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    PyTorch 中通道在最后的内存格式（beta）
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/44/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    使用 PyTorch C++ 前端
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/45/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    自定义 C++ 和 CUDA 扩展
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/46/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    使用自定义 C++ 运算符扩展 TorchScript
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/47/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    使用自定义 C++ 类扩展 TorchScript
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/48/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    TorchScript 中的动态并行性
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/49/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    C++ 前端中的 Autograd
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/50/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    在 C++ 中注册调度运算符
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_8" >
        
          
          <label class="md-nav__link" for="__nav_4_8" id="__nav_4_8_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    模型优化
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_8_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4_8">
            <span class="md-nav__icon md-icon"></span>
            模型优化
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/52/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    分析您的 PyTorch 模块
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/53/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    使用 Ray Tune 的超参数调整
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/54/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    模型剪裁教程
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/55/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    LSTM 单词语言模型上的动态量化（beta）
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/56/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    BERT 上的动态量化（Beta）
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/57/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    PyTorch 中使用 Eager 模式的静态量化（beta）
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/58/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    计算机视觉的量化迁移学习教程（beta）
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_9" >
        
          
          <label class="md-nav__link" for="__nav_4_9" id="__nav_4_9_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    并行和分布式训练
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_9_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4_9">
            <span class="md-nav__icon md-icon"></span>
            并行和分布式训练
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/60/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    PyTorch 分布式概述
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/61/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    单机模型并行最佳实践
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/62/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    分布式数据并行入门
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/63/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    用 PyTorch 编写分布式应用
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/64/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    分布式 RPC 框架入门
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/65/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    使用分布式 RPC 框架实现参数服务器
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/66/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    使用 RPC 的分布式管道并行化
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/67/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    使用异步执行实现批量 RPC 处理
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/68/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    将分布式DataParallel与分布式 RPC 框架相结合
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" checked>
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    PyTorch 1.4 中文文档 & 教程
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            PyTorch 1.4 中文文档 & 教程
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_1" >
        
          
          <label class="md-nav__link" for="__nav_5_1" id="__nav_5_1_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    入门
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_1">
            <span class="md-nav__icon md-icon"></span>
            入门
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_1_1" >
        
          
          <label class="md-nav__link" for="__nav_5_1_1" id="__nav_5_1_1_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    使用 PyTorch 进行深度学习：60 分钟的闪电战
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_1_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_1_1">
            <span class="md-nav__icon md-icon"></span>
            使用 PyTorch 进行深度学习：60 分钟的闪电战
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../4/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    介绍
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../blitz/tensor_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    什么是PyTorch
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../blitz/autograd_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Autograd：自动求导
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../blitz/neural_networks_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    神经网络
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../blitz/cifar10_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    训练分类器
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../blitz/data_parallel_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    可选：数据并行
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../5/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    编写自定义数据集，数据加载器和转换
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../6/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    使用 TensorBoard 可视化模型，数据和训练
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_2" >
        
          
          <label class="md-nav__link" for="__nav_5_2" id="__nav_5_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    图片
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_2">
            <span class="md-nav__icon md-icon"></span>
            图片
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../8/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    TorchVision 对象检测微调教程
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../9/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    转移学习的计算机视觉教程
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../10/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    空间变压器网络教程
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../11/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    使用 PyTorch 进行神经传递
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../12/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    对抗示例生成
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../13/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    DCGAN 教程
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_3" >
        
          
          <label class="md-nav__link" for="__nav_5_3" id="__nav_5_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    音频
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_3">
            <span class="md-nav__icon md-icon"></span>
            音频
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../15/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    torchaudio 教程
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_4" >
        
          
          <label class="md-nav__link" for="__nav_5_4" id="__nav_5_4_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    文本
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_4">
            <span class="md-nav__icon md-icon"></span>
            文本
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../17/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    NLP From Scratch: 使用char-RNN对姓氏进行分类
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../18/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    NLP From Scratch: 生成名称与字符级RNN
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../19/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    NLP From Scratch: 基于注意力机制的 seq2seq 神经网络翻译
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../20/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    使用 TorchText 进行文本分类
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../21/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    使用 TorchText 进行语言翻译
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../22/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    使用 nn.Transformer 和 TorchText 进行序列到序列建模
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_5" >
        
          
          <label class="md-nav__link" for="__nav_5_5" id="__nav_5_5_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    命名为 Tensor(实验性）
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_5">
            <span class="md-nav__icon md-icon"></span>
            命名为 Tensor(实验性）
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../24/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    (实验性)PyTorch 中的命名张量简介
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_6" >
        
          
          <label class="md-nav__link" for="__nav_5_6" id="__nav_5_6_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    强化学习
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_6">
            <span class="md-nav__icon md-icon"></span>
            强化学习
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../26/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    强化学习(DQN)教程
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_7" >
        
          
          <label class="md-nav__link" for="__nav_5_7" id="__nav_5_7_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    在生产中部署 PyTorch 模型
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_7">
            <span class="md-nav__icon md-icon"></span>
            在生产中部署 PyTorch 模型
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../28/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    通过带有 Flask 的 REST API 在 Python 中部署 PyTorch
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../29/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    TorchScript 简介
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../30/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    在 C --中加载 TorchScript 模型
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../31/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    (可选）将模型从 PyTorch 导出到 ONNX 并使用 ONNX Runtime 运行
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_8" >
        
          
          <label class="md-nav__link" for="__nav_5_8" id="__nav_5_8_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    并行和分布式训练
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_8_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_8">
            <span class="md-nav__icon md-icon"></span>
            并行和分布式训练
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../33/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    单机模型并行最佳实践
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../34/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    分布式数据并行入门
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../35/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    用 PyTorch 编写分布式应用程序
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../36/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    分布式 RPC 框架入门
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../37/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    (高级）带有 Amazon AWS 的 PyTorch 1.0 分布式训练师
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_9" >
        
          
          <label class="md-nav__link" for="__nav_5_9" id="__nav_5_9_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    扩展 PyTorch
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_9_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_9">
            <span class="md-nav__icon md-icon"></span>
            扩展 PyTorch
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../39/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    使用自定义 C --运算符扩展 TorchScript
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../40/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    使用自定义 C --类扩展 TorchScript
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../41/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    使用 numpy 和 scipy 创建扩展
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../42/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    自定义 C --和 CUDA 扩展
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_10" >
        
          
          <label class="md-nav__link" for="__nav_5_10" id="__nav_5_10_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    模型优化
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_10_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_10">
            <span class="md-nav__icon md-icon"></span>
            模型优化
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../44/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    LSTM Word 语言模型上的(实验）动态量化
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../45/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    (实验性）在 PyTorch 中使用 Eager 模式进行静态量化
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../46/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    (实验性）计算机视觉教程的量化转移学习
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../47/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    (实验）BERT 上的动态量化
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../48/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    修剪教程
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_11" >
        
          
          <label class="md-nav__link" for="__nav_5_11" id="__nav_5_11_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    PyTorch 用其他语言
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_11_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_11">
            <span class="md-nav__icon md-icon"></span>
            PyTorch 用其他语言
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../50/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    使用 PyTorch C --前端
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_12" >
        
          
          <label class="md-nav__link" for="__nav_5_12" id="__nav_5_12_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    PyTorch 基础知识
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_12_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_12">
            <span class="md-nav__icon md-icon"></span>
            PyTorch 基础知识
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../52/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    通过示例学习 PyTorch
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../53/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    torch.nn 到底是什么？
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_13" >
        
          
          <label class="md-nav__link" for="__nav_5_13" id="__nav_5_13_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    笔记
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_13_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_13">
            <span class="md-nav__icon md-icon"></span>
            笔记
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../56/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    自动求导机制
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../57/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    广播语义
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../58/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    CPU 线程和 TorchScript 推断
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../59/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    CUDA 语义
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../60/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    分布式 Autograd 设计
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../61/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    扩展 PyTorch
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../62/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    经常问的问题
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../63/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    大规模部署的功能
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../64/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    并行处理最佳实践
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../65/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    重现性
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../66/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    远程参考协议
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../67/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    序列化语义
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../68/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Windows 常见问题
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../69/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    XLA 设备上的 PyTorch
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_14" >
        
          
          <label class="md-nav__link" for="__nav_5_14" id="__nav_5_14_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    语言绑定
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_14_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_14">
            <span class="md-nav__icon md-icon"></span>
            语言绑定
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../71/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    PyTorch C -- API
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../72/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    PyTorch Java API
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
    
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_15" checked>
        
          
          <label class="md-nav__link" for="__nav_5_15" id="__nav_5_15_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Python API
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_15_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_5_15">
            <span class="md-nav__icon md-icon"></span>
            Python API
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    torch
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    torch
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#_1" class="md-nav__link">
    <span class="md-ellipsis">
      张量
    </span>
  </a>
  
    <nav class="md-nav" aria-label="张量">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_2" class="md-nav__link">
    <span class="md-ellipsis">
      创作行动
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_3" class="md-nav__link">
    <span class="md-ellipsis">
      索引，切片，联接，操作变更
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_4" class="md-nav__link">
    <span class="md-ellipsis">
      发电机
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_5" class="md-nav__link">
    <span class="md-ellipsis">
      随机抽样
    </span>
  </a>
  
    <nav class="md-nav" aria-label="随机抽样">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_6" class="md-nav__link">
    <span class="md-ellipsis">
      就地随机抽样
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_7" class="md-nav__link">
    <span class="md-ellipsis">
      准随机抽样
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_8" class="md-nav__link">
    <span class="md-ellipsis">
      序列化
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_9" class="md-nav__link">
    <span class="md-ellipsis">
      平行性
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_10" class="md-nav__link">
    <span class="md-ellipsis">
      局部禁用梯度计算
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_11" class="md-nav__link">
    <span class="md-ellipsis">
      数学运算
    </span>
  </a>
  
    <nav class="md-nav" aria-label="数学运算">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_12" class="md-nav__link">
    <span class="md-ellipsis">
      逐点操作
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_13" class="md-nav__link">
    <span class="md-ellipsis">
      减少操作
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_14" class="md-nav__link">
    <span class="md-ellipsis">
      比较行动
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_15" class="md-nav__link">
    <span class="md-ellipsis">
      光谱操作
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_16" class="md-nav__link">
    <span class="md-ellipsis">
      其他作业
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#blas-lapack" class="md-nav__link">
    <span class="md-ellipsis">
      BLAS 和 LAPACK 操作
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_17" class="md-nav__link">
    <span class="md-ellipsis">
      实用工具
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../75/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    torch.nn
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../76/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    torch功能
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../77/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    torch张量
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../78/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    张量属性
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../79/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    自动差分包-Torch.Autograd
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../80/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    torch.cuda
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../81/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    分布式通讯包-Torch.Distributed
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../82/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    概率分布-torch分布
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../83/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    torch.hub
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../84/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    torch脚本
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../85/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    torch.nn.init
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../86/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    torch.onnx
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../87/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    torch.optim
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../88/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    量化
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../89/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    分布式 RPC 框架
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../90/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    torch随机
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../91/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    torch稀疏
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../92/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    torch存储
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../93/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    torch.utils.bottleneck
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../94/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    torch.utils.checkpoint
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../95/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    torch.utils.cpp_extension
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../96/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    torch.utils.data
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../97/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    torch.utils.dlpack
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../98/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    torch.utils.model_zoo
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../99/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    torch.utils.tensorboard
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../100/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    类型信息
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../101/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    命名张量
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../102/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    命名为 Tensors 操作员范围
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../103/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    糟糕！
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_16" >
        
          
          <label class="md-nav__link" for="__nav_5_16" id="__nav_5_16_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    torchvision参考
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_16_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_16">
            <span class="md-nav__icon md-icon"></span>
            torchvision参考
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../105/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    torchvision
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_17" >
        
          
          <label class="md-nav__link" for="__nav_5_17" id="__nav_5_17_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    音频参考
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_17_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_17">
            <span class="md-nav__icon md-icon"></span>
            音频参考
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../107/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    torchaudio
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_18" >
        
          
          <label class="md-nav__link" for="__nav_5_18" id="__nav_5_18_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    torchtext参考
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_18_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_18">
            <span class="md-nav__icon md-icon"></span>
            torchtext参考
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../109/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    torchtext
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_19" >
        
          
          <label class="md-nav__link" for="__nav_5_19" id="__nav_5_19_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    社区
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_19_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_19">
            <span class="md-nav__icon md-icon"></span>
            社区
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../111/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    PyTorch 贡献指南
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../112/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    PyTorch 治理
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../113/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    PyTorch 治理| 感兴趣的人
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" >
        
          
          <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    PyTorch 1.0 中文文档 & 教程
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            PyTorch 1.0 中文文档 & 教程
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    目录
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6_2" >
        
          
          <label class="md-nav__link" for="__nav_6_2" id="__nav_6_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    中文教程
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_6_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6_2">
            <span class="md-nav__icon md-icon"></span>
            中文教程
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6_2_1" >
        
          
          <label class="md-nav__link" for="__nav_6_2_1" id="__nav_6_2_1_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    入门
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_6_2_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6_2_1">
            <span class="md-nav__icon md-icon"></span>
            入门
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6_2_1_1" >
        
          
          <label class="md-nav__link" for="__nav_6_2_1_1" id="__nav_6_2_1_1_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    PyTorch 深度学习: 60 分钟极速入门
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="4" aria-labelledby="__nav_6_2_1_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6_2_1_1">
            <span class="md-nav__icon md-icon"></span>
            PyTorch 深度学习: 60 分钟极速入门
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/deep_learning_60min_blitz/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    介绍
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/blitz_tensor_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    什么是 PyTorch？
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/blitz_autograd_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Autograd：自动求导
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/blitz_neural_networks_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    神经网络
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/blitz_cifar10_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    训练分类器
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/blitz_data_parallel_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    可选：数据并行处理
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/data_loading_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    数据加载和处理教程
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/pytorch_with_examples/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    用例子学习 PyTorch
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/transfer_learning_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    迁移学习教程
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/deploy_seq2seq_hybrid_frontend_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    混合前端的 seq2seq 模型部署
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/saving_loading_models/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Saving and Loading Models
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/nn_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    What is torch.nn really?
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6_2_2" >
        
          
          <label class="md-nav__link" for="__nav_6_2_2" id="__nav_6_2_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    图像
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_6_2_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6_2_2">
            <span class="md-nav__icon md-icon"></span>
            图像
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/finetuning_torchvision_models_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Torchvision 模型微调
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/spatial_transformer_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    空间变换器网络教程
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/neural_style_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    使用 PyTorch 进行图像风格转换
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/fgsm_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    对抗性示例生成
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/super_resolution_with_caffe2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    使用 ONNX 将模型从 PyTorch 传输到 Caffe2 和移动端
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6_2_3" >
        
          
          <label class="md-nav__link" for="__nav_6_2_3" id="__nav_6_2_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    文本
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_6_2_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6_2_3">
            <span class="md-nav__icon md-icon"></span>
            文本
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/chatbot_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    聊天机器人教程
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/char_rnn_generation_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    使用字符级别特征的 RNN 网络生成姓氏
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/char_rnn_classification_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    使用字符级别特征的 RNN 网络进行姓氏分类
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6_2_3_4" >
        
          
          <label class="md-nav__link" for="__nav_6_2_3_4" id="__nav_6_2_3_4_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Deep Learning for NLP with Pytorch
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="4" aria-labelledby="__nav_6_2_3_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6_2_3_4">
            <span class="md-nav__icon md-icon"></span>
            Deep Learning for NLP with Pytorch
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/deep_learning_nlp_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    在深度学习和 NLP 中使用 Pytorch
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/nlp_pytorch_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    PyTorch 介绍
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/nlp_deep_learning_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    使用 PyTorch 进行深度学习
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/nlp_word_embeddings_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Word Embeddings: Encoding Lexical Semantics
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/nlp_sequence_models_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    序列模型和 LSTM 网络
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/nlp_advanced_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Advanced: Making Dynamic Decisions and the Bi-LSTM CRF
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/seq2seq_translation_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    基于注意力机制的 seq2seq 神经网络翻译
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6_2_4" >
        
          
          <label class="md-nav__link" for="__nav_6_2_4" id="__nav_6_2_4_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    生成
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_6_2_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6_2_4">
            <span class="md-nav__icon md-icon"></span>
            生成
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/dcgan_faces_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    DCGAN Tutorial
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6_2_5" >
        
          
          <label class="md-nav__link" for="__nav_6_2_5" id="__nav_6_2_5_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    强化学习
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_6_2_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6_2_5">
            <span class="md-nav__icon md-icon"></span>
            强化学习
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/reinforcement_q_learning/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Reinforcement Learning (DQN) Tutorial
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6_2_6" >
        
          
          <label class="md-nav__link" for="__nav_6_2_6" id="__nav_6_2_6_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    扩展 PyTorch
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_6_2_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6_2_6">
            <span class="md-nav__icon md-icon"></span>
            扩展 PyTorch
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/numpy_extensions_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    用 numpy 和 scipy 创建扩展
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/cpp_extension/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Custom C++ and CUDA Extensions
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/torch_script_custom_ops/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Extending TorchScript with Custom C++ Operators
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6_2_7" >
        
          
          <label class="md-nav__link" for="__nav_6_2_7" id="__nav_6_2_7_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    生产性使用
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_6_2_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6_2_7">
            <span class="md-nav__icon md-icon"></span>
            生产性使用
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/dist_tuto/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Writing Distributed Applications with PyTorch
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/aws_distributed_training_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    使用 Amazon AWS 进行分布式训练
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/ONNXLive/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    ONNX 现场演示教程
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/cpp_export/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    在 C++ 中加载 PYTORCH 模型
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6_2_8" >
        
          
          <label class="md-nav__link" for="__nav_6_2_8" id="__nav_6_2_8_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    其它语言中的 PyTorch
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_6_2_8_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6_2_8">
            <span class="md-nav__icon md-icon"></span>
            其它语言中的 PyTorch
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/cpp_frontend/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    使用 PyTorch C++ 前端
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6_3" >
        
          
          <label class="md-nav__link" for="__nav_6_3" id="__nav_6_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    中文文档
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_6_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6_3">
            <span class="md-nav__icon md-icon"></span>
            中文文档
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6_3_1" >
        
          
          <label class="md-nav__link" for="__nav_6_3_1" id="__nav_6_3_1_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    注解
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_6_3_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6_3_1">
            <span class="md-nav__icon md-icon"></span>
            注解
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/notes_autograd/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    自动求导机制
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/notes_broadcasting/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    广播语义
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/notes_cuda/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    CUDA 语义
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/notes_extending/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Extending PyTorch
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/notes_faq/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Frequently Asked Questions
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/notes_multiprocessing/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Multiprocessing best practices
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/notes_randomness/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Reproducibility
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/notes_serialization/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Serialization semantics
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/notes_windows/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Windows FAQ
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6_3_2" >
        
          
          <label class="md-nav__link" for="__nav_6_3_2" id="__nav_6_3_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    包参考
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_6_3_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6_3_2">
            <span class="md-nav__icon md-icon"></span>
            包参考
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6_3_2_1" >
        
          
          <label class="md-nav__link" for="__nav_6_3_2_1" id="__nav_6_3_2_1_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    torch
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="4" aria-labelledby="__nav_6_3_2_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6_3_2_1">
            <span class="md-nav__icon md-icon"></span>
            torch
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/torch/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    介绍
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/torch_tensors/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Tensors
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/torch_random_sampling/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Random sampling
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/torch_serialization_parallelism_utilities/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Serialization, Parallelism, Utilities
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6_3_2_1_5" >
        
          
          <label class="md-nav__link" for="__nav_6_3_2_1_5" id="__nav_6_3_2_1_5_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Math operations
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="5" aria-labelledby="__nav_6_3_2_1_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6_3_2_1_5">
            <span class="md-nav__icon md-icon"></span>
            Math operations
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/torch_math_operations_pointwise_ops/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Pointwise Ops
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/torch_math_operations_reduction_ops/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Reduction Ops
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/torch_math_operations_comparison_ops/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Comparison Ops
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/torch_math_operations_spectral_ops/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Spectral Ops
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/torch_math_operations_other_ops/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Other Operations
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/torch_math_operations_blas_lapack_ops/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    BLAS and LAPACK Operations
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/tensors/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    torch.Tensor
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/tensor_attributes/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Tensor Attributes
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/type_info/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    数据类型信息
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/sparse/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    torch.sparse
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/cuda/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    torch.cuda
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/storage/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    torch.Storage
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/nn/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    torch.nn
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/nn_functional/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    torch.nn.functional
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/nn_init/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    torch.nn.init
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/optim/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    torch.optim
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/autograd/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Automatic differentiation package - torch.autograd
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/distributed/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Distributed communication package - torch.distributed
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/distributions/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Probability distributions - torch.distributions
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/jit/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Torch Script
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/multiprocessing/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    多进程包 - torch.multiprocessing
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/bottleneck/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    torch.utils.bottleneck
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/checkpoint/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    torch.utils.checkpoint
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/docs_cpp_extension/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    torch.utils.cpp_extension
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/data/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    torch.utils.data
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/dlpack/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    torch.utils.dlpack
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/hub/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    torch.hub
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/model_zoo/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    torch.utils.model_zoo
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/onnx/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    torch.onnx
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/distributed_deprecated/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Distributed communication package (deprecated) - torch.distributed.deprecated
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6_3_3" >
        
          
          <label class="md-nav__link" for="__nav_6_3_3" id="__nav_6_3_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    torchvision 参考
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_6_3_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6_3_3">
            <span class="md-nav__icon md-icon"></span>
            torchvision 参考
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/docs_torchvision_ref/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    目录
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/torchvision_datasets/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    torchvision.datasets
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/torchvision_models/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    torchvision.models
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/torchvision_transforms/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    torchvision.transforms
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/torchvision_utils/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    torchvision.utils
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="https://pytorch0x.apachecn.org" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    PyTorch 0.4 中文文档
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="https://pytorch0x.apachecn.org" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    PyTorch 0.3 中文文档 & 教程
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="https://pytorch0x.apachecn.org" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    PyTorch 0.2 中文文档
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../contrib/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    贡献指南
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="https://www.apachecn.org/about" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    关于我们
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="https://www.apachecn.org/join" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    加入我们
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="https://docs.apachecn.org" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    中文资源合集
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#_1" class="md-nav__link">
    <span class="md-ellipsis">
      张量
    </span>
  </a>
  
    <nav class="md-nav" aria-label="张量">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_2" class="md-nav__link">
    <span class="md-ellipsis">
      创作行动
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_3" class="md-nav__link">
    <span class="md-ellipsis">
      索引，切片，联接，操作变更
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_4" class="md-nav__link">
    <span class="md-ellipsis">
      发电机
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_5" class="md-nav__link">
    <span class="md-ellipsis">
      随机抽样
    </span>
  </a>
  
    <nav class="md-nav" aria-label="随机抽样">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_6" class="md-nav__link">
    <span class="md-ellipsis">
      就地随机抽样
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_7" class="md-nav__link">
    <span class="md-ellipsis">
      准随机抽样
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_8" class="md-nav__link">
    <span class="md-ellipsis">
      序列化
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_9" class="md-nav__link">
    <span class="md-ellipsis">
      平行性
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_10" class="md-nav__link">
    <span class="md-ellipsis">
      局部禁用梯度计算
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_11" class="md-nav__link">
    <span class="md-ellipsis">
      数学运算
    </span>
  </a>
  
    <nav class="md-nav" aria-label="数学运算">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_12" class="md-nav__link">
    <span class="md-ellipsis">
      逐点操作
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_13" class="md-nav__link">
    <span class="md-ellipsis">
      减少操作
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_14" class="md-nav__link">
    <span class="md-ellipsis">
      比较行动
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_15" class="md-nav__link">
    <span class="md-ellipsis">
      光谱操作
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_16" class="md-nav__link">
    <span class="md-ellipsis">
      其他作业
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#blas-lapack" class="md-nav__link">
    <span class="md-ellipsis">
      BLAS 和 LAPACK 操作
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_17" class="md-nav__link">
    <span class="md-ellipsis">
      实用工具
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  
    <a href="https://github.com/apachecn/pytorch-doc-zh/edit/master/docs/1.4/74.md" title="Edit this page" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4v-2m10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1 2.1 2.1Z"/></svg>
    </a>
  
  
    
      
    
    <a href="https://github.com/apachecn/pytorch-doc-zh/raw/master/docs/1.4/74.md" title="View source of this page" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 18c.56 0 1 .44 1 1s-.44 1-1 1-1-.44-1-1 .44-1 1-1m0-3c-2.73 0-5.06 1.66-6 4 .94 2.34 3.27 4 6 4s5.06-1.66 6-4c-.94-2.34-3.27-4-6-4m0 6.5a2.5 2.5 0 0 1-2.5-2.5 2.5 2.5 0 0 1 2.5-2.5 2.5 2.5 0 0 1 2.5 2.5 2.5 2.5 0 0 1-2.5 2.5M9.27 20H6V4h7v5h5v4.07c.7.08 1.36.25 2 .49V8l-6-6H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h4.5a8.15 8.15 0 0 1-1.23-2Z"/></svg>
    </a>
  


<h1 id="torch">torch</h1>
<blockquote>
<p>原文： <a href="https://pytorch.org/docs/stable/torch.html">https://pytorch.org/docs/stable/torch.html</a></p>
</blockquote>
<p>torch程序包包含多维张量的数据结构，并定义了这些数据的数学运算。 此外，它提供了许多实用程序，可用于有效地序列化张量和任意类型，以及其他有用的实用程序。</p>
<p>它具有 CUDA 对应项，使您能够在具有计算能力&gt; = 3.0 的 NVIDIA GPU 上运行张量计算。</p>
<h2 id="_1">张量</h2>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a>torch.is_tensor(obj)¶
</code></pre></div>
<p>如果 &lt;cite&gt;obj&lt;/cite&gt; 是 PyTorch 张量，则返回 True。</p>
<p>参数</p>
<p><strong>obj</strong> (<em>对象</em>）–要测试的对象</p>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-1-1" name="__codelineno-1-1" href="#__codelineno-1-1"></a>torch.is_storage(obj)¶
</code></pre></div>
<p>如果 &lt;cite&gt;obj&lt;/cite&gt; 是 PyTorch 存储对象，则返回 True。</p>
<p>Parameters</p>
<p><strong>obj</strong> (<em>Object</em>) – Object to test</p>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-2-1" name="__codelineno-2-1" href="#__codelineno-2-1"></a>torch.is_floating_point(input) -&gt; (bool)¶
</code></pre></div>
<p>如果<code>input</code>的数据类型是浮点数据类型，即<code>torch.float64</code>，<code>torch.float32</code>和<code>torch.float16</code>之一，则返回 True。</p>
<p>Parameters</p>
<p><strong>输入</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–要测试的 PyTorch 张量</p>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-3-1" name="__codelineno-3-1" href="#__codelineno-3-1"></a>torch.set_default_dtype(d)¶
</code></pre></div>
<p>将默认浮点 dtype 设置为<code>d</code>。 该类型将用作 <a href="#torch.tensor" title="torch.tensor"><code>torch.tensor()</code></a> 中类型推断的默认浮点类型。</p>
<p>默认浮点 dtype 最初为<code>torch.float32</code>。</p>
<p>Parameters</p>
<p><strong>d</strong>  (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>)–浮点 dtype，使其成为默认值</p>
<p>例：</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-4-1" name="__codelineno-4-1" href="#__codelineno-4-1"></a>&gt;&gt;&gt; torch.tensor([1.2, 3]).dtype           # initial default for floating point is torch.float32
<a id="__codelineno-4-2" name="__codelineno-4-2" href="#__codelineno-4-2"></a>torch.float32
<a id="__codelineno-4-3" name="__codelineno-4-3" href="#__codelineno-4-3"></a>&gt;&gt;&gt; torch.set_default_dtype(torch.float64)
<a id="__codelineno-4-4" name="__codelineno-4-4" href="#__codelineno-4-4"></a>&gt;&gt;&gt; torch.tensor([1.2, 3]).dtype           # a new floating point tensor
<a id="__codelineno-4-5" name="__codelineno-4-5" href="#__codelineno-4-5"></a>torch.float64
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-5-1" name="__codelineno-5-1" href="#__codelineno-5-1"></a>torch.get_default_dtype() → torch.dtype¶
</code></pre></div>
<p>获取当前的默认浮点数 <a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a> 。</p>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-6-1" name="__codelineno-6-1" href="#__codelineno-6-1"></a>&gt;&gt;&gt; torch.get_default_dtype()  # initial default for floating point is torch.float32
<a id="__codelineno-6-2" name="__codelineno-6-2" href="#__codelineno-6-2"></a>torch.float32
<a id="__codelineno-6-3" name="__codelineno-6-3" href="#__codelineno-6-3"></a>&gt;&gt;&gt; torch.set_default_dtype(torch.float64)
<a id="__codelineno-6-4" name="__codelineno-6-4" href="#__codelineno-6-4"></a>&gt;&gt;&gt; torch.get_default_dtype()  # default is now changed to torch.float64
<a id="__codelineno-6-5" name="__codelineno-6-5" href="#__codelineno-6-5"></a>torch.float64
<a id="__codelineno-6-6" name="__codelineno-6-6" href="#__codelineno-6-6"></a>&gt;&gt;&gt; torch.set_default_tensor_type(torch.FloatTensor)  # setting tensor type also affects this
<a id="__codelineno-6-7" name="__codelineno-6-7" href="#__codelineno-6-7"></a>&gt;&gt;&gt; torch.get_default_dtype()  # changed to torch.float32, the dtype for torch.FloatTensor
<a id="__codelineno-6-8" name="__codelineno-6-8" href="#__codelineno-6-8"></a>torch.float32
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-7-1" name="__codelineno-7-1" href="#__codelineno-7-1"></a>torch.set_default_tensor_type(t)¶
</code></pre></div>
<p>将默认的<code>torch.Tensor</code>类型设置为浮点张量类型<code>t</code>。 该类型还将用作 <a href="#torch.tensor" title="torch.tensor"><code>torch.tensor()</code></a> 中类型推断的默认浮点类型。</p>
<p>默认的浮点张量类型最初为<code>torch.FloatTensor</code>。</p>
<p>Parameters</p>
<p><strong>t</strong>  (<em>python：type</em> <em>或</em> <em>字符串</em>）–浮点张量类型或其名称</p>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-8-1" name="__codelineno-8-1" href="#__codelineno-8-1"></a>&gt;&gt;&gt; torch.tensor([1.2, 3]).dtype    # initial default for floating point is torch.float32
<a id="__codelineno-8-2" name="__codelineno-8-2" href="#__codelineno-8-2"></a>torch.float32
<a id="__codelineno-8-3" name="__codelineno-8-3" href="#__codelineno-8-3"></a>&gt;&gt;&gt; torch.set_default_tensor_type(torch.DoubleTensor)
<a id="__codelineno-8-4" name="__codelineno-8-4" href="#__codelineno-8-4"></a>&gt;&gt;&gt; torch.tensor([1.2, 3]).dtype    # a new floating point tensor
<a id="__codelineno-8-5" name="__codelineno-8-5" href="#__codelineno-8-5"></a>torch.float64
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-9-1" name="__codelineno-9-1" href="#__codelineno-9-1"></a>torch.numel(input) → int¶
</code></pre></div>
<p>返回<code>input</code>张量中的元素总数。</p>
<p>Parameters</p>
<p><strong>输入</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–输入张量。</p>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-10-1" name="__codelineno-10-1" href="#__codelineno-10-1"></a>&gt;&gt;&gt; a = torch.randn(1, 2, 3, 4, 5)
<a id="__codelineno-10-2" name="__codelineno-10-2" href="#__codelineno-10-2"></a>&gt;&gt;&gt; torch.numel(a)
<a id="__codelineno-10-3" name="__codelineno-10-3" href="#__codelineno-10-3"></a>120
<a id="__codelineno-10-4" name="__codelineno-10-4" href="#__codelineno-10-4"></a>&gt;&gt;&gt; a = torch.zeros(4,4)
<a id="__codelineno-10-5" name="__codelineno-10-5" href="#__codelineno-10-5"></a>&gt;&gt;&gt; torch.numel(a)
<a id="__codelineno-10-6" name="__codelineno-10-6" href="#__codelineno-10-6"></a>16
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-11-1" name="__codelineno-11-1" href="#__codelineno-11-1"></a>torch.set_printoptions(precision=None, threshold=None, edgeitems=None, linewidth=None, profile=None, sci_mode=None)¶
</code></pre></div>
<p>设置打印选项。 从 NumPy 无耻地拿走的物品</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>precision</strong> –浮点输出的精度位数(默认= 4）。</p>
</li>
<li>
<p><strong>阈值</strong> –触发汇总而不是完整的 &lt;cite&gt;repr&lt;/cite&gt; 的数组元素总数(默认= 1000）。</p>
</li>
<li>
<p><strong>edgeitems</strong> -每个维的开始和结束处摘要中数组项目的数量(默认= 3）。</p>
</li>
<li>
<p><strong>线宽</strong> –用于插入换行符的每行字符数(默认= 80）。 阈值矩阵将忽略此参数。</p>
</li>
<li>
<p><strong>配置文件</strong> – Sane 默认用于漂亮的打印。 可以使用以上任何选项覆盖。 (&lt;cite&gt;默认&lt;/cite&gt;，&lt;cite&gt;短&lt;/cite&gt;，&lt;cite&gt;完整&lt;/cite&gt;中的任何一种）</p>
</li>
<li>
<p><strong>sci_mode</strong> –启用(真）或禁用(假）科学计数法。 如果指定了 None(默认），则该值由 &lt;cite&gt;_Formatter&lt;/cite&gt; 定义</p>
</li>
</ul>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-12-1" name="__codelineno-12-1" href="#__codelineno-12-1"></a>torch.set_flush_denormal(mode) → bool¶
</code></pre></div>
<p>禁用 CPU 上的非正常浮​​点数。</p>
<p>如果您的系统支持刷新非正规数并且已成功配置刷新非正规模式，则返回<code>True</code>。 <a href="#torch.set_flush_denormal" title="torch.set_flush_denormal"><code>set_flush_denormal()</code></a> 仅在支持 SSE3 的 x86 架构上受支持。</p>
<p>Parameters</p>
<p><strong>模式</strong> (<em>bool</em> )–控制是否启用冲洗非正常模式</p>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-13-1" name="__codelineno-13-1" href="#__codelineno-13-1"></a>&gt;&gt;&gt; torch.set_flush_denormal(True)
<a id="__codelineno-13-2" name="__codelineno-13-2" href="#__codelineno-13-2"></a>True
<a id="__codelineno-13-3" name="__codelineno-13-3" href="#__codelineno-13-3"></a>&gt;&gt;&gt; torch.tensor([1e-323], dtype=torch.float64)
<a id="__codelineno-13-4" name="__codelineno-13-4" href="#__codelineno-13-4"></a>tensor([ 0.], dtype=torch.float64)
<a id="__codelineno-13-5" name="__codelineno-13-5" href="#__codelineno-13-5"></a>&gt;&gt;&gt; torch.set_flush_denormal(False)
<a id="__codelineno-13-6" name="__codelineno-13-6" href="#__codelineno-13-6"></a>True
<a id="__codelineno-13-7" name="__codelineno-13-7" href="#__codelineno-13-7"></a>&gt;&gt;&gt; torch.tensor([1e-323], dtype=torch.float64)
<a id="__codelineno-13-8" name="__codelineno-13-8" href="#__codelineno-13-8"></a>tensor(9.88131e-324 *
<a id="__codelineno-13-9" name="__codelineno-13-9" href="#__codelineno-13-9"></a>       [ 1.0000], dtype=torch.float64)
</code></pre></div>
<h3 id="_2">创作行动</h3>
<p>注意</p>
<p>随机抽样创建操作列在<a href="#random-sampling">随机抽样</a>下，包括： <a href="#torch.rand" title="torch.rand"><code>torch.rand()</code></a> <a href="#torch.rand_like" title="torch.rand_like"><code>torch.rand_like()</code></a> <a href="#torch.randn" title="torch.randn"><code>torch.randn()</code></a> <a href="#torch.randn_like" title="torch.randn_like"><code>torch.randn_like()</code></a> <a href="#torch.randint" title="torch.randint"><code>torch.randint()</code></a> <a href="#torch.randint_like" title="torch.randint_like"><code>torch.randint_like()</code></a> <a href="#torch.randperm" title="torch.randperm"><code>torch.randperm()</code></a> 您也可以将 <a href="#torch.empty" title="torch.empty"><code>torch.empty()</code></a> 与<a href="#inplace-random-sampling">输入一起使用 位随机抽样</a>方法来创建 <a href="tensors.html#torch.Tensor" title="torch.Tensor"><code>torch.Tensor</code></a> ，并从更广泛的分布范围内采样值。</p>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-14-1" name="__codelineno-14-1" href="#__codelineno-14-1"></a>torch.tensor(data, dtype=None, device=None, requires_grad=False, pin_memory=False) → Tensor¶
</code></pre></div>
<p>用<code>data</code>构造一个张量。</p>
<p>警告</p>
<p><a href="#torch.tensor" title="torch.tensor"><code>torch.tensor()</code></a> 始终复制<code>data</code>。 如果您具有张量<code>data</code>并希望避免复制，请使用 <a href="tensors.html#torch.Tensor.requires_grad_" title="torch.Tensor.requires_grad_"><code>torch.Tensor.requires_grad_()</code></a> 或 <a href="autograd.html#torch.Tensor.detach" title="torch.Tensor.detach"><code>torch.Tensor.detach()</code></a> 。 如果您有 NumPy <code>ndarray</code>并想避免复制，请使用 <a href="#torch.as_tensor" title="torch.as_tensor"><code>torch.as_tensor()</code></a> 。</p>
<p>Warning</p>
<p>当数据是张量 &lt;cite&gt;x&lt;/cite&gt; 时， <a href="#torch.tensor" title="torch.tensor"><code>torch.tensor()</code></a> 从传递的任何数据中读出“数据”，并构造一个叶子变量。 因此，<code>torch.tensor(x)</code>等同于<code>x.clone().detach()</code>，<code>torch.tensor(x, requires_grad=True)</code>等同于<code>x.clone().detach().requires_grad_(True)</code>。 建议使用<code>clone()</code>和<code>detach()</code>的等效项。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>数据</strong> (<em>array_like</em> )–张量的初始数据。 可以是列表，元组，NumPy <code>ndarray</code>，标量和其他类型。</p>
</li>
<li>
<p><strong>dtype</strong>  (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a> ，可选）–返回张量的所需数据类型。 默认值：如果<code>None</code>，则从<code>data</code>推断数据类型。</p>
</li>
<li>
<p><strong>设备</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a> ，可选）–返回张量的所需设备。 默认值：如果<code>None</code>，则使用当前设备作为默认张量类型(请参见 <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>)。 <code>device</code>将是用于 CPU 张量类型的 CPU，并且是用于 CUDA 张量类型的当前 CUDA 设备。</p>
</li>
<li>
<p><strong>require_grad</strong>  (<em>bool</em> <em>，</em> <em>可选</em>）–如果 autograd 应该在返回的张量上记录操作。 默认值：<code>False</code>。</p>
</li>
<li>
<p><strong>pin_memory</strong>  (<em>bool</em> <em>，</em> <em>可选</em>）–如果设置，返回的张量将分配在固定的内存中。 仅适用于 CPU 张量。 默认值：<code>False</code>。</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-15-1" name="__codelineno-15-1" href="#__codelineno-15-1"></a>&gt;&gt;&gt; torch.tensor([[0.1, 1.2], [2.2, 3.1], [4.9, 5.2]])
<a id="__codelineno-15-2" name="__codelineno-15-2" href="#__codelineno-15-2"></a>tensor([[ 0.1000,  1.2000],
<a id="__codelineno-15-3" name="__codelineno-15-3" href="#__codelineno-15-3"></a>        [ 2.2000,  3.1000],
<a id="__codelineno-15-4" name="__codelineno-15-4" href="#__codelineno-15-4"></a>        [ 4.9000,  5.2000]])
<a id="__codelineno-15-5" name="__codelineno-15-5" href="#__codelineno-15-5"></a>
<a id="__codelineno-15-6" name="__codelineno-15-6" href="#__codelineno-15-6"></a>&gt;&gt;&gt; torch.tensor([0, 1])  # Type inference on data
<a id="__codelineno-15-7" name="__codelineno-15-7" href="#__codelineno-15-7"></a>tensor([ 0,  1])
<a id="__codelineno-15-8" name="__codelineno-15-8" href="#__codelineno-15-8"></a>
<a id="__codelineno-15-9" name="__codelineno-15-9" href="#__codelineno-15-9"></a>&gt;&gt;&gt; torch.tensor([[0.11111, 0.222222, 0.3333333]],
<a id="__codelineno-15-10" name="__codelineno-15-10" href="#__codelineno-15-10"></a>                 dtype=torch.float64,
<a id="__codelineno-15-11" name="__codelineno-15-11" href="#__codelineno-15-11"></a>                 device=torch.device(&#39;cuda:0&#39;))  # creates a torch.cuda.DoubleTensor
<a id="__codelineno-15-12" name="__codelineno-15-12" href="#__codelineno-15-12"></a>tensor([[ 0.1111,  0.2222,  0.3333]], dtype=torch.float64, device=&#39;cuda:0&#39;)
<a id="__codelineno-15-13" name="__codelineno-15-13" href="#__codelineno-15-13"></a>
<a id="__codelineno-15-14" name="__codelineno-15-14" href="#__codelineno-15-14"></a>&gt;&gt;&gt; torch.tensor(3.14159)  # Create a scalar (zero-dimensional tensor)
<a id="__codelineno-15-15" name="__codelineno-15-15" href="#__codelineno-15-15"></a>tensor(3.1416)
<a id="__codelineno-15-16" name="__codelineno-15-16" href="#__codelineno-15-16"></a>
<a id="__codelineno-15-17" name="__codelineno-15-17" href="#__codelineno-15-17"></a>&gt;&gt;&gt; torch.tensor([])  # Create an empty tensor (of size (0,))
<a id="__codelineno-15-18" name="__codelineno-15-18" href="#__codelineno-15-18"></a>tensor([])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-16-1" name="__codelineno-16-1" href="#__codelineno-16-1"></a>torch.sparse_coo_tensor(indices, values, size=None, dtype=None, device=None, requires_grad=False) → Tensor¶
</code></pre></div>
<p>在给定<code>values</code>和给定<code>values</code>的情况下，以非零元素构造 COO(rdinate）格式的稀疏张量。 稀疏张量可以是&lt;cite&gt;而不是&lt;/cite&gt;，在那种情况下，索引中有重复的坐标，并且该索引处的值是所有重复值条目的总和： <a href="https://pytorch.org/docs/stable/sparse.html">torch.sparse</a> 。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>索引</strong> (<em>array_like</em> )–张量的初始数据。 可以是列表，元组，NumPy <code>ndarray</code>，标量和其他类型。 将在内部强制转换为<code>torch.LongTensor</code>。 索引是矩阵中非零值的坐标，因此应为二维，其中第一维是张量维数，第二维是非零值数。</p>
</li>
<li>
<p><strong>值</strong> (<em>array_like</em> )–张量的初始值。 可以是列表，元组，NumPy <code>ndarray</code>，标量和其他类型。</p>
</li>
<li>
<p><strong>大小</strong>(列表，元组或<code>torch.Size</code>，可选）–稀疏张量的大小。 如果未提供，则将推断大小为足以容纳所有非零元素的最小大小。</p>
</li>
<li>
<p><strong>dtype</strong>  (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a> ，可选）–返回张量的所需数据类型。 默认值：如果为 None，则从<code>values</code>推断数据类型。</p>
</li>
<li>
<p><strong>设备</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a> ，可选）–返回张量的所需设备。 默认值：如果为 None，则使用当前设备作为默认张量类型(请参见 <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>)。 <code>device</code>将是用于 CPU 张量类型的 CPU，是用于 CUDA 张量类型的当前 CUDA 设备。</p>
</li>
<li>
<p><strong>requires_grad</strong> (<em>bool__,</em> <em>optional</em>) – If autograd should record operations on the returned tensor. Default: <code>False</code>.</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-17-1" name="__codelineno-17-1" href="#__codelineno-17-1"></a>&gt;&gt;&gt; i = torch.tensor([[0, 1, 1],
<a id="__codelineno-17-2" name="__codelineno-17-2" href="#__codelineno-17-2"></a>                      [2, 0, 2]])
<a id="__codelineno-17-3" name="__codelineno-17-3" href="#__codelineno-17-3"></a>&gt;&gt;&gt; v = torch.tensor([3, 4, 5], dtype=torch.float32)
<a id="__codelineno-17-4" name="__codelineno-17-4" href="#__codelineno-17-4"></a>&gt;&gt;&gt; torch.sparse_coo_tensor(i, v, [2, 4])
<a id="__codelineno-17-5" name="__codelineno-17-5" href="#__codelineno-17-5"></a>tensor(indices=tensor([[0, 1, 1],
<a id="__codelineno-17-6" name="__codelineno-17-6" href="#__codelineno-17-6"></a>                       [2, 0, 2]]),
<a id="__codelineno-17-7" name="__codelineno-17-7" href="#__codelineno-17-7"></a>       values=tensor([3., 4., 5.]),
<a id="__codelineno-17-8" name="__codelineno-17-8" href="#__codelineno-17-8"></a>       size=(2, 4), nnz=3, layout=torch.sparse_coo)
<a id="__codelineno-17-9" name="__codelineno-17-9" href="#__codelineno-17-9"></a>
<a id="__codelineno-17-10" name="__codelineno-17-10" href="#__codelineno-17-10"></a>&gt;&gt;&gt; torch.sparse_coo_tensor(i, v)  # Shape inference
<a id="__codelineno-17-11" name="__codelineno-17-11" href="#__codelineno-17-11"></a>tensor(indices=tensor([[0, 1, 1],
<a id="__codelineno-17-12" name="__codelineno-17-12" href="#__codelineno-17-12"></a>                       [2, 0, 2]]),
<a id="__codelineno-17-13" name="__codelineno-17-13" href="#__codelineno-17-13"></a>       values=tensor([3., 4., 5.]),
<a id="__codelineno-17-14" name="__codelineno-17-14" href="#__codelineno-17-14"></a>       size=(2, 3), nnz=3, layout=torch.sparse_coo)
<a id="__codelineno-17-15" name="__codelineno-17-15" href="#__codelineno-17-15"></a>
<a id="__codelineno-17-16" name="__codelineno-17-16" href="#__codelineno-17-16"></a>&gt;&gt;&gt; torch.sparse_coo_tensor(i, v, [2, 4],
<a id="__codelineno-17-17" name="__codelineno-17-17" href="#__codelineno-17-17"></a>                            dtype=torch.float64,
<a id="__codelineno-17-18" name="__codelineno-17-18" href="#__codelineno-17-18"></a>                            device=torch.device(&#39;cuda:0&#39;))
<a id="__codelineno-17-19" name="__codelineno-17-19" href="#__codelineno-17-19"></a>tensor(indices=tensor([[0, 1, 1],
<a id="__codelineno-17-20" name="__codelineno-17-20" href="#__codelineno-17-20"></a>                       [2, 0, 2]]),
<a id="__codelineno-17-21" name="__codelineno-17-21" href="#__codelineno-17-21"></a>       values=tensor([3., 4., 5.]),
<a id="__codelineno-17-22" name="__codelineno-17-22" href="#__codelineno-17-22"></a>       device=&#39;cuda:0&#39;, size=(2, 4), nnz=3, dtype=torch.float64,
<a id="__codelineno-17-23" name="__codelineno-17-23" href="#__codelineno-17-23"></a>       layout=torch.sparse_coo)
<a id="__codelineno-17-24" name="__codelineno-17-24" href="#__codelineno-17-24"></a>
<a id="__codelineno-17-25" name="__codelineno-17-25" href="#__codelineno-17-25"></a># Create an empty sparse tensor with the following invariants:
<a id="__codelineno-17-26" name="__codelineno-17-26" href="#__codelineno-17-26"></a>#   1\. sparse_dim + dense_dim = len(SparseTensor.shape)
<a id="__codelineno-17-27" name="__codelineno-17-27" href="#__codelineno-17-27"></a>#   2\. SparseTensor._indices().shape = (sparse_dim, nnz)
<a id="__codelineno-17-28" name="__codelineno-17-28" href="#__codelineno-17-28"></a>#   3\. SparseTensor._values().shape = (nnz, SparseTensor.shape[sparse_dim:])
<a id="__codelineno-17-29" name="__codelineno-17-29" href="#__codelineno-17-29"></a>#
<a id="__codelineno-17-30" name="__codelineno-17-30" href="#__codelineno-17-30"></a># For instance, to create an empty sparse tensor with nnz = 0, dense_dim = 0 and
<a id="__codelineno-17-31" name="__codelineno-17-31" href="#__codelineno-17-31"></a># sparse_dim = 1 (hence indices is a 2D tensor of shape = (1, 0))
<a id="__codelineno-17-32" name="__codelineno-17-32" href="#__codelineno-17-32"></a>&gt;&gt;&gt; S = torch.sparse_coo_tensor(torch.empty([1, 0]), [], [1])
<a id="__codelineno-17-33" name="__codelineno-17-33" href="#__codelineno-17-33"></a>tensor(indices=tensor([], size=(1, 0)),
<a id="__codelineno-17-34" name="__codelineno-17-34" href="#__codelineno-17-34"></a>       values=tensor([], size=(0,)),
<a id="__codelineno-17-35" name="__codelineno-17-35" href="#__codelineno-17-35"></a>       size=(1,), nnz=0, layout=torch.sparse_coo)
<a id="__codelineno-17-36" name="__codelineno-17-36" href="#__codelineno-17-36"></a>
<a id="__codelineno-17-37" name="__codelineno-17-37" href="#__codelineno-17-37"></a># and to create an empty sparse tensor with nnz = 0, dense_dim = 1 and
<a id="__codelineno-17-38" name="__codelineno-17-38" href="#__codelineno-17-38"></a># sparse_dim = 1
<a id="__codelineno-17-39" name="__codelineno-17-39" href="#__codelineno-17-39"></a>&gt;&gt;&gt; S = torch.sparse_coo_tensor(torch.empty([1, 0]), torch.empty([0, 2]), [1, 2])
<a id="__codelineno-17-40" name="__codelineno-17-40" href="#__codelineno-17-40"></a>tensor(indices=tensor([], size=(1, 0)),
<a id="__codelineno-17-41" name="__codelineno-17-41" href="#__codelineno-17-41"></a>       values=tensor([], size=(0, 2)),
<a id="__codelineno-17-42" name="__codelineno-17-42" href="#__codelineno-17-42"></a>       size=(1, 2), nnz=0, layout=torch.sparse_coo)
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-18-1" name="__codelineno-18-1" href="#__codelineno-18-1"></a>torch.as_tensor(data, dtype=None, device=None) → Tensor¶
</code></pre></div>
<p>将数据转换为&lt;cite&gt;torch。张量&lt;/cite&gt;。 如果数据已经是具有相同 &lt;cite&gt;dtype&lt;/cite&gt; 和&lt;cite&gt;设备&lt;/cite&gt;的&lt;cite&gt;张量&lt;/cite&gt;，则不会执行任何复制，否则将使用新的&lt;cite&gt;张量&lt;/cite&gt;。 如果数据&lt;cite&gt;张量&lt;/cite&gt;具有<code>requires_grad=True</code>，则返回保留计算图的计算图。 同样，如果数据是对应的 &lt;cite&gt;dtype&lt;/cite&gt; 的<code>ndarray</code>，并且&lt;cite&gt;设备&lt;/cite&gt;是 cpu，则不会执行任何复制。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>data</strong> (<em>array_like</em>) – Initial data for the tensor. Can be a list, tuple, NumPy <code>ndarray</code>, scalar, and other types.</p>
</li>
<li>
<p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) – the desired data type of returned tensor. Default: if <code>None</code>, infers data type from <code>data</code>.</p>
</li>
<li>
<p><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) – the desired device of returned tensor. Default: if <code>None</code>, uses the current device for the default tensor type (see <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>). <code>device</code> will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-19-1" name="__codelineno-19-1" href="#__codelineno-19-1"></a>&gt;&gt;&gt; a = numpy.array([1, 2, 3])
<a id="__codelineno-19-2" name="__codelineno-19-2" href="#__codelineno-19-2"></a>&gt;&gt;&gt; t = torch.as_tensor(a)
<a id="__codelineno-19-3" name="__codelineno-19-3" href="#__codelineno-19-3"></a>&gt;&gt;&gt; t
<a id="__codelineno-19-4" name="__codelineno-19-4" href="#__codelineno-19-4"></a>tensor([ 1,  2,  3])
<a id="__codelineno-19-5" name="__codelineno-19-5" href="#__codelineno-19-5"></a>&gt;&gt;&gt; t[0] = -1
<a id="__codelineno-19-6" name="__codelineno-19-6" href="#__codelineno-19-6"></a>&gt;&gt;&gt; a
<a id="__codelineno-19-7" name="__codelineno-19-7" href="#__codelineno-19-7"></a>array([-1,  2,  3])
<a id="__codelineno-19-8" name="__codelineno-19-8" href="#__codelineno-19-8"></a>
<a id="__codelineno-19-9" name="__codelineno-19-9" href="#__codelineno-19-9"></a>&gt;&gt;&gt; a = numpy.array([1, 2, 3])
<a id="__codelineno-19-10" name="__codelineno-19-10" href="#__codelineno-19-10"></a>&gt;&gt;&gt; t = torch.as_tensor(a, device=torch.device(&#39;cuda&#39;))
<a id="__codelineno-19-11" name="__codelineno-19-11" href="#__codelineno-19-11"></a>&gt;&gt;&gt; t
<a id="__codelineno-19-12" name="__codelineno-19-12" href="#__codelineno-19-12"></a>tensor([ 1,  2,  3])
<a id="__codelineno-19-13" name="__codelineno-19-13" href="#__codelineno-19-13"></a>&gt;&gt;&gt; t[0] = -1
<a id="__codelineno-19-14" name="__codelineno-19-14" href="#__codelineno-19-14"></a>&gt;&gt;&gt; a
<a id="__codelineno-19-15" name="__codelineno-19-15" href="#__codelineno-19-15"></a>array([1,  2,  3])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-20-1" name="__codelineno-20-1" href="#__codelineno-20-1"></a>torch.as_strided(input, size, stride, storage_offset=0) → Tensor¶
</code></pre></div>
<p>创建具有指定<code>size</code>，<code>stride</code>和<code>storage_offset</code>的现有&lt;cite&gt;炬管&lt;/cite&gt; <code>input</code>的视图。</p>
<p>Warning</p>
<p>创建的张量中的一个以上元素可以引用单个存储位置。 结果，就地操作(尤其是矢量化的操作）可能会导致错误的行为。 如果需要写张量，请先克隆它们。</p>
<p>许多 PyTorch 函数可返回张量视图，并在此函数内部实现。 这些功能，例如 <a href="tensors.html#torch.Tensor.expand" title="torch.Tensor.expand"><code>torch.Tensor.expand()</code></a> ，更易于阅读，因此更可取。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>大小</strong>(<em>元组</em> <em>或</em> <em>python：ints</em> )–输出张量的形状</p>
</li>
<li>
<p><strong>跨度</strong>(<em>元组</em> <em>或</em> <em>python：ints</em> )–输出张量的跨度</p>
</li>
<li>
<p><strong>storage_offset</strong>  (<em>python：int</em> <em>，</em> <em>可选</em>）–输出张量的基础存储中的偏移量</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-21-1" name="__codelineno-21-1" href="#__codelineno-21-1"></a>&gt;&gt;&gt; x = torch.randn(3, 3)
<a id="__codelineno-21-2" name="__codelineno-21-2" href="#__codelineno-21-2"></a>&gt;&gt;&gt; x
<a id="__codelineno-21-3" name="__codelineno-21-3" href="#__codelineno-21-3"></a>tensor([[ 0.9039,  0.6291,  1.0795],
<a id="__codelineno-21-4" name="__codelineno-21-4" href="#__codelineno-21-4"></a>        [ 0.1586,  2.1939, -0.4900],
<a id="__codelineno-21-5" name="__codelineno-21-5" href="#__codelineno-21-5"></a>        [-0.1909, -0.7503,  1.9355]])
<a id="__codelineno-21-6" name="__codelineno-21-6" href="#__codelineno-21-6"></a>&gt;&gt;&gt; t = torch.as_strided(x, (2, 2), (1, 2))
<a id="__codelineno-21-7" name="__codelineno-21-7" href="#__codelineno-21-7"></a>&gt;&gt;&gt; t
<a id="__codelineno-21-8" name="__codelineno-21-8" href="#__codelineno-21-8"></a>tensor([[0.9039, 1.0795],
<a id="__codelineno-21-9" name="__codelineno-21-9" href="#__codelineno-21-9"></a>        [0.6291, 0.1586]])
<a id="__codelineno-21-10" name="__codelineno-21-10" href="#__codelineno-21-10"></a>&gt;&gt;&gt; t = torch.as_strided(x, (2, 2), (1, 2), 1)
<a id="__codelineno-21-11" name="__codelineno-21-11" href="#__codelineno-21-11"></a>tensor([[0.6291, 0.1586],
<a id="__codelineno-21-12" name="__codelineno-21-12" href="#__codelineno-21-12"></a>        [1.0795, 2.1939]])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-22-1" name="__codelineno-22-1" href="#__codelineno-22-1"></a>torch.from_numpy(ndarray) → Tensor¶
</code></pre></div>
<p>从<code>numpy.ndarray</code>创建 <a href="tensors.html#torch.Tensor" title="torch.Tensor"><code>Tensor</code></a> 。</p>
<p>返回的张量和<code>ndarray</code>共享相同的内存。 对张量的修改将反映在<code>ndarray</code>中，反之亦然。 返回的张量不可调整大小。</p>
<p>当前它接受具有<code>numpy.float64</code>，<code>numpy.float32</code>，<code>numpy.float16</code>，<code>numpy.int64</code>，<code>numpy.int32</code>，<code>numpy.int16</code>，<code>numpy.int8</code>，<code>numpy.uint8</code>和<code>numpy.bool</code> d 类型的<code>ndarray</code>。</p>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-23-1" name="__codelineno-23-1" href="#__codelineno-23-1"></a>&gt;&gt;&gt; a = numpy.array([1, 2, 3])
<a id="__codelineno-23-2" name="__codelineno-23-2" href="#__codelineno-23-2"></a>&gt;&gt;&gt; t = torch.from_numpy(a)
<a id="__codelineno-23-3" name="__codelineno-23-3" href="#__codelineno-23-3"></a>&gt;&gt;&gt; t
<a id="__codelineno-23-4" name="__codelineno-23-4" href="#__codelineno-23-4"></a>tensor([ 1,  2,  3])
<a id="__codelineno-23-5" name="__codelineno-23-5" href="#__codelineno-23-5"></a>&gt;&gt;&gt; t[0] = -1
<a id="__codelineno-23-6" name="__codelineno-23-6" href="#__codelineno-23-6"></a>&gt;&gt;&gt; a
<a id="__codelineno-23-7" name="__codelineno-23-7" href="#__codelineno-23-7"></a>array([-1,  2,  3])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-24-1" name="__codelineno-24-1" href="#__codelineno-24-1"></a>torch.zeros(*size, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) → Tensor¶
</code></pre></div>
<p>返回一个由标量值 &lt;cite&gt;0&lt;/cite&gt; 填充的张量，其形状由变量参数<code>size</code>定义。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>大小</strong> (<em>python：int ...</em> )–定义输出张量形状的整数序列。 可以是可变数量的参数，也可以是列表或元组之类的集合。</p>
</li>
<li>
<p><strong>输出</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a> <em>，</em> <em>可选</em>）–输出张量。</p>
</li>
<li>
<p><strong>dtype</strong>  (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a> ，可选）–返回张量的所需数据类型。 默认值：如果<code>None</code>使用全局默认值(请参见 <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>)。</p>
</li>
<li>
<p><strong>布局</strong> (<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code>torch.layout</code></a> ，可选）–返回的 Tensor 所需的布局。 默认值：<code>torch.strided</code>。</p>
</li>
<li>
<p><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) – the desired device of returned tensor. Default: if <code>None</code>, uses the current device for the default tensor type (see <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>). <code>device</code> will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.</p>
</li>
<li>
<p><strong>requires_grad</strong> (<em>bool__,</em> <em>optional</em>) – If autograd should record operations on the returned tensor. Default: <code>False</code>.</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-25-1" name="__codelineno-25-1" href="#__codelineno-25-1"></a>&gt;&gt;&gt; torch.zeros(2, 3)
<a id="__codelineno-25-2" name="__codelineno-25-2" href="#__codelineno-25-2"></a>tensor([[ 0.,  0.,  0.],
<a id="__codelineno-25-3" name="__codelineno-25-3" href="#__codelineno-25-3"></a>        [ 0.,  0.,  0.]])
<a id="__codelineno-25-4" name="__codelineno-25-4" href="#__codelineno-25-4"></a>
<a id="__codelineno-25-5" name="__codelineno-25-5" href="#__codelineno-25-5"></a>&gt;&gt;&gt; torch.zeros(5)
<a id="__codelineno-25-6" name="__codelineno-25-6" href="#__codelineno-25-6"></a>tensor([ 0.,  0.,  0.,  0.,  0.])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-26-1" name="__codelineno-26-1" href="#__codelineno-26-1"></a>torch.zeros_like(input, dtype=None, layout=None, device=None, requires_grad=False) → Tensor¶
</code></pre></div>
<p>返回一个由标量值 &lt;cite&gt;0&lt;/cite&gt; 填充的张量，其大小与<code>input</code>相同。 <code>torch.zeros_like(input)</code>等效于<code>torch.zeros(input.size(), dtype=input.dtype, layout=input.layout, device=input.device)</code>。</p>
<p>Warning</p>
<p>从 0.4 开始，此功能不支持<code>out</code>关键字。 作为替代，旧的<code>torch.zeros_like(input, out=output)</code>等效于<code>torch.zeros(input.size(), out=output)</code>。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>输入</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)– <code>input</code>的大小将确定输出张量的大小。</p>
</li>
<li>
<p><strong>dtype</strong>  (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a> ，可选）–返回的 Tensor 的所需数据类型。 默认值：如果为<code>None</code>，则默认为<code>input</code>的 dtype。</p>
</li>
<li>
<p><strong>布局</strong> (<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code>torch.layout</code></a> ，可选）–返回张量的所需布局。 默认值：如果为<code>None</code>，则默认为<code>input</code>的布局。</p>
</li>
<li>
<p><strong>设备</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a> ，可选）–返回张量的所需设备。 默认值：如果为<code>None</code>，则默认为<code>input</code>的设备。</p>
</li>
<li>
<p><strong>requires_grad</strong> (<em>bool__,</em> <em>optional</em>) – If autograd should record operations on the returned tensor. Default: <code>False</code>.</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-27-1" name="__codelineno-27-1" href="#__codelineno-27-1"></a>&gt;&gt;&gt; input = torch.empty(2, 3)
<a id="__codelineno-27-2" name="__codelineno-27-2" href="#__codelineno-27-2"></a>&gt;&gt;&gt; torch.zeros_like(input)
<a id="__codelineno-27-3" name="__codelineno-27-3" href="#__codelineno-27-3"></a>tensor([[ 0.,  0.,  0.],
<a id="__codelineno-27-4" name="__codelineno-27-4" href="#__codelineno-27-4"></a>        [ 0.,  0.,  0.]])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-28-1" name="__codelineno-28-1" href="#__codelineno-28-1"></a>torch.ones(*size, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) → Tensor¶
</code></pre></div>
<p>返回一个由标量值 &lt;cite&gt;1&lt;/cite&gt; 填充的张量，其形状由变量自变量<code>size</code>定义。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>size</strong> (<em>python:int...</em>) – a sequence of integers defining the shape of the output tensor. Can be a variable number of arguments or a collection like a list or tuple.</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
<li>
<p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) – the desired data type of returned tensor. Default: if <code>None</code>, uses a global default (see <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>).</p>
</li>
<li>
<p><strong>layout</strong> (<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code>torch.layout</code></a>, optional) – the desired layout of returned Tensor. Default: <code>torch.strided</code>.</p>
</li>
<li>
<p><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) – the desired device of returned tensor. Default: if <code>None</code>, uses the current device for the default tensor type (see <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>). <code>device</code> will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.</p>
</li>
<li>
<p><strong>requires_grad</strong> (<em>bool__,</em> <em>optional</em>) – If autograd should record operations on the returned tensor. Default: <code>False</code>.</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-29-1" name="__codelineno-29-1" href="#__codelineno-29-1"></a>&gt;&gt;&gt; torch.ones(2, 3)
<a id="__codelineno-29-2" name="__codelineno-29-2" href="#__codelineno-29-2"></a>tensor([[ 1.,  1.,  1.],
<a id="__codelineno-29-3" name="__codelineno-29-3" href="#__codelineno-29-3"></a>        [ 1.,  1.,  1.]])
<a id="__codelineno-29-4" name="__codelineno-29-4" href="#__codelineno-29-4"></a>
<a id="__codelineno-29-5" name="__codelineno-29-5" href="#__codelineno-29-5"></a>&gt;&gt;&gt; torch.ones(5)
<a id="__codelineno-29-6" name="__codelineno-29-6" href="#__codelineno-29-6"></a>tensor([ 1.,  1.,  1.,  1.,  1.])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-30-1" name="__codelineno-30-1" href="#__codelineno-30-1"></a>torch.ones_like(input, dtype=None, layout=None, device=None, requires_grad=False) → Tensor¶
</code></pre></div>
<p>返回一个由标量值 &lt;cite&gt;1&lt;/cite&gt; 填充的张量，其大小与<code>input</code>相同。 <code>torch.ones_like(input)</code>等效于<code>torch.ones(input.size(), dtype=input.dtype, layout=input.layout, device=input.device)</code>。</p>
<p>Warning</p>
<p>从 0.4 开始，此功能不支持<code>out</code>关键字。 作为替代，旧的<code>torch.ones_like(input, out=output)</code>等效于<code>torch.ones(input.size(), out=output)</code>。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the size of <code>input</code> will determine size of the output tensor.</p>
</li>
<li>
<p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) – the desired data type of returned Tensor. Default: if <code>None</code>, defaults to the dtype of <code>input</code>.</p>
</li>
<li>
<p><strong>layout</strong> (<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code>torch.layout</code></a>, optional) – the desired layout of returned tensor. Default: if <code>None</code>, defaults to the layout of <code>input</code>.</p>
</li>
<li>
<p><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) – the desired device of returned tensor. Default: if <code>None</code>, defaults to the device of <code>input</code>.</p>
</li>
<li>
<p><strong>requires_grad</strong> (<em>bool__,</em> <em>optional</em>) – If autograd should record operations on the returned tensor. Default: <code>False</code>.</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-31-1" name="__codelineno-31-1" href="#__codelineno-31-1"></a>&gt;&gt;&gt; input = torch.empty(2, 3)
<a id="__codelineno-31-2" name="__codelineno-31-2" href="#__codelineno-31-2"></a>&gt;&gt;&gt; torch.ones_like(input)
<a id="__codelineno-31-3" name="__codelineno-31-3" href="#__codelineno-31-3"></a>tensor([[ 1.,  1.,  1.],
<a id="__codelineno-31-4" name="__codelineno-31-4" href="#__codelineno-31-4"></a>        [ 1.,  1.,  1.]])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-32-1" name="__codelineno-32-1" href="#__codelineno-32-1"></a>torch.arange(start=0, end, step=1, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) → Tensor¶
</code></pre></div>
<p>返回大小为<img alt="" src="../img/fb2784cdfbfdd3f567366d5157f05b62.jpg" />的一维张量，该值具有从&lt;cite&gt;开始&lt;/cite&gt;开始具有公共差<code>step</code>的间隔<code>[start, end)</code>的值。</p>
<p>请注意，与<code>end</code>比较时，非整数<code>step</code>会出现浮点舍入错误； 为了避免不一致，在这种情况下，建议在<code>end</code>中添加一个小的ε。</p>
<p><img alt="" src="../img/e3f9924ddb1c14b63336e10cb955b09f.jpg" /></p>
<p>Parameters</p>
<ul>
<li>
<p><strong>起始</strong>(<em>编号</em>）–点集的起始值。 默认值：<code>0</code>。</p>
</li>
<li>
<p><strong>结束</strong>(<em>编号</em>）–点集的结束值</p>
</li>
<li>
<p><strong>步骤</strong>(<em>编号</em>）–每对相邻点之间的间隙。 默认值：<code>1</code>。</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
<li>
<p><strong>dtype</strong>  (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a> ，可选）–返回张量的所需数据类型。 默认值：如果<code>None</code>使用全局默认值(请参阅 <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>)。 如果未提供 &lt;cite&gt;dtype&lt;/cite&gt; ，则从其他输入参数推断数据类型。 如果&lt;cite&gt;开始&lt;/cite&gt;，&lt;cite&gt;结束&lt;/cite&gt;或&lt;cite&gt;停止&lt;/cite&gt;中的任何一个是浮点，则推断 &lt;cite&gt;dtype&lt;/cite&gt; 为默认 dtype，请参见[ <a href="#torch.get_default_dtype" title="torch.get_default_dtype"><code>get_default_dtype()</code></a> 。 否则，将 &lt;cite&gt;dtype&lt;/cite&gt; 推断为 &lt;cite&gt;torch.int64&lt;/cite&gt; 。</p>
</li>
<li>
<p><strong>layout</strong> (<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code>torch.layout</code></a>, optional) – the desired layout of returned Tensor. Default: <code>torch.strided</code>.</p>
</li>
<li>
<p><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) – the desired device of returned tensor. Default: if <code>None</code>, uses the current device for the default tensor type (see <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>). <code>device</code> will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.</p>
</li>
<li>
<p><strong>requires_grad</strong> (<em>bool__,</em> <em>optional</em>) – If autograd should record operations on the returned tensor. Default: <code>False</code>.</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-33-1" name="__codelineno-33-1" href="#__codelineno-33-1"></a>&gt;&gt;&gt; torch.arange(5)
<a id="__codelineno-33-2" name="__codelineno-33-2" href="#__codelineno-33-2"></a>tensor([ 0,  1,  2,  3,  4])
<a id="__codelineno-33-3" name="__codelineno-33-3" href="#__codelineno-33-3"></a>&gt;&gt;&gt; torch.arange(1, 4)
<a id="__codelineno-33-4" name="__codelineno-33-4" href="#__codelineno-33-4"></a>tensor([ 1,  2,  3])
<a id="__codelineno-33-5" name="__codelineno-33-5" href="#__codelineno-33-5"></a>&gt;&gt;&gt; torch.arange(1, 2.5, 0.5)
<a id="__codelineno-33-6" name="__codelineno-33-6" href="#__codelineno-33-6"></a>tensor([ 1.0000,  1.5000,  2.0000])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-34-1" name="__codelineno-34-1" href="#__codelineno-34-1"></a>torch.range(start=0, end, step=1, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) → Tensor¶
</code></pre></div>
<p>在步骤<code>step</code>中返回大小为<img alt="" src="../img/c04824f45400f61b164b031c61248a15.jpg" />的一维张量，其值从<code>start</code>到<code>end</code>。 阶跃是张量中两个值之间的差距。</p>
<p><img alt="" src="../img/ce7cb189b2d3908fa0a51507da3efd58.jpg" /></p>
<p>Warning</p>
<p>不推荐使用此功能，而推荐使用 <a href="#torch.arange" title="torch.arange"><code>torch.arange()</code></a> 。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>start</strong>  (<em>python：float</em> )–点集的起始值。 默认值：<code>0</code>。</p>
</li>
<li>
<p><strong>end</strong>  (<em>python：float</em> )–点集的结束值</p>
</li>
<li>
<p><strong>步骤</strong> (<em>python：float</em> )–每对相邻点之间的间隙。 默认值：<code>1</code>。</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
<li>
<p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) – the desired data type of returned tensor. Default: if <code>None</code>, uses a global default (see <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>). If &lt;cite&gt;dtype&lt;/cite&gt; is not given, infer the data type from the other input arguments. If any of &lt;cite&gt;start&lt;/cite&gt;, &lt;cite&gt;end&lt;/cite&gt;, or &lt;cite&gt;stop&lt;/cite&gt; are floating-point, the &lt;cite&gt;dtype&lt;/cite&gt; is inferred to be the default dtype, see <a href="#torch.get_default_dtype" title="torch.get_default_dtype"><code>get_default_dtype()</code></a>. Otherwise, the &lt;cite&gt;dtype&lt;/cite&gt; is inferred to be &lt;cite&gt;torch.int64&lt;/cite&gt;.</p>
</li>
<li>
<p><strong>layout</strong> (<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code>torch.layout</code></a>, optional) – the desired layout of returned Tensor. Default: <code>torch.strided</code>.</p>
</li>
<li>
<p><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) – the desired device of returned tensor. Default: if <code>None</code>, uses the current device for the default tensor type (see <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>). <code>device</code> will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.</p>
</li>
<li>
<p><strong>requires_grad</strong> (<em>bool__,</em> <em>optional</em>) – If autograd should record operations on the returned tensor. Default: <code>False</code>.</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-35-1" name="__codelineno-35-1" href="#__codelineno-35-1"></a>&gt;&gt;&gt; torch.range(1, 4)
<a id="__codelineno-35-2" name="__codelineno-35-2" href="#__codelineno-35-2"></a>tensor([ 1.,  2.,  3.,  4.])
<a id="__codelineno-35-3" name="__codelineno-35-3" href="#__codelineno-35-3"></a>&gt;&gt;&gt; torch.range(1, 4, 0.5)
<a id="__codelineno-35-4" name="__codelineno-35-4" href="#__codelineno-35-4"></a>tensor([ 1.0000,  1.5000,  2.0000,  2.5000,  3.0000,  3.5000,  4.0000])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-36-1" name="__codelineno-36-1" href="#__codelineno-36-1"></a>torch.linspace(start, end, steps=100, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) → Tensor¶
</code></pre></div>
<p>返回<code>start</code>和<code>end</code>之间等距点的<code>steps</code>的一维张量。</p>
<p>输出张量为<code>steps</code>大小的 1-D。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>开始</strong> (<em>python：float</em> )–点集的起始值</p>
</li>
<li>
<p><strong>end</strong> (<em>python:float</em>) – the ending value for the set of points</p>
</li>
<li>
<p><strong>步骤</strong> (<em>python：int</em> )–在<code>start</code>和<code>end</code>之间采样的点数。 默认值：<code>100</code>。</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
<li>
<p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) – the desired data type of returned tensor. Default: if <code>None</code>, uses a global default (see <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>).</p>
</li>
<li>
<p><strong>layout</strong> (<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code>torch.layout</code></a>, optional) – the desired layout of returned Tensor. Default: <code>torch.strided</code>.</p>
</li>
<li>
<p><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) – the desired device of returned tensor. Default: if <code>None</code>, uses the current device for the default tensor type (see <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>). <code>device</code> will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.</p>
</li>
<li>
<p><strong>requires_grad</strong> (<em>bool__,</em> <em>optional</em>) – If autograd should record operations on the returned tensor. Default: <code>False</code>.</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-37-1" name="__codelineno-37-1" href="#__codelineno-37-1"></a>&gt;&gt;&gt; torch.linspace(3, 10, steps=5)
<a id="__codelineno-37-2" name="__codelineno-37-2" href="#__codelineno-37-2"></a>tensor([  3.0000,   4.7500,   6.5000,   8.2500,  10.0000])
<a id="__codelineno-37-3" name="__codelineno-37-3" href="#__codelineno-37-3"></a>&gt;&gt;&gt; torch.linspace(-10, 10, steps=5)
<a id="__codelineno-37-4" name="__codelineno-37-4" href="#__codelineno-37-4"></a>tensor([-10.,  -5.,   0.,   5.,  10.])
<a id="__codelineno-37-5" name="__codelineno-37-5" href="#__codelineno-37-5"></a>&gt;&gt;&gt; torch.linspace(start=-10, end=10, steps=5)
<a id="__codelineno-37-6" name="__codelineno-37-6" href="#__codelineno-37-6"></a>tensor([-10.,  -5.,   0.,   5.,  10.])
<a id="__codelineno-37-7" name="__codelineno-37-7" href="#__codelineno-37-7"></a>&gt;&gt;&gt; torch.linspace(start=-10, end=10, steps=1)
<a id="__codelineno-37-8" name="__codelineno-37-8" href="#__codelineno-37-8"></a>tensor([-10.])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-38-1" name="__codelineno-38-1" href="#__codelineno-38-1"></a>torch.logspace(start, end, steps=100, base=10.0, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) → Tensor¶
</code></pre></div>
<p>返回与<img alt="" src="../img/6a2c1af1d373a2b11ada878076f5b2a5.jpg" />和<img alt="" src="../img/6dcd2d5bb84df471dd6574611a280f9d.jpg" />之间的底数<code>base</code>对数间隔的<code>steps</code>点的一维张量。</p>
<p>The output tensor is 1-D of size <code>steps</code>.</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>start</strong> (<em>python:float</em>) – the starting value for the set of points</p>
</li>
<li>
<p><strong>end</strong> (<em>python:float</em>) – the ending value for the set of points</p>
</li>
<li>
<p><strong>steps</strong> (<em>python:int</em>) – number of points to sample between <code>start</code> and <code>end</code>. Default: <code>100</code>.</p>
</li>
<li>
<p><strong>基数</strong> (<em>python：float</em> )–对数函数的基数。 默认值：<code>10.0</code>。</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
<li>
<p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) – the desired data type of returned tensor. Default: if <code>None</code>, uses a global default (see <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>).</p>
</li>
<li>
<p><strong>layout</strong> (<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code>torch.layout</code></a>, optional) – the desired layout of returned Tensor. Default: <code>torch.strided</code>.</p>
</li>
<li>
<p><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) – the desired device of returned tensor. Default: if <code>None</code>, uses the current device for the default tensor type (see <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>). <code>device</code> will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.</p>
</li>
<li>
<p><strong>requires_grad</strong> (<em>bool__,</em> <em>optional</em>) – If autograd should record operations on the returned tensor. Default: <code>False</code>.</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-39-1" name="__codelineno-39-1" href="#__codelineno-39-1"></a>&gt;&gt;&gt; torch.logspace(start=-10, end=10, steps=5)
<a id="__codelineno-39-2" name="__codelineno-39-2" href="#__codelineno-39-2"></a>tensor([ 1.0000e-10,  1.0000e-05,  1.0000e+00,  1.0000e+05,  1.0000e+10])
<a id="__codelineno-39-3" name="__codelineno-39-3" href="#__codelineno-39-3"></a>&gt;&gt;&gt; torch.logspace(start=0.1, end=1.0, steps=5)
<a id="__codelineno-39-4" name="__codelineno-39-4" href="#__codelineno-39-4"></a>tensor([  1.2589,   2.1135,   3.5481,   5.9566,  10.0000])
<a id="__codelineno-39-5" name="__codelineno-39-5" href="#__codelineno-39-5"></a>&gt;&gt;&gt; torch.logspace(start=0.1, end=1.0, steps=1)
<a id="__codelineno-39-6" name="__codelineno-39-6" href="#__codelineno-39-6"></a>tensor([1.2589])
<a id="__codelineno-39-7" name="__codelineno-39-7" href="#__codelineno-39-7"></a>&gt;&gt;&gt; torch.logspace(start=2, end=2, steps=1, base=2)
<a id="__codelineno-39-8" name="__codelineno-39-8" href="#__codelineno-39-8"></a>tensor([4.0])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-40-1" name="__codelineno-40-1" href="#__codelineno-40-1"></a>torch.eye(n, m=None, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) → Tensor¶
</code></pre></div>
<p>返回一个二维张量，对角线上有一个，其他位置为零。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>n</strong>  (<em>python：int</em> )–行数</p>
</li>
<li>
<p><strong>m</strong>  (<em>python：int</em> <em>，</em> <em>可选</em>）–默认为<code>n</code>的列数</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
<li>
<p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) – the desired data type of returned tensor. Default: if <code>None</code>, uses a global default (see <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>).</p>
</li>
<li>
<p><strong>layout</strong> (<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code>torch.layout</code></a>, optional) – the desired layout of returned Tensor. Default: <code>torch.strided</code>.</p>
</li>
<li>
<p><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) – the desired device of returned tensor. Default: if <code>None</code>, uses the current device for the default tensor type (see <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>). <code>device</code> will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.</p>
</li>
<li>
<p><strong>requires_grad</strong> (<em>bool__,</em> <em>optional</em>) – If autograd should record operations on the returned tensor. Default: <code>False</code>.</p>
</li>
</ul>
<p>退货</p>
<p>二维张量，对角线上有一个，其他位置为零</p>
<p>返回类型</p>
<p><a href="tensors.html#torch.Tensor" title="torch.Tensor">张量</a></p>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-41-1" name="__codelineno-41-1" href="#__codelineno-41-1"></a>&gt;&gt;&gt; torch.eye(3)
<a id="__codelineno-41-2" name="__codelineno-41-2" href="#__codelineno-41-2"></a>tensor([[ 1.,  0.,  0.],
<a id="__codelineno-41-3" name="__codelineno-41-3" href="#__codelineno-41-3"></a>        [ 0.,  1.,  0.],
<a id="__codelineno-41-4" name="__codelineno-41-4" href="#__codelineno-41-4"></a>        [ 0.,  0.,  1.]])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-42-1" name="__codelineno-42-1" href="#__codelineno-42-1"></a>torch.empty(*size, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False, pin_memory=False) → Tensor¶
</code></pre></div>
<p>返回填充有未初始化数据的张量。 张量的形状由变量参数<code>size</code>定义。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>size</strong> (<em>python:int...</em>) – a sequence of integers defining the shape of the output tensor. Can be a variable number of arguments or a collection like a list or tuple.</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
<li>
<p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) – the desired data type of returned tensor. Default: if <code>None</code>, uses a global default (see <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>).</p>
</li>
<li>
<p><strong>layout</strong> (<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code>torch.layout</code></a>, optional) – the desired layout of returned Tensor. Default: <code>torch.strided</code>.</p>
</li>
<li>
<p><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) – the desired device of returned tensor. Default: if <code>None</code>, uses the current device for the default tensor type (see <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>). <code>device</code> will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.</p>
</li>
<li>
<p><strong>requires_grad</strong> (<em>bool__,</em> <em>optional</em>) – If autograd should record operations on the returned tensor. Default: <code>False</code>.</p>
</li>
<li>
<p><strong>pin_memory</strong> (<em>bool__,</em> <em>optional</em>) – If set, returned tensor would be allocated in the pinned memory. Works only for CPU tensors. Default: <code>False</code>.</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-43-1" name="__codelineno-43-1" href="#__codelineno-43-1"></a>&gt;&gt;&gt; torch.empty(2, 3)
<a id="__codelineno-43-2" name="__codelineno-43-2" href="#__codelineno-43-2"></a>tensor(1.00000e-08 *
<a id="__codelineno-43-3" name="__codelineno-43-3" href="#__codelineno-43-3"></a>       [[ 6.3984,  0.0000,  0.0000],
<a id="__codelineno-43-4" name="__codelineno-43-4" href="#__codelineno-43-4"></a>        [ 0.0000,  0.0000,  0.0000]])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-44-1" name="__codelineno-44-1" href="#__codelineno-44-1"></a>torch.empty_like(input, dtype=None, layout=None, device=None, requires_grad=False) → Tensor¶
</code></pre></div>
<p>返回与<code>input</code>相同大小的未初始化张量。 <code>torch.empty_like(input)</code>等效于<code>torch.empty(input.size(), dtype=input.dtype, layout=input.layout, device=input.device)</code>。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the size of <code>input</code> will determine size of the output tensor.</p>
</li>
<li>
<p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) – the desired data type of returned Tensor. Default: if <code>None</code>, defaults to the dtype of <code>input</code>.</p>
</li>
<li>
<p><strong>layout</strong> (<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code>torch.layout</code></a>, optional) – the desired layout of returned tensor. Default: if <code>None</code>, defaults to the layout of <code>input</code>.</p>
</li>
<li>
<p><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) – the desired device of returned tensor. Default: if <code>None</code>, defaults to the device of <code>input</code>.</p>
</li>
<li>
<p><strong>requires_grad</strong> (<em>bool__,</em> <em>optional</em>) – If autograd should record operations on the returned tensor. Default: <code>False</code>.</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-45-1" name="__codelineno-45-1" href="#__codelineno-45-1"></a>&gt;&gt;&gt; torch.empty((2,3), dtype=torch.int64)
<a id="__codelineno-45-2" name="__codelineno-45-2" href="#__codelineno-45-2"></a>tensor([[ 9.4064e+13,  2.8000e+01,  9.3493e+13],
<a id="__codelineno-45-3" name="__codelineno-45-3" href="#__codelineno-45-3"></a>        [ 7.5751e+18,  7.1428e+18,  7.5955e+18]])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-46-1" name="__codelineno-46-1" href="#__codelineno-46-1"></a>torch.empty_strided(size, stride, dtype=None, layout=None, device=None, requires_grad=False, pin_memory=False) → Tensor¶
</code></pre></div>
<p>返回填充有未初始化数据的张量。 张量的形状和步幅分别由变量参数<code>size</code>和<code>stride</code>定义。 <code>torch.empty_strided(size, stride)</code>等同于<code>torch.empty(size).as_strided(size, stride)</code>。</p>
<p>Warning</p>
<p>创建的张量中的一个以上元素可以引用单个存储位置。 结果，就地操作(尤其是矢量化的操作）可能会导致错误的行为。 如果需要写张量，请先克隆它们。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>大小</strong>(python：ints 的_元组）–输出张量的形状_</p>
</li>
<li>
<p><strong>跨度</strong>(python：ints 的_元组）–输出张量的跨度_</p>
</li>
<li>
<p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) – the desired data type of returned tensor. Default: if <code>None</code>, uses a global default (see <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>).</p>
</li>
<li>
<p><strong>layout</strong> (<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code>torch.layout</code></a>, optional) – the desired layout of returned Tensor. Default: <code>torch.strided</code>.</p>
</li>
<li>
<p><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) – the desired device of returned tensor. Default: if <code>None</code>, uses the current device for the default tensor type (see <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>). <code>device</code> will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.</p>
</li>
<li>
<p><strong>requires_grad</strong> (<em>bool__,</em> <em>optional</em>) – If autograd should record operations on the returned tensor. Default: <code>False</code>.</p>
</li>
<li>
<p><strong>pin_memory</strong> (<em>bool__,</em> <em>optional</em>) – If set, returned tensor would be allocated in the pinned memory. Works only for CPU tensors. Default: <code>False</code>.</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-47-1" name="__codelineno-47-1" href="#__codelineno-47-1"></a>&gt;&gt;&gt; a = torch.empty_strided((2, 3), (1, 2))
<a id="__codelineno-47-2" name="__codelineno-47-2" href="#__codelineno-47-2"></a>&gt;&gt;&gt; a
<a id="__codelineno-47-3" name="__codelineno-47-3" href="#__codelineno-47-3"></a>tensor([[8.9683e-44, 4.4842e-44, 5.1239e+07],
<a id="__codelineno-47-4" name="__codelineno-47-4" href="#__codelineno-47-4"></a>        [0.0000e+00, 0.0000e+00, 3.0705e-41]])
<a id="__codelineno-47-5" name="__codelineno-47-5" href="#__codelineno-47-5"></a>&gt;&gt;&gt; a.stride()
<a id="__codelineno-47-6" name="__codelineno-47-6" href="#__codelineno-47-6"></a>(1, 2)
<a id="__codelineno-47-7" name="__codelineno-47-7" href="#__codelineno-47-7"></a>&gt;&gt;&gt; a.size()
<a id="__codelineno-47-8" name="__codelineno-47-8" href="#__codelineno-47-8"></a>torch.Size([2, 3])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-48-1" name="__codelineno-48-1" href="#__codelineno-48-1"></a>torch.full(size, fill_value, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) → Tensor¶
</code></pre></div>
<p>返回大小为<code>size</code>的张量，其中填充了<code>fill_value</code>。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>大小</strong> (<em>python：int ...</em> )–定义输出张量形状的整数列表，元组或<code>torch.Size</code>。</p>
</li>
<li>
<p><strong>fill_value</strong> –用来填充输出张量的数字。</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
<li>
<p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) – the desired data type of returned tensor. Default: if <code>None</code>, uses a global default (see <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>).</p>
</li>
<li>
<p><strong>layout</strong> (<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code>torch.layout</code></a>, optional) – the desired layout of returned Tensor. Default: <code>torch.strided</code>.</p>
</li>
<li>
<p><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) – the desired device of returned tensor. Default: if <code>None</code>, uses the current device for the default tensor type (see <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>). <code>device</code> will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.</p>
</li>
<li>
<p><strong>requires_grad</strong> (<em>bool__,</em> <em>optional</em>) – If autograd should record operations on the returned tensor. Default: <code>False</code>.</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-49-1" name="__codelineno-49-1" href="#__codelineno-49-1"></a>&gt;&gt;&gt; torch.full((2, 3), 3.141592)
<a id="__codelineno-49-2" name="__codelineno-49-2" href="#__codelineno-49-2"></a>tensor([[ 3.1416,  3.1416,  3.1416],
<a id="__codelineno-49-3" name="__codelineno-49-3" href="#__codelineno-49-3"></a>        [ 3.1416,  3.1416,  3.1416]])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-50-1" name="__codelineno-50-1" href="#__codelineno-50-1"></a>torch.full_like(input, fill_value, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) → Tensor¶
</code></pre></div>
<p>返回与填充有<code>fill_value</code>的<code>input</code>大小相同的张量。 <code>torch.full_like(input, fill_value)</code>等同于<code>torch.full(input.size(), fill_value, dtype=input.dtype, layout=input.layout, device=input.device)</code>。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the size of <code>input</code> will determine size of the output tensor.</p>
</li>
<li>
<p><strong>fill_value</strong> – the number to fill the output tensor with.</p>
</li>
<li>
<p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) – the desired data type of returned Tensor. Default: if <code>None</code>, defaults to the dtype of <code>input</code>.</p>
</li>
<li>
<p><strong>layout</strong> (<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code>torch.layout</code></a>, optional) – the desired layout of returned tensor. Default: if <code>None</code>, defaults to the layout of <code>input</code>.</p>
</li>
<li>
<p><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) – the desired device of returned tensor. Default: if <code>None</code>, defaults to the device of <code>input</code>.</p>
</li>
<li>
<p><strong>requires_grad</strong> (<em>bool__,</em> <em>optional</em>) – If autograd should record operations on the returned tensor. Default: <code>False</code>.</p>
</li>
</ul>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-51-1" name="__codelineno-51-1" href="#__codelineno-51-1"></a>torch.quantize_per_tensor(input, scale, zero_point, dtype) → Tensor¶
</code></pre></div>
<p>将浮点张量转换为具有给定比例和零点的量化张量。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>输入</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–浮点张量进行量化</p>
</li>
<li>
<p><strong>标度</strong> (<em>python：float</em> )–适用于量化公式的标度</p>
</li>
<li>
<p><strong>zero_point</strong>  (<em>python：int</em> )–映射为浮点零的整数值偏移</p>
</li>
<li>
<p><strong>dtype</strong>  (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>)–返回张量的所需数据类型。 必须是量化的 dtypes 之一：<code>torch.quint8</code>，<code>torch.qint8</code>和<code>torch.qint32</code></p>
</li>
</ul>
<p>Returns</p>
<p>新量化的张量</p>
<p>Return type</p>
<p><a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-52-1" name="__codelineno-52-1" href="#__codelineno-52-1"></a>&gt;&gt;&gt; torch.quantize_per_tensor(torch.tensor([-1.0, 0.0, 1.0, 2.0]), 0.1, 10, torch.quint8)
<a id="__codelineno-52-2" name="__codelineno-52-2" href="#__codelineno-52-2"></a>tensor([-1.,  0.,  1.,  2.], size=(4,), dtype=torch.quint8,
<a id="__codelineno-52-3" name="__codelineno-52-3" href="#__codelineno-52-3"></a>       quantization_scheme=torch.per_tensor_affine, scale=0.1, zero_point=10)
<a id="__codelineno-52-4" name="__codelineno-52-4" href="#__codelineno-52-4"></a>&gt;&gt;&gt; torch.quantize_per_tensor(torch.tensor([-1.0, 0.0, 1.0, 2.0]), 0.1, 10, torch.quint8).int_repr()
<a id="__codelineno-52-5" name="__codelineno-52-5" href="#__codelineno-52-5"></a>tensor([ 0, 10, 20, 30], dtype=torch.uint8)
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-53-1" name="__codelineno-53-1" href="#__codelineno-53-1"></a>torch.quantize_per_channel(input, scales, zero_points, axis, dtype) → Tensor¶
</code></pre></div>
<p>将浮点张量转换为具有给定比例和零点的每通道量化张量。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – float tensor to quantize</p>
</li>
<li>
<p><strong>秤</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–要使用的一维浮标秤，尺寸应匹配<code>input.size(axis)</code></p>
</li>
<li>
<p><strong>zero_points</strong>  (<em>python：int</em> )–要使用的整数 1D 张量偏移量，大小应与<code>input.size(axis)</code>相匹配</p>
</li>
<li>
<p><strong>轴</strong> (<em>python：int</em> )–应用每个通道量化的维度</p>
</li>
<li>
<p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>) – the desired data type of returned tensor. Has to be one of the quantized dtypes: <code>torch.quint8</code>, <code>torch.qint8</code>, <code>torch.qint32</code></p>
</li>
</ul>
<p>Returns</p>
<p>A newly quantized tensor</p>
<p>Return type</p>
<p><a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-54-1" name="__codelineno-54-1" href="#__codelineno-54-1"></a>&gt;&gt;&gt; x = torch.tensor([[-1.0, 0.0], [1.0, 2.0]])
<a id="__codelineno-54-2" name="__codelineno-54-2" href="#__codelineno-54-2"></a>&gt;&gt;&gt; torch.quantize_per_channel(x, torch.tensor([0.1, 0.01]), torch.tensor([10, 0]), 0, torch.quint8)
<a id="__codelineno-54-3" name="__codelineno-54-3" href="#__codelineno-54-3"></a>tensor([[-1.,  0.],
<a id="__codelineno-54-4" name="__codelineno-54-4" href="#__codelineno-54-4"></a>        [ 1.,  2.]], size=(2, 2), dtype=torch.quint8,
<a id="__codelineno-54-5" name="__codelineno-54-5" href="#__codelineno-54-5"></a>       quantization_scheme=torch.per_channel_affine,
<a id="__codelineno-54-6" name="__codelineno-54-6" href="#__codelineno-54-6"></a>       scale=tensor([0.1000, 0.0100], dtype=torch.float64),
<a id="__codelineno-54-7" name="__codelineno-54-7" href="#__codelineno-54-7"></a>       zero_point=tensor([10,  0]), axis=0)
<a id="__codelineno-54-8" name="__codelineno-54-8" href="#__codelineno-54-8"></a>&gt;&gt;&gt; torch.quantize_per_channel(x, torch.tensor([0.1, 0.01]), torch.tensor([10, 0]), 0, torch.quint8).int_repr()
<a id="__codelineno-54-9" name="__codelineno-54-9" href="#__codelineno-54-9"></a>tensor([[  0,  10],
<a id="__codelineno-54-10" name="__codelineno-54-10" href="#__codelineno-54-10"></a>        [100, 200]], dtype=torch.uint8)
</code></pre></div>
<h3 id="_3">索引，切片，联接，操作变更</h3>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-55-1" name="__codelineno-55-1" href="#__codelineno-55-1"></a>torch.cat(tensors, dim=0, out=None) → Tensor¶
</code></pre></div>
<p>在给定维度上连接<code>seq</code>张量的给定序列。 所有张量必须具有相同的形状(在连接维中除外）或为空。</p>
<p><a href="#torch.cat" title="torch.cat"><code>torch.cat()</code></a> 可以看作是 <a href="#torch.split" title="torch.split"><code>torch.split()</code></a> 和 <a href="#torch.chunk" title="torch.chunk"><code>torch.chunk()</code></a> 的逆运算。</p>
<p>通过示例可以更好地理解 <a href="#torch.cat" title="torch.cat"><code>torch.cat()</code></a> 。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>张量</strong>(张量_序列_）–同一类型的任何 python 张量序列。 提供的非空张量必须具有相同的形状，但猫的尺寸除外。</p>
</li>
<li>
<p><strong>暗淡的</strong> (<em>python：int</em> <em>，</em> <em>可选</em>）–张量级联的尺寸</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-56-1" name="__codelineno-56-1" href="#__codelineno-56-1"></a>&gt;&gt;&gt; x = torch.randn(2, 3)
<a id="__codelineno-56-2" name="__codelineno-56-2" href="#__codelineno-56-2"></a>&gt;&gt;&gt; x
<a id="__codelineno-56-3" name="__codelineno-56-3" href="#__codelineno-56-3"></a>tensor([[ 0.6580, -1.0969, -0.4614],
<a id="__codelineno-56-4" name="__codelineno-56-4" href="#__codelineno-56-4"></a>        [-0.1034, -0.5790,  0.1497]])
<a id="__codelineno-56-5" name="__codelineno-56-5" href="#__codelineno-56-5"></a>&gt;&gt;&gt; torch.cat((x, x, x), 0)
<a id="__codelineno-56-6" name="__codelineno-56-6" href="#__codelineno-56-6"></a>tensor([[ 0.6580, -1.0969, -0.4614],
<a id="__codelineno-56-7" name="__codelineno-56-7" href="#__codelineno-56-7"></a>        [-0.1034, -0.5790,  0.1497],
<a id="__codelineno-56-8" name="__codelineno-56-8" href="#__codelineno-56-8"></a>        [ 0.6580, -1.0969, -0.4614],
<a id="__codelineno-56-9" name="__codelineno-56-9" href="#__codelineno-56-9"></a>        [-0.1034, -0.5790,  0.1497],
<a id="__codelineno-56-10" name="__codelineno-56-10" href="#__codelineno-56-10"></a>        [ 0.6580, -1.0969, -0.4614],
<a id="__codelineno-56-11" name="__codelineno-56-11" href="#__codelineno-56-11"></a>        [-0.1034, -0.5790,  0.1497]])
<a id="__codelineno-56-12" name="__codelineno-56-12" href="#__codelineno-56-12"></a>&gt;&gt;&gt; torch.cat((x, x, x), 1)
<a id="__codelineno-56-13" name="__codelineno-56-13" href="#__codelineno-56-13"></a>tensor([[ 0.6580, -1.0969, -0.4614,  0.6580, -1.0969, -0.4614,  0.6580,
<a id="__codelineno-56-14" name="__codelineno-56-14" href="#__codelineno-56-14"></a>         -1.0969, -0.4614],
<a id="__codelineno-56-15" name="__codelineno-56-15" href="#__codelineno-56-15"></a>        [-0.1034, -0.5790,  0.1497, -0.1034, -0.5790,  0.1497, -0.1034,
<a id="__codelineno-56-16" name="__codelineno-56-16" href="#__codelineno-56-16"></a>         -0.5790,  0.1497]])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-57-1" name="__codelineno-57-1" href="#__codelineno-57-1"></a>torch.chunk(input, chunks, dim=0) → List of Tensors¶
</code></pre></div>
<p>将张量拆分为特定数量的块。</p>
<p>如果沿给定维度<code>dim</code>的张量大小不能被<code>chunks</code>整除，则最后一块将较小。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>输入</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–要分割的张量</p>
</li>
<li>
<p><strong>块</strong> (<em>python：int</em> )–要返回的块数</p>
</li>
<li>
<p><strong>暗淡的</strong> (<em>python：int</em> )–沿其张量分裂的尺寸</p>
</li>
</ul>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-58-1" name="__codelineno-58-1" href="#__codelineno-58-1"></a>torch.gather(input, dim, index, out=None, sparse_grad=False) → Tensor¶
</code></pre></div>
<p>沿&lt;cite&gt;昏暗&lt;/cite&gt;指定的轴收集值。</p>
<p>对于 3-D 张量，输出指定为：</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-59-1" name="__codelineno-59-1" href="#__codelineno-59-1"></a>out[i][j][k] = input[index[i][j][k]][j][k]  # if dim == 0
<a id="__codelineno-59-2" name="__codelineno-59-2" href="#__codelineno-59-2"></a>out[i][j][k] = input[i][index[i][j][k]][k]  # if dim == 1
<a id="__codelineno-59-3" name="__codelineno-59-3" href="#__codelineno-59-3"></a>out[i][j][k] = input[i][j][index[i][j][k]]  # if dim == 2
</code></pre></div>
<p>如果<code>input</code>是大小为<img alt="" src="../img/a89ebd4ba33f6e8d4b992302696fcb6b.jpg" />和<code>dim = i</code>的 n 维张量，则<code>index</code>必须是大小为<img alt="" src="../img/0998ac39fb5e46e656f2261d0af181b6.jpg" />的<img alt="" src="../img/5f0051b0454fa75ca446b59b47eff6f6.jpg" />-维张量，其中<img alt="" src="../img/5890f3c556cb9391345ace3ce2c49ff9.jpg" />和<code>out</code>具有相同的大小 大小为<code>index</code>。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>输入</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–源张量</p>
</li>
<li>
<p><strong>暗淡的</strong> (<em>python：int</em> )–沿其索引的轴</p>
</li>
<li>
<p><strong>索引</strong> (<em>LongTensor</em> )–要收集的元素的索引</p>
</li>
<li>
<p><strong>输出</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a> <em>，</em> <em>可选</em>）–目标张量</p>
</li>
<li>
<p><strong>sparse_grad</strong>  (<em>bool</em> <em>，</em> <em>可选</em>）–如果<code>True</code>，则梯度 w.r.t. <code>input</code>将是一个稀疏张量。</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-60-1" name="__codelineno-60-1" href="#__codelineno-60-1"></a>&gt;&gt;&gt; t = torch.tensor([[1,2],[3,4]])
<a id="__codelineno-60-2" name="__codelineno-60-2" href="#__codelineno-60-2"></a>&gt;&gt;&gt; torch.gather(t, 1, torch.tensor([[0,0],[1,0]]))
<a id="__codelineno-60-3" name="__codelineno-60-3" href="#__codelineno-60-3"></a>tensor([[ 1,  1],
<a id="__codelineno-60-4" name="__codelineno-60-4" href="#__codelineno-60-4"></a>        [ 4,  3]])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-61-1" name="__codelineno-61-1" href="#__codelineno-61-1"></a>torch.index_select(input, dim, index, out=None) → Tensor¶
</code></pre></div>
<p>返回一个新张量，该张量使用<code>index</code> LongTensor 中的<code>index</code>中的条目沿维度<code>dim</code>索引<code>input</code>张量。</p>
<p>返回的张量具有与原始张量(<code>input</code>）相同的维数。 <code>dim</code>的尺寸与<code>index</code>的长度相同； 其他尺寸与原始张量中的尺寸相同。</p>
<p>Note</p>
<p>返回的张量不与原始张量使用相同的存储空间<strong>而不是</strong>。 如果<code>out</code>的形状与预期的形状不同，我们将默默地将其更改为正确的形状，并在必要时重新分配基础存储。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>暗淡的</strong> (<em>python：int</em> )–我们索引的维度</p>
</li>
<li>
<p><strong>索引</strong> (<em>LongTensor</em> )–包含要索引的索引的一维张量</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-62-1" name="__codelineno-62-1" href="#__codelineno-62-1"></a>&gt;&gt;&gt; x = torch.randn(3, 4)
<a id="__codelineno-62-2" name="__codelineno-62-2" href="#__codelineno-62-2"></a>&gt;&gt;&gt; x
<a id="__codelineno-62-3" name="__codelineno-62-3" href="#__codelineno-62-3"></a>tensor([[ 0.1427,  0.0231, -0.5414, -1.0009],
<a id="__codelineno-62-4" name="__codelineno-62-4" href="#__codelineno-62-4"></a>        [-0.4664,  0.2647, -0.1228, -1.1068],
<a id="__codelineno-62-5" name="__codelineno-62-5" href="#__codelineno-62-5"></a>        [-1.1734, -0.6571,  0.7230, -0.6004]])
<a id="__codelineno-62-6" name="__codelineno-62-6" href="#__codelineno-62-6"></a>&gt;&gt;&gt; indices = torch.tensor([0, 2])
<a id="__codelineno-62-7" name="__codelineno-62-7" href="#__codelineno-62-7"></a>&gt;&gt;&gt; torch.index_select(x, 0, indices)
<a id="__codelineno-62-8" name="__codelineno-62-8" href="#__codelineno-62-8"></a>tensor([[ 0.1427,  0.0231, -0.5414, -1.0009],
<a id="__codelineno-62-9" name="__codelineno-62-9" href="#__codelineno-62-9"></a>        [-1.1734, -0.6571,  0.7230, -0.6004]])
<a id="__codelineno-62-10" name="__codelineno-62-10" href="#__codelineno-62-10"></a>&gt;&gt;&gt; torch.index_select(x, 1, indices)
<a id="__codelineno-62-11" name="__codelineno-62-11" href="#__codelineno-62-11"></a>tensor([[ 0.1427, -0.5414],
<a id="__codelineno-62-12" name="__codelineno-62-12" href="#__codelineno-62-12"></a>        [-0.4664, -0.1228],
<a id="__codelineno-62-13" name="__codelineno-62-13" href="#__codelineno-62-13"></a>        [-1.1734,  0.7230]])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-63-1" name="__codelineno-63-1" href="#__codelineno-63-1"></a>torch.masked_select(input, mask, out=None) → Tensor¶
</code></pre></div>
<p>返回一个新的一维张量，该张量根据布尔值掩码<code>mask</code>为其 &lt;cite&gt;BoolTensor&lt;/cite&gt; 索引<code>input</code>张量。</p>
<p><code>mask</code>张量和<code>input</code>张量的形状不需要匹配，但它们必须是<a href="notes/broadcasting.html#broadcasting-semantics">可广播的</a>。</p>
<p>Note</p>
<p>返回的张量是否<strong>而不是</strong>使用与原始张量相同的存储</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>掩码</strong> (<em>ByteTensor</em> )–包含二进制掩码的张量，以使用</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-64-1" name="__codelineno-64-1" href="#__codelineno-64-1"></a>&gt;&gt;&gt; x = torch.randn(3, 4)
<a id="__codelineno-64-2" name="__codelineno-64-2" href="#__codelineno-64-2"></a>&gt;&gt;&gt; x
<a id="__codelineno-64-3" name="__codelineno-64-3" href="#__codelineno-64-3"></a>tensor([[ 0.3552, -2.3825, -0.8297,  0.3477],
<a id="__codelineno-64-4" name="__codelineno-64-4" href="#__codelineno-64-4"></a>        [-1.2035,  1.2252,  0.5002,  0.6248],
<a id="__codelineno-64-5" name="__codelineno-64-5" href="#__codelineno-64-5"></a>        [ 0.1307, -2.0608,  0.1244,  2.0139]])
<a id="__codelineno-64-6" name="__codelineno-64-6" href="#__codelineno-64-6"></a>&gt;&gt;&gt; mask = x.ge(0.5)
<a id="__codelineno-64-7" name="__codelineno-64-7" href="#__codelineno-64-7"></a>&gt;&gt;&gt; mask
<a id="__codelineno-64-8" name="__codelineno-64-8" href="#__codelineno-64-8"></a>tensor([[False, False, False, False],
<a id="__codelineno-64-9" name="__codelineno-64-9" href="#__codelineno-64-9"></a>        [False, True, True, True],
<a id="__codelineno-64-10" name="__codelineno-64-10" href="#__codelineno-64-10"></a>        [False, False, False, True]])
<a id="__codelineno-64-11" name="__codelineno-64-11" href="#__codelineno-64-11"></a>&gt;&gt;&gt; torch.masked_select(x, mask)
<a id="__codelineno-64-12" name="__codelineno-64-12" href="#__codelineno-64-12"></a>tensor([ 1.2252,  0.5002,  0.6248,  2.0139])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-65-1" name="__codelineno-65-1" href="#__codelineno-65-1"></a>torch.narrow(input, dim, start, length) → Tensor¶
</code></pre></div>
<p>返回一个新的张量，该张量是<code>input</code>张量的缩小版本。 尺寸<code>dim</code>从<code>start</code>输入到<code>start + length</code>。 返回的张量和<code>input</code>张量共享相同的基础存储。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>输入</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–张量变窄</p>
</li>
<li>
<p><strong>暗淡的</strong> (<em>python：int</em> )–缩小范围</p>
</li>
<li>
<p><strong>开始</strong> (<em>python：int</em> )–起始尺寸</p>
</li>
<li>
<p><strong>长度</strong> (<em>python：int</em> )–到最终尺寸的距离</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-66-1" name="__codelineno-66-1" href="#__codelineno-66-1"></a>&gt;&gt;&gt; x = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
<a id="__codelineno-66-2" name="__codelineno-66-2" href="#__codelineno-66-2"></a>&gt;&gt;&gt; torch.narrow(x, 0, 0, 2)
<a id="__codelineno-66-3" name="__codelineno-66-3" href="#__codelineno-66-3"></a>tensor([[ 1,  2,  3],
<a id="__codelineno-66-4" name="__codelineno-66-4" href="#__codelineno-66-4"></a>        [ 4,  5,  6]])
<a id="__codelineno-66-5" name="__codelineno-66-5" href="#__codelineno-66-5"></a>&gt;&gt;&gt; torch.narrow(x, 1, 1, 2)
<a id="__codelineno-66-6" name="__codelineno-66-6" href="#__codelineno-66-6"></a>tensor([[ 2,  3],
<a id="__codelineno-66-7" name="__codelineno-66-7" href="#__codelineno-66-7"></a>        [ 5,  6],
<a id="__codelineno-66-8" name="__codelineno-66-8" href="#__codelineno-66-8"></a>        [ 8,  9]])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-67-1" name="__codelineno-67-1" href="#__codelineno-67-1"></a>torch.nonzero(input, *, out=None, as_tuple=False) → LongTensor or tuple of LongTensors¶
</code></pre></div>
<p>Note</p>
<p><a href="#torch.nonzero" title="torch.nonzero"><code>torch.nonzero(..., as_tuple=False)</code></a> (默认值）返回一个二维张量，其中每一行都是非零值的索引。</p>
<p><a href="#torch.nonzero" title="torch.nonzero"><code>torch.nonzero(..., as_tuple=True)</code></a> 返回一维索引张量的元组，允许进行高级索引，因此<code>x[x.nonzero(as_tuple=True)]</code>给出张量<code>x</code>的所有非零值。 在返回的元组中，每个索引张量都包含特定维度的非零索引。</p>
<p>有关这两种行为的更多详细信息，请参见下文。</p>
<p><strong>当</strong> <code>as_tuple</code> <strong>为“ False”(默认）</strong>时：</p>
<p>返回一个张量，该张量包含<code>input</code>所有非零元素的索引。 结果中的每一行都包含<code>input</code>中非零元素的索引。 结果按字典顺序排序，最后一个索引更改最快(C 样式）。</p>
<p>如果<code>input</code>具有<img alt="" src="../img/5f0051b0454fa75ca446b59b47eff6f6.jpg" />尺寸，则所得索引张量<code>out</code>的大小为<img alt="" src="../img/b6080a6ed8a7dd471ec6fd4b1023bc23.jpg" />，其中<img alt="" src="../img/e1bf4c09825257a3ffbe6bddb254bcb6.jpg" />是<code>input</code>张量中非零元素的总数。</p>
<p><strong>当</strong> <code>as_tuple</code> <strong>为“ True”</strong> 时：</p>
<p>返回一维张量的元组，在<code>input</code>中每个维度一个张量，每个张量包含<code>input</code>所有非零元素的索引(在该维度中）。</p>
<p>如果<code>input</code>具有<img alt="" src="../img/5f0051b0454fa75ca446b59b47eff6f6.jpg" />尺寸，则生成的元组包含<img alt="" src="../img/e1bf4c09825257a3ffbe6bddb254bcb6.jpg" />大小的<img alt="" src="../img/5f0051b0454fa75ca446b59b47eff6f6.jpg" />张量，其中<img alt="" src="../img/e1bf4c09825257a3ffbe6bddb254bcb6.jpg" />是<code>input</code>张量中非零元素的总数。</p>
<p>作为一种特殊情况，当<code>input</code>具有零维和非零标量值时，会将其视为具有一个元素的一维张量。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>out</strong>  (<em>LongTensor</em> <em>，</em> <em>可选</em>）–包含索引的输出张量</p>
</li>
</ul>
<p>Returns</p>
<p>如果<code>as_tuple</code>为<code>False</code>，则包含索引的输出张量。 如果<code>as_tuple</code>为<code>True</code>，则每个维度都有一个 1-D 张量，其中包含沿着该维度的每个非零元素的索引。</p>
<p>Return type</p>
<p>LongTensor 或 LongTensor 的元组</p>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-68-1" name="__codelineno-68-1" href="#__codelineno-68-1"></a>&gt;&gt;&gt; torch.nonzero(torch.tensor([1, 1, 1, 0, 1]))
<a id="__codelineno-68-2" name="__codelineno-68-2" href="#__codelineno-68-2"></a>tensor([[ 0],
<a id="__codelineno-68-3" name="__codelineno-68-3" href="#__codelineno-68-3"></a>        [ 1],
<a id="__codelineno-68-4" name="__codelineno-68-4" href="#__codelineno-68-4"></a>        [ 2],
<a id="__codelineno-68-5" name="__codelineno-68-5" href="#__codelineno-68-5"></a>        [ 4]])
<a id="__codelineno-68-6" name="__codelineno-68-6" href="#__codelineno-68-6"></a>&gt;&gt;&gt; torch.nonzero(torch.tensor([[0.6, 0.0, 0.0, 0.0],
<a id="__codelineno-68-7" name="__codelineno-68-7" href="#__codelineno-68-7"></a>                                [0.0, 0.4, 0.0, 0.0],
<a id="__codelineno-68-8" name="__codelineno-68-8" href="#__codelineno-68-8"></a>                                [0.0, 0.0, 1.2, 0.0],
<a id="__codelineno-68-9" name="__codelineno-68-9" href="#__codelineno-68-9"></a>                                [0.0, 0.0, 0.0,-0.4]]))
<a id="__codelineno-68-10" name="__codelineno-68-10" href="#__codelineno-68-10"></a>tensor([[ 0,  0],
<a id="__codelineno-68-11" name="__codelineno-68-11" href="#__codelineno-68-11"></a>        [ 1,  1],
<a id="__codelineno-68-12" name="__codelineno-68-12" href="#__codelineno-68-12"></a>        [ 2,  2],
<a id="__codelineno-68-13" name="__codelineno-68-13" href="#__codelineno-68-13"></a>        [ 3,  3]])
<a id="__codelineno-68-14" name="__codelineno-68-14" href="#__codelineno-68-14"></a>&gt;&gt;&gt; torch.nonzero(torch.tensor([1, 1, 1, 0, 1]), as_tuple=True)
<a id="__codelineno-68-15" name="__codelineno-68-15" href="#__codelineno-68-15"></a>(tensor([0, 1, 2, 4]),)
<a id="__codelineno-68-16" name="__codelineno-68-16" href="#__codelineno-68-16"></a>&gt;&gt;&gt; torch.nonzero(torch.tensor([[0.6, 0.0, 0.0, 0.0],
<a id="__codelineno-68-17" name="__codelineno-68-17" href="#__codelineno-68-17"></a>                                [0.0, 0.4, 0.0, 0.0],
<a id="__codelineno-68-18" name="__codelineno-68-18" href="#__codelineno-68-18"></a>                                [0.0, 0.0, 1.2, 0.0],
<a id="__codelineno-68-19" name="__codelineno-68-19" href="#__codelineno-68-19"></a>                                [0.0, 0.0, 0.0,-0.4]]), as_tuple=True)
<a id="__codelineno-68-20" name="__codelineno-68-20" href="#__codelineno-68-20"></a>(tensor([0, 1, 2, 3]), tensor([0, 1, 2, 3]))
<a id="__codelineno-68-21" name="__codelineno-68-21" href="#__codelineno-68-21"></a>&gt;&gt;&gt; torch.nonzero(torch.tensor(5), as_tuple=True)
<a id="__codelineno-68-22" name="__codelineno-68-22" href="#__codelineno-68-22"></a>(tensor([0]),)
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-69-1" name="__codelineno-69-1" href="#__codelineno-69-1"></a>torch.reshape(input, shape) → Tensor¶
</code></pre></div>
<p>返回具有与<code>input</code>相同的数据和元素数量，但具有指定形状的张量。 如果可能，返回的张量将是<code>input</code>的视图。 否则，它将是副本。 连续输入和具有兼容步幅的输入可以在不复制的情况下进行重塑，但是您不应该依赖复制与查看行为。</p>
<p>当可以返回视图时，请参见 <a href="tensors.html#torch.Tensor.view" title="torch.Tensor.view"><code>torch.Tensor.view()</code></a> 。</p>
<p>单个尺寸可能为-1，在这种情况下，它是根据<code>input</code>中的其余尺寸和元素数量推断出来的。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>输入</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–要重塑的张量</p>
</li>
<li>
<p><strong>形状</strong> (<em>python：ints</em> 的元组）–新形状</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-70-1" name="__codelineno-70-1" href="#__codelineno-70-1"></a>&gt;&gt;&gt; a = torch.arange(4.)
<a id="__codelineno-70-2" name="__codelineno-70-2" href="#__codelineno-70-2"></a>&gt;&gt;&gt; torch.reshape(a, (2, 2))
<a id="__codelineno-70-3" name="__codelineno-70-3" href="#__codelineno-70-3"></a>tensor([[ 0.,  1.],
<a id="__codelineno-70-4" name="__codelineno-70-4" href="#__codelineno-70-4"></a>        [ 2.,  3.]])
<a id="__codelineno-70-5" name="__codelineno-70-5" href="#__codelineno-70-5"></a>&gt;&gt;&gt; b = torch.tensor([[0, 1], [2, 3]])
<a id="__codelineno-70-6" name="__codelineno-70-6" href="#__codelineno-70-6"></a>&gt;&gt;&gt; torch.reshape(b, (-1,))
<a id="__codelineno-70-7" name="__codelineno-70-7" href="#__codelineno-70-7"></a>tensor([ 0,  1,  2,  3])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-71-1" name="__codelineno-71-1" href="#__codelineno-71-1"></a>torch.split(tensor, split_size_or_sections, dim=0)¶
</code></pre></div>
<p>将张量拆分为多个块。</p>
<p>如果<code>split_size_or_sections</code>是整数类型，则 <a href="#torch.tensor" title="torch.tensor"><code>tensor</code></a> 将被拆分为大小相等的块(如果可能）。 如果沿给定维度<code>dim</code>的张量大小不能被<code>split_size</code>整除，则最后一个块将较小。</p>
<p>如果<code>split_size_or_sections</code>是列表，则根据<code>split_size_or_sections</code>将 <a href="#torch.tensor" title="torch.tensor"><code>tensor</code></a> 拆分为<code>dim</code>，大小为<code>dim</code>。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>张量</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–张量分裂。</p>
</li>
<li>
<p><strong>split_size_or_sections</strong>  (<em>python：int</em> <em>）或</em> <em>(</em> <em>列表</em> <em>(</em> <em>python ：int</em> <em>）</em>）–单个块的大小或每个块的大小列表</p>
</li>
<li>
<p><strong>暗淡的</strong> (<em>python：int</em> )–沿其张量分裂的尺寸。</p>
</li>
</ul>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-72-1" name="__codelineno-72-1" href="#__codelineno-72-1"></a>torch.squeeze(input, dim=None, out=None) → Tensor¶
</code></pre></div>
<p>返回一个张量，其中所有尺寸为 &lt;cite&gt;1&lt;/cite&gt; 的<code>input</code>尺寸均被删除。</p>
<p>例如，如果&lt;cite&gt;输入&lt;/cite&gt;的形状为：<img alt="" src="../img/1f976021505083151a3b5d3311ab04c2.jpg" />，则张量中的&lt;cite&gt;张量将为：<img alt="" src="../img/7af8285a40441ae3080550e9267b63f8.jpg" />。&lt;/cite&gt;</p>
<p>给定<code>dim</code>时，仅在给定尺寸上执行挤压操作。 如果&lt;cite&gt;输入&lt;/cite&gt;的形状为：<img alt="" src="../img/fb1a3c771fc7be02c64e40a24e873471.jpg" />，<code>squeeze(input, 0)</code>保持张量不变，但是<code>squeeze(input, 1)</code>会将张量压缩为<img alt="" src="../img/e48d9b756847043d64d7565153ad4faf.jpg" />形状。</p>
<p>Note</p>
<p>返回的张量与输入张量共享存储，因此更改一个张量的内容将更改另一个张量的内容。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>暗淡的</strong> (<em>python：int</em> <em>，</em> <em>可选</em>）–如果给定，则仅在此维度上压缩输入</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-73-1" name="__codelineno-73-1" href="#__codelineno-73-1"></a>&gt;&gt;&gt; x = torch.zeros(2, 1, 2, 1, 2)
<a id="__codelineno-73-2" name="__codelineno-73-2" href="#__codelineno-73-2"></a>&gt;&gt;&gt; x.size()
<a id="__codelineno-73-3" name="__codelineno-73-3" href="#__codelineno-73-3"></a>torch.Size([2, 1, 2, 1, 2])
<a id="__codelineno-73-4" name="__codelineno-73-4" href="#__codelineno-73-4"></a>&gt;&gt;&gt; y = torch.squeeze(x)
<a id="__codelineno-73-5" name="__codelineno-73-5" href="#__codelineno-73-5"></a>&gt;&gt;&gt; y.size()
<a id="__codelineno-73-6" name="__codelineno-73-6" href="#__codelineno-73-6"></a>torch.Size([2, 2, 2])
<a id="__codelineno-73-7" name="__codelineno-73-7" href="#__codelineno-73-7"></a>&gt;&gt;&gt; y = torch.squeeze(x, 0)
<a id="__codelineno-73-8" name="__codelineno-73-8" href="#__codelineno-73-8"></a>&gt;&gt;&gt; y.size()
<a id="__codelineno-73-9" name="__codelineno-73-9" href="#__codelineno-73-9"></a>torch.Size([2, 1, 2, 1, 2])
<a id="__codelineno-73-10" name="__codelineno-73-10" href="#__codelineno-73-10"></a>&gt;&gt;&gt; y = torch.squeeze(x, 1)
<a id="__codelineno-73-11" name="__codelineno-73-11" href="#__codelineno-73-11"></a>&gt;&gt;&gt; y.size()
<a id="__codelineno-73-12" name="__codelineno-73-12" href="#__codelineno-73-12"></a>torch.Size([2, 2, 1, 2])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-74-1" name="__codelineno-74-1" href="#__codelineno-74-1"></a>torch.stack(tensors, dim=0, out=None) → Tensor¶
</code></pre></div>
<p>将张量的序列沿新维度连接起来。</p>
<p>所有张量都必须具有相同的大小。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>张量</strong>(<em>张量序列</em>）–连接的张量序列</p>
</li>
<li>
<p><strong>暗淡的</strong> (<em>python：int</em> )–插入的尺寸。 必须介于 0 和级联张量的维数之间(含）</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-75-1" name="__codelineno-75-1" href="#__codelineno-75-1"></a>torch.t(input) → Tensor¶
</code></pre></div>
<p>期望<code>input</code>为&lt; = 2-D 张量，并转置尺寸 0 和 1。</p>
<p>将按原样返回 0-D 和 1-D 张量，并且可以将 2-D 张量视为<code>transpose(input, 0, 1)</code>的简写函数。</p>
<p>Parameters</p>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-76-1" name="__codelineno-76-1" href="#__codelineno-76-1"></a>&gt;&gt;&gt; x = torch.randn(())
<a id="__codelineno-76-2" name="__codelineno-76-2" href="#__codelineno-76-2"></a>&gt;&gt;&gt; x
<a id="__codelineno-76-3" name="__codelineno-76-3" href="#__codelineno-76-3"></a>tensor(0.1995)
<a id="__codelineno-76-4" name="__codelineno-76-4" href="#__codelineno-76-4"></a>&gt;&gt;&gt; torch.t(x)
<a id="__codelineno-76-5" name="__codelineno-76-5" href="#__codelineno-76-5"></a>tensor(0.1995)
<a id="__codelineno-76-6" name="__codelineno-76-6" href="#__codelineno-76-6"></a>&gt;&gt;&gt; x = torch.randn(3)
<a id="__codelineno-76-7" name="__codelineno-76-7" href="#__codelineno-76-7"></a>&gt;&gt;&gt; x
<a id="__codelineno-76-8" name="__codelineno-76-8" href="#__codelineno-76-8"></a>tensor([ 2.4320, -0.4608,  0.7702])
<a id="__codelineno-76-9" name="__codelineno-76-9" href="#__codelineno-76-9"></a>&gt;&gt;&gt; torch.t(x)
<a id="__codelineno-76-10" name="__codelineno-76-10" href="#__codelineno-76-10"></a>tensor([.2.4320,.-0.4608,..0.7702])
<a id="__codelineno-76-11" name="__codelineno-76-11" href="#__codelineno-76-11"></a>&gt;&gt;&gt; x = torch.randn(2, 3)
<a id="__codelineno-76-12" name="__codelineno-76-12" href="#__codelineno-76-12"></a>&gt;&gt;&gt; x
<a id="__codelineno-76-13" name="__codelineno-76-13" href="#__codelineno-76-13"></a>tensor([[ 0.4875,  0.9158, -0.5872],
<a id="__codelineno-76-14" name="__codelineno-76-14" href="#__codelineno-76-14"></a>        [ 0.3938, -0.6929,  0.6932]])
<a id="__codelineno-76-15" name="__codelineno-76-15" href="#__codelineno-76-15"></a>&gt;&gt;&gt; torch.t(x)
<a id="__codelineno-76-16" name="__codelineno-76-16" href="#__codelineno-76-16"></a>tensor([[ 0.4875,  0.3938],
<a id="__codelineno-76-17" name="__codelineno-76-17" href="#__codelineno-76-17"></a>        [ 0.9158, -0.6929],
<a id="__codelineno-76-18" name="__codelineno-76-18" href="#__codelineno-76-18"></a>        [-0.5872,  0.6932]])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-77-1" name="__codelineno-77-1" href="#__codelineno-77-1"></a>torch.take(input, index) → Tensor¶
</code></pre></div>
<p>返回给定索引处带有<code>input</code>元素的新张量。 将输入张量视为视为一维张量。 结果采用与索引相同的形状。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>索引</strong> (<em>LongTensor</em> )–张量索引</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-78-1" name="__codelineno-78-1" href="#__codelineno-78-1"></a>&gt;&gt;&gt; src = torch.tensor([[4, 3, 5],
<a id="__codelineno-78-2" name="__codelineno-78-2" href="#__codelineno-78-2"></a>                        [6, 7, 8]])
<a id="__codelineno-78-3" name="__codelineno-78-3" href="#__codelineno-78-3"></a>&gt;&gt;&gt; torch.take(src, torch.tensor([0, 2, 5]))
<a id="__codelineno-78-4" name="__codelineno-78-4" href="#__codelineno-78-4"></a>tensor([ 4,  5,  8])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-79-1" name="__codelineno-79-1" href="#__codelineno-79-1"></a>torch.transpose(input, dim0, dim1) → Tensor¶
</code></pre></div>
<p>返回一个张量，该张量是<code>input</code>的转置版本。 给定的尺寸<code>dim0</code>和<code>dim1</code>被交换。</p>
<p>产生的<code>out</code>张量与<code>input</code>张量共享其基础存储，因此更改一个内容将更改另一个内容。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>dim0</strong>  (<em>python：int</em> )–要转置的第一个维度</p>
</li>
<li>
<p><strong>dim1</strong>  (<em>python：int</em> )–要转置的第二维</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-80-1" name="__codelineno-80-1" href="#__codelineno-80-1"></a>&gt;&gt;&gt; x = torch.randn(2, 3)
<a id="__codelineno-80-2" name="__codelineno-80-2" href="#__codelineno-80-2"></a>&gt;&gt;&gt; x
<a id="__codelineno-80-3" name="__codelineno-80-3" href="#__codelineno-80-3"></a>tensor([[ 1.0028, -0.9893,  0.5809],
<a id="__codelineno-80-4" name="__codelineno-80-4" href="#__codelineno-80-4"></a>        [-0.1669,  0.7299,  0.4942]])
<a id="__codelineno-80-5" name="__codelineno-80-5" href="#__codelineno-80-5"></a>&gt;&gt;&gt; torch.transpose(x, 0, 1)
<a id="__codelineno-80-6" name="__codelineno-80-6" href="#__codelineno-80-6"></a>tensor([[ 1.0028, -0.1669],
<a id="__codelineno-80-7" name="__codelineno-80-7" href="#__codelineno-80-7"></a>        [-0.9893,  0.7299],
<a id="__codelineno-80-8" name="__codelineno-80-8" href="#__codelineno-80-8"></a>        [ 0.5809,  0.4942]])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-81-1" name="__codelineno-81-1" href="#__codelineno-81-1"></a>torch.unbind(input, dim=0) → seq¶
</code></pre></div>
<p>删除张量尺寸。</p>
<p>返回给定维度上所有切片的元组，已经没有它。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>输入</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–要解除绑定的张量</p>
</li>
<li>
<p><strong>暗淡的</strong> (<em>python：int</em> )–要移除的尺寸</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-82-1" name="__codelineno-82-1" href="#__codelineno-82-1"></a>&gt;&gt;&gt; torch.unbind(torch.tensor([[1, 2, 3],
<a id="__codelineno-82-2" name="__codelineno-82-2" href="#__codelineno-82-2"></a>&gt;&gt;&gt;                            [4, 5, 6],
<a id="__codelineno-82-3" name="__codelineno-82-3" href="#__codelineno-82-3"></a>&gt;&gt;&gt;                            [7, 8, 9]]))
<a id="__codelineno-82-4" name="__codelineno-82-4" href="#__codelineno-82-4"></a>(tensor([1, 2, 3]), tensor([4, 5, 6]), tensor([7, 8, 9]))
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-83-1" name="__codelineno-83-1" href="#__codelineno-83-1"></a>torch.unsqueeze(input, dim, out=None) → Tensor¶
</code></pre></div>
<p>返回在指定位置插入的尺寸为 1 的新张量。</p>
<p>返回的张量与此张量共享相同的基础数据。</p>
<p>可以使用<code>[-input.dim() - 1, input.dim() + 1)</code>范围内的<code>dim</code>值。 负的<code>dim</code>对应于<code>dim</code> = <code>dim + input.dim() + 1</code>处应用的 <a href="#torch.unsqueeze" title="torch.unsqueeze"><code>unsqueeze()</code></a> 。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>暗淡的</strong> (<em>python：int</em> )–插入单例尺寸的索引</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-84-1" name="__codelineno-84-1" href="#__codelineno-84-1"></a>&gt;&gt;&gt; x = torch.tensor([1, 2, 3, 4])
<a id="__codelineno-84-2" name="__codelineno-84-2" href="#__codelineno-84-2"></a>&gt;&gt;&gt; torch.unsqueeze(x, 0)
<a id="__codelineno-84-3" name="__codelineno-84-3" href="#__codelineno-84-3"></a>tensor([[ 1,  2,  3,  4]])
<a id="__codelineno-84-4" name="__codelineno-84-4" href="#__codelineno-84-4"></a>&gt;&gt;&gt; torch.unsqueeze(x, 1)
<a id="__codelineno-84-5" name="__codelineno-84-5" href="#__codelineno-84-5"></a>tensor([[ 1],
<a id="__codelineno-84-6" name="__codelineno-84-6" href="#__codelineno-84-6"></a>        [ 2],
<a id="__codelineno-84-7" name="__codelineno-84-7" href="#__codelineno-84-7"></a>        [ 3],
<a id="__codelineno-84-8" name="__codelineno-84-8" href="#__codelineno-84-8"></a>        [ 4]])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-85-1" name="__codelineno-85-1" href="#__codelineno-85-1"></a>torch.where()¶
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-86-1" name="__codelineno-86-1" href="#__codelineno-86-1"></a>torch.where(condition, x, y) → Tensor
</code></pre></div>
<p>返回从<code>x</code>或<code>y</code>中选择的元素的张量，具体取决于<code>condition</code>。</p>
<p>该操作定义为：</p>
<p><img alt="" src="../img/12ee903e238f296681b1cef26fef0f8f.jpg" /></p>
<p>Note</p>
<p>张量<code>condition</code>，<code>x</code>和<code>y</code>必须是<a href="notes/broadcasting.html#broadcasting-semantics">可广播的</a>。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>条件</strong> (<a href="tensors.html#torch.BoolTensor" title="torch.BoolTensor"><em>BoolTensor</em></a>)–当为 True(非零）时，产生 x，否则产生 y</p>
</li>
<li>
<p><strong>x</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–在<code>condition</code>为<code>True</code>的索引处选择的值</p>
</li>
<li>
<p><strong>y</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–在<code>condition</code>为<code>False</code>的索引处选择的值</p>
</li>
</ul>
<p>Returns</p>
<p>形状张量等于<code>condition</code>，<code>x</code>，<code>y</code>的广播形状</p>
<p>Return type</p>
<p><a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-87-1" name="__codelineno-87-1" href="#__codelineno-87-1"></a>&gt;&gt;&gt; x = torch.randn(3, 2)
<a id="__codelineno-87-2" name="__codelineno-87-2" href="#__codelineno-87-2"></a>&gt;&gt;&gt; y = torch.ones(3, 2)
<a id="__codelineno-87-3" name="__codelineno-87-3" href="#__codelineno-87-3"></a>&gt;&gt;&gt; x
<a id="__codelineno-87-4" name="__codelineno-87-4" href="#__codelineno-87-4"></a>tensor([[-0.4620,  0.3139],
<a id="__codelineno-87-5" name="__codelineno-87-5" href="#__codelineno-87-5"></a>        [ 0.3898, -0.7197],
<a id="__codelineno-87-6" name="__codelineno-87-6" href="#__codelineno-87-6"></a>        [ 0.0478, -0.1657]])
<a id="__codelineno-87-7" name="__codelineno-87-7" href="#__codelineno-87-7"></a>&gt;&gt;&gt; torch.where(x &gt; 0, x, y)
<a id="__codelineno-87-8" name="__codelineno-87-8" href="#__codelineno-87-8"></a>tensor([[ 1.0000,  0.3139],
<a id="__codelineno-87-9" name="__codelineno-87-9" href="#__codelineno-87-9"></a>        [ 0.3898,  1.0000],
<a id="__codelineno-87-10" name="__codelineno-87-10" href="#__codelineno-87-10"></a>        [ 0.0478,  1.0000]])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-88-1" name="__codelineno-88-1" href="#__codelineno-88-1"></a>torch.where(condition) → tuple of LongTensor
</code></pre></div>
<p><code>torch.where(condition)</code>与<code>torch.nonzero(condition, as_tuple=True)</code>相同。</p>
<p>Note</p>
<p>另请参见 <a href="#torch.nonzero" title="torch.nonzero"><code>torch.nonzero()</code></a> 。</p>
<h2 id="_4">发电机</h2>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-89-1" name="__codelineno-89-1" href="#__codelineno-89-1"></a>class torch._C.Generator(device=&#39;cpu&#39;) → Generator¶
</code></pre></div>
<p>创建并返回一个生成器对象，该对象管理产生伪随机数的算法的状态。 在许多<a href="#inplace-random-sampling">就地随机采样</a>函数中用作关键字参数。</p>
<p>Parameters</p>
<p><strong>设备</strong>(<code>torch.device</code>，可选）–生成器所需的设备。</p>
<p>Returns</p>
<p>一个 torch.Generator 对象。</p>
<p>Return type</p>
<p><a href="#torch._C.Generator" title="torch._C.Generator">生成器</a></p>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-90-1" name="__codelineno-90-1" href="#__codelineno-90-1"></a>&gt;&gt;&gt; g_cpu = torch.Generator()
<a id="__codelineno-90-2" name="__codelineno-90-2" href="#__codelineno-90-2"></a>&gt;&gt;&gt; g_cuda = torch.Generator(device=&#39;cuda&#39;)
</code></pre></div>
<div class="highlight"><pre><span></span><code><a id="__codelineno-91-1" name="__codelineno-91-1" href="#__codelineno-91-1"></a>device¶
</code></pre></div>
<p>Generator.device-&gt;设备</p>
<p>获取生成器的当前设备。</p>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-92-1" name="__codelineno-92-1" href="#__codelineno-92-1"></a>&gt;&gt;&gt; g_cpu = torch.Generator()
<a id="__codelineno-92-2" name="__codelineno-92-2" href="#__codelineno-92-2"></a>&gt;&gt;&gt; g_cpu.device
<a id="__codelineno-92-3" name="__codelineno-92-3" href="#__codelineno-92-3"></a>device(type=&#39;cpu&#39;)
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-93-1" name="__codelineno-93-1" href="#__codelineno-93-1"></a>get_state() → Tensor¶
</code></pre></div>
<p>返回生成器状态为<code>torch.ByteTensor</code>。</p>
<p>Returns</p>
<p>一个<code>torch.ByteTensor</code>，其中包含将生成器还原到特定时间点的所有必要位。</p>
<p>Return type</p>
<p><a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-94-1" name="__codelineno-94-1" href="#__codelineno-94-1"></a>&gt;&gt;&gt; g_cpu = torch.Generator()
<a id="__codelineno-94-2" name="__codelineno-94-2" href="#__codelineno-94-2"></a>&gt;&gt;&gt; g_cpu.get_state()
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-95-1" name="__codelineno-95-1" href="#__codelineno-95-1"></a>initial_seed() → int¶
</code></pre></div>
<p>返回用于生成随机数的初始种子。</p>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-96-1" name="__codelineno-96-1" href="#__codelineno-96-1"></a>&gt;&gt;&gt; g_cpu = torch.Generator()
<a id="__codelineno-96-2" name="__codelineno-96-2" href="#__codelineno-96-2"></a>&gt;&gt;&gt; g_cpu.initial_seed()
<a id="__codelineno-96-3" name="__codelineno-96-3" href="#__codelineno-96-3"></a>2147483647
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-97-1" name="__codelineno-97-1" href="#__codelineno-97-1"></a>manual_seed(seed) → Generator¶
</code></pre></div>
<p>设置用于生成随机数的种子。 返回一个&lt;cite&gt;torch.生成器&lt;/cite&gt;对象。 建议设置一个大种子，即一个具有 0 和 1 位平衡的数字。 避免在种子中包含许多 0 位。</p>
<p>Parameters</p>
<p><strong>种子</strong> (<em>python：int</em> )–所需的种子。</p>
<p>Returns</p>
<p>An torch.Generator object.</p>
<p>Return type</p>
<p><a href="#torch._C.Generator" title="torch._C.Generator">Generator</a></p>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-98-1" name="__codelineno-98-1" href="#__codelineno-98-1"></a>&gt;&gt;&gt; g_cpu = torch.Generator()
<a id="__codelineno-98-2" name="__codelineno-98-2" href="#__codelineno-98-2"></a>&gt;&gt;&gt; g_cpu.manual_seed(2147483647)
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-99-1" name="__codelineno-99-1" href="#__codelineno-99-1"></a>seed() → int¶
</code></pre></div>
<p>从 std :: random_device 或当前时间获取不确定的随机数，并将其用作生成器的种子。</p>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-100-1" name="__codelineno-100-1" href="#__codelineno-100-1"></a>&gt;&gt;&gt; g_cpu = torch.Generator()
<a id="__codelineno-100-2" name="__codelineno-100-2" href="#__codelineno-100-2"></a>&gt;&gt;&gt; g_cpu.seed()
<a id="__codelineno-100-3" name="__codelineno-100-3" href="#__codelineno-100-3"></a>1516516984916
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-101-1" name="__codelineno-101-1" href="#__codelineno-101-1"></a>set_state(new_state) → void¶
</code></pre></div>
<p>设置生成器状态。</p>
<p>Parameters</p>
<p><strong>new_state</strong>  (<em>Torch.ByteTensor</em> )–所需状态。</p>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-102-1" name="__codelineno-102-1" href="#__codelineno-102-1"></a>&gt;&gt;&gt; g_cpu = torch.Generator()
<a id="__codelineno-102-2" name="__codelineno-102-2" href="#__codelineno-102-2"></a>&gt;&gt;&gt; g_cpu_other = torch.Generator()
<a id="__codelineno-102-3" name="__codelineno-102-3" href="#__codelineno-102-3"></a>&gt;&gt;&gt; g_cpu.set_state(g_cpu_other.get_state())
</code></pre></div>
<h2 id="_5">随机抽样</h2>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-103-1" name="__codelineno-103-1" href="#__codelineno-103-1"></a>torch.seed()¶
</code></pre></div>
<p>将用于生成随机数的种子设置为不确定的随机数。 返回用于播种 RNG 的 64 位数字。</p>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-104-1" name="__codelineno-104-1" href="#__codelineno-104-1"></a>torch.manual_seed(seed)¶
</code></pre></div>
<p>设置用于生成随机数的种子。 返回一个&lt;cite&gt;torch.生成器&lt;/cite&gt;对象。</p>
<p>Parameters</p>
<p><strong>seed</strong> (<em>python:int</em>) – The desired seed.</p>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-105-1" name="__codelineno-105-1" href="#__codelineno-105-1"></a>torch.initial_seed()¶
</code></pre></div>
<p>返回长为 Python &lt;cite&gt;long&lt;/cite&gt; 的用于生成随机数的初始种子。</p>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-106-1" name="__codelineno-106-1" href="#__codelineno-106-1"></a>torch.get_rng_state()¶
</code></pre></div>
<p>以 &lt;cite&gt;torch.ByteTensor&lt;/cite&gt; 的形式返回随机数生成器状态。</p>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-107-1" name="__codelineno-107-1" href="#__codelineno-107-1"></a>torch.set_rng_state(new_state)¶
</code></pre></div>
<p>设置随机数生成器状态。</p>
<p>Parameters</p>
<p><strong>new_state</strong>  (<em>torch.ByteTensor</em> )–所需状态</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-108-1" name="__codelineno-108-1" href="#__codelineno-108-1"></a>torch.default_generator Returns the default CPU torch.Generator¶
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-109-1" name="__codelineno-109-1" href="#__codelineno-109-1"></a>torch.bernoulli(input, *, generator=None, out=None) → Tensor¶
</code></pre></div>
<p>从伯努利分布中提取二进制随机数(0 或 1）。</p>
<p><code>input</code>张量应为包含用于绘制二进制随机数的概率的张量。 因此，<code>input</code>中的所有值都必须在以下范围内：<img alt="" src="../img/b96a669c7fd5b0f5ea99f4373d6eab2e.jpg" />。</p>
<p>输出张量的<img alt="" src="../img/55a50814a1cfd1ab95d71ca3a7f311fb.jpg" />元素将根据<code>input</code>中给出的<img alt="" src="../img/55a50814a1cfd1ab95d71ca3a7f311fb.jpg" />概率值绘制一个<img alt="" src="../img/21c5bc8d40ee5ab2e18d64aaba8359c7.jpg" />值。</p>
<p><img alt="" src="../img/f1fc132b2ecf782642591904c4f9b7de.jpg" /></p>
<p>返回的<code>out</code>张量仅具有值 0 或 1，并且具有与<code>input</code>相同的形状。</p>
<p><code>out</code>可以具有整数<code>dtype</code>，但是<code>input</code>必须具有浮点<code>dtype</code>。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>输入</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–伯努利分布的概率值的输入张量</p>
</li>
<li>
<p><strong>生成器</strong>(<code>torch.Generator</code>，可选）–用于采样的伪随机数生成器</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-110-1" name="__codelineno-110-1" href="#__codelineno-110-1"></a>&gt;&gt;&gt; a = torch.empty(3, 3).uniform_(0, 1)  # generate a uniform random matrix with range [0, 1]
<a id="__codelineno-110-2" name="__codelineno-110-2" href="#__codelineno-110-2"></a>&gt;&gt;&gt; a
<a id="__codelineno-110-3" name="__codelineno-110-3" href="#__codelineno-110-3"></a>tensor([[ 0.1737,  0.0950,  0.3609],
<a id="__codelineno-110-4" name="__codelineno-110-4" href="#__codelineno-110-4"></a>        [ 0.7148,  0.0289,  0.2676],
<a id="__codelineno-110-5" name="__codelineno-110-5" href="#__codelineno-110-5"></a>        [ 0.9456,  0.8937,  0.7202]])
<a id="__codelineno-110-6" name="__codelineno-110-6" href="#__codelineno-110-6"></a>&gt;&gt;&gt; torch.bernoulli(a)
<a id="__codelineno-110-7" name="__codelineno-110-7" href="#__codelineno-110-7"></a>tensor([[ 1.,  0.,  0.],
<a id="__codelineno-110-8" name="__codelineno-110-8" href="#__codelineno-110-8"></a>        [ 0.,  0.,  0.],
<a id="__codelineno-110-9" name="__codelineno-110-9" href="#__codelineno-110-9"></a>        [ 1.,  1.,  1.]])
<a id="__codelineno-110-10" name="__codelineno-110-10" href="#__codelineno-110-10"></a>
<a id="__codelineno-110-11" name="__codelineno-110-11" href="#__codelineno-110-11"></a>&gt;&gt;&gt; a = torch.ones(3, 3) # probability of drawing &quot;1&quot; is 1
<a id="__codelineno-110-12" name="__codelineno-110-12" href="#__codelineno-110-12"></a>&gt;&gt;&gt; torch.bernoulli(a)
<a id="__codelineno-110-13" name="__codelineno-110-13" href="#__codelineno-110-13"></a>tensor([[ 1.,  1.,  1.],
<a id="__codelineno-110-14" name="__codelineno-110-14" href="#__codelineno-110-14"></a>        [ 1.,  1.,  1.],
<a id="__codelineno-110-15" name="__codelineno-110-15" href="#__codelineno-110-15"></a>        [ 1.,  1.,  1.]])
<a id="__codelineno-110-16" name="__codelineno-110-16" href="#__codelineno-110-16"></a>&gt;&gt;&gt; a = torch.zeros(3, 3) # probability of drawing &quot;1&quot; is 0
<a id="__codelineno-110-17" name="__codelineno-110-17" href="#__codelineno-110-17"></a>&gt;&gt;&gt; torch.bernoulli(a)
<a id="__codelineno-110-18" name="__codelineno-110-18" href="#__codelineno-110-18"></a>tensor([[ 0.,  0.,  0.],
<a id="__codelineno-110-19" name="__codelineno-110-19" href="#__codelineno-110-19"></a>        [ 0.,  0.,  0.],
<a id="__codelineno-110-20" name="__codelineno-110-20" href="#__codelineno-110-20"></a>        [ 0.,  0.,  0.]])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-111-1" name="__codelineno-111-1" href="#__codelineno-111-1"></a>torch.multinomial(input, num_samples, replacement=False, *, generator=None, out=None) → LongTensor¶
</code></pre></div>
<p>返回一个张量，其中每行包含<code>num_samples</code>索引，这些索引是从位于张量<code>input</code>的相应行中的多项式概率分布中采样的。</p>
<p>Note</p>
<p><code>input</code>的行不需要加总为 1(在这种情况下，我们将这些值用作权重），但必须为非负数，有限且总和为非零。</p>
<p>根据每个样本的采样时间，索引从左到右排序(第一个样本放在第一列中）。</p>
<p>如果<code>input</code>是向量，则<code>out</code>是大小<code>num_samples</code>的向量。</p>
<p>如果<code>input</code>是具有 &lt;cite&gt;m&lt;/cite&gt; 行的矩阵，则<code>out</code>是形状<img alt="" src="../img/5d2de3653458e6a14a8a4f0fa87b3ad1.jpg" />的矩阵。</p>
<p>如果替换为<code>True</code>，则抽取样本进行替换。</p>
<p>如果没有，则它们将被替换而不会被绘制，这意味着当为一行绘制样本索引时，无法为该行再次绘制它。</p>
<p>Note</p>
<p>如果绘制时不进行替换，则<code>num_samples</code>必须小于<code>input</code>中非零元素的数目(如果是矩阵，则必须小于<code>input</code>每行中非零元素的最小数目）。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>输入</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–包含概率的输入张量</p>
</li>
<li>
<p><strong>num_samples</strong>  (<em>python：int</em> )–要绘制的样本数</p>
</li>
<li>
<p><strong>替换</strong> (<em>bool</em> <em>，</em> <em>可选</em>）–是否使用替换绘制</p>
</li>
<li>
<p><strong>generator</strong> (<code>torch.Generator</code>, optional) – a pseudorandom number generator for sampling</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-112-1" name="__codelineno-112-1" href="#__codelineno-112-1"></a>&gt;&gt;&gt; weights = torch.tensor([0, 10, 3, 0], dtype=torch.float) # create a tensor of weights
<a id="__codelineno-112-2" name="__codelineno-112-2" href="#__codelineno-112-2"></a>&gt;&gt;&gt; torch.multinomial(weights, 2)
<a id="__codelineno-112-3" name="__codelineno-112-3" href="#__codelineno-112-3"></a>tensor([1, 2])
<a id="__codelineno-112-4" name="__codelineno-112-4" href="#__codelineno-112-4"></a>&gt;&gt;&gt; torch.multinomial(weights, 4) # ERROR!
<a id="__codelineno-112-5" name="__codelineno-112-5" href="#__codelineno-112-5"></a>RuntimeError: invalid argument 2: invalid multinomial distribution (with replacement=False,
<a id="__codelineno-112-6" name="__codelineno-112-6" href="#__codelineno-112-6"></a>not enough non-negative category to sample) at ../aten/src/TH/generic/THTensorRandom.cpp:320
<a id="__codelineno-112-7" name="__codelineno-112-7" href="#__codelineno-112-7"></a>&gt;&gt;&gt; torch.multinomial(weights, 4, replacement=True)
<a id="__codelineno-112-8" name="__codelineno-112-8" href="#__codelineno-112-8"></a>tensor([ 2,  1,  1,  1])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-113-1" name="__codelineno-113-1" href="#__codelineno-113-1"></a>torch.normal()¶
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-114-1" name="__codelineno-114-1" href="#__codelineno-114-1"></a>torch.normal(mean, std, *, generator=None, out=None) → Tensor
</code></pre></div>
<p>返回从均值和标准差给出的独立正态分布中得出的随机数张量。</p>
<p><a href="#torch.mean" title="torch.mean"><code>mean</code></a> 是一个张量，每个输出元素的正态分布均值</p>
<p><a href="#torch.std" title="torch.std"><code>std</code></a> 是一个张量，每个输出元素的正态分布的标准偏差</p>
<p><a href="#torch.mean" title="torch.mean"><code>mean</code></a> 和 <a href="#torch.std" title="torch.std"><code>std</code></a> 的形状不需要匹配，但是每个张量中元素的总数必须相同。</p>
<p>Note</p>
<p>当形状不匹配时，将 <a href="#torch.mean" title="torch.mean"><code>mean</code></a> 的形状用作返回的输出张量的形状</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>均值</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–每个元素均值的张量</p>
</li>
<li>
<p><strong>std</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–每个元素的标准偏差张量</p>
</li>
<li>
<p><strong>generator</strong> (<code>torch.Generator</code>, optional) – a pseudorandom number generator for sampling</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-115-1" name="__codelineno-115-1" href="#__codelineno-115-1"></a>&gt;&gt;&gt; torch.normal(mean=torch.arange(1., 11.), std=torch.arange(1, 0, -0.1))
<a id="__codelineno-115-2" name="__codelineno-115-2" href="#__codelineno-115-2"></a>tensor([  1.0425,   3.5672,   2.7969,   4.2925,   4.7229,   6.2134,
<a id="__codelineno-115-3" name="__codelineno-115-3" href="#__codelineno-115-3"></a>          8.0505,   8.1408,   9.0563,  10.0566])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-116-1" name="__codelineno-116-1" href="#__codelineno-116-1"></a>torch.normal(mean=0.0, std, out=None) → Tensor
</code></pre></div>
<p>与上面的功能相似，但均值在所有绘制的元素之间共享。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>平均值</strong> (<em>python：float</em> <em>，</em> <em>可选</em>）–所有分布的平均值</p>
</li>
<li>
<p><strong>std</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the tensor of per-element standard deviations</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-117-1" name="__codelineno-117-1" href="#__codelineno-117-1"></a>&gt;&gt;&gt; torch.normal(mean=0.5, std=torch.arange(1., 6.))
<a id="__codelineno-117-2" name="__codelineno-117-2" href="#__codelineno-117-2"></a>tensor([-1.2793, -1.0732, -2.0687,  5.1177, -1.2303])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-118-1" name="__codelineno-118-1" href="#__codelineno-118-1"></a>torch.normal(mean, std=1.0, out=None) → Tensor
</code></pre></div>
<p>与上面的函数相似，但是标准偏差在所有绘制的元素之间共享。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>mean</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the tensor of per-element means</p>
</li>
<li>
<p><strong>std</strong>  (<em>python：float</em> <em>，</em> <em>可选</em>）–所有发行版的标准差</p>
</li>
<li>
<p><strong>out</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a> <em>，</em> <em>可选</em>）–输出张量</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-119-1" name="__codelineno-119-1" href="#__codelineno-119-1"></a>&gt;&gt;&gt; torch.normal(mean=torch.arange(1., 6.))
<a id="__codelineno-119-2" name="__codelineno-119-2" href="#__codelineno-119-2"></a>tensor([ 1.1552,  2.6148,  2.6535,  5.8318,  4.2361])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-120-1" name="__codelineno-120-1" href="#__codelineno-120-1"></a>torch.normal(mean, std, size, *, out=None) → Tensor
</code></pre></div>
<p>与上述功能相似，但均值和标准差在所有绘制的元素之间共享。 所得张量的大小由<code>size</code>给出。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>平均值</strong> (<em>python：float</em> )–所有分布的平均值</p>
</li>
<li>
<p><strong>std</strong>  (<em>python：float</em> )–所有分布的标准偏差</p>
</li>
<li>
<p><strong>大小</strong> (<em>python：int ...</em> )–定义输出张量形状的整数序列。</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-121-1" name="__codelineno-121-1" href="#__codelineno-121-1"></a>&gt;&gt;&gt; torch.normal(2, 3, size=(1, 4))
<a id="__codelineno-121-2" name="__codelineno-121-2" href="#__codelineno-121-2"></a>tensor([[-1.3987, -1.9544,  3.6048,  0.7909]])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-122-1" name="__codelineno-122-1" href="#__codelineno-122-1"></a>torch.rand(*size, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) → Tensor¶
</code></pre></div>
<p>从区间<img alt="" src="../img/22e975ba7fcff9173338360452b96179.jpg" />返回均匀分布的随机张量</p>
<p>张量的形状由变量参数<code>size</code>定义。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>size</strong> (<em>python:int...</em>) – a sequence of integers defining the shape of the output tensor. Can be a variable number of arguments or a collection like a list or tuple.</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
<li>
<p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) – the desired data type of returned tensor. Default: if <code>None</code>, uses a global default (see <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>).</p>
</li>
<li>
<p><strong>layout</strong> (<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code>torch.layout</code></a>, optional) – the desired layout of returned Tensor. Default: <code>torch.strided</code>.</p>
</li>
<li>
<p><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) – the desired device of returned tensor. Default: if <code>None</code>, uses the current device for the default tensor type (see <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>). <code>device</code> will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.</p>
</li>
<li>
<p><strong>requires_grad</strong> (<em>bool__,</em> <em>optional</em>) – If autograd should record operations on the returned tensor. Default: <code>False</code>.</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-123-1" name="__codelineno-123-1" href="#__codelineno-123-1"></a>&gt;&gt;&gt; torch.rand(4)
<a id="__codelineno-123-2" name="__codelineno-123-2" href="#__codelineno-123-2"></a>tensor([ 0.5204,  0.2503,  0.3525,  0.5673])
<a id="__codelineno-123-3" name="__codelineno-123-3" href="#__codelineno-123-3"></a>&gt;&gt;&gt; torch.rand(2, 3)
<a id="__codelineno-123-4" name="__codelineno-123-4" href="#__codelineno-123-4"></a>tensor([[ 0.8237,  0.5781,  0.6879],
<a id="__codelineno-123-5" name="__codelineno-123-5" href="#__codelineno-123-5"></a>        [ 0.3816,  0.7249,  0.0998]])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-124-1" name="__codelineno-124-1" href="#__codelineno-124-1"></a>torch.rand_like(input, dtype=None, layout=None, device=None, requires_grad=False) → Tensor¶
</code></pre></div>
<p>返回与<code>input</code>大小相同的张量，该张量由间隔<img alt="" src="../img/22e975ba7fcff9173338360452b96179.jpg" />上均匀分布的随机数填充。 <code>torch.rand_like(input)</code>等效于<code>torch.rand(input.size(), dtype=input.dtype, layout=input.layout, device=input.device)</code>。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the size of <code>input</code> will determine size of the output tensor.</p>
</li>
<li>
<p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) – the desired data type of returned Tensor. Default: if <code>None</code>, defaults to the dtype of <code>input</code>.</p>
</li>
<li>
<p><strong>layout</strong> (<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code>torch.layout</code></a>, optional) – the desired layout of returned tensor. Default: if <code>None</code>, defaults to the layout of <code>input</code>.</p>
</li>
<li>
<p><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) – the desired device of returned tensor. Default: if <code>None</code>, defaults to the device of <code>input</code>.</p>
</li>
<li>
<p><strong>requires_grad</strong> (<em>bool__,</em> <em>optional</em>) – If autograd should record operations on the returned tensor. Default: <code>False</code>.</p>
</li>
</ul>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-125-1" name="__codelineno-125-1" href="#__codelineno-125-1"></a>torch.randint(low=0, high, size, *, generator=None, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) → Tensor¶
</code></pre></div>
<p>返回一个由在<code>low</code>(包括）和<code>high</code>(不包括）之间均匀生成的随机整数填充的张量。</p>
<p>The shape of the tensor is defined by the variable argument <code>size</code>.</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>低</strong> (<em>python：int</em> <em>，</em> <em>可选</em>）–从分布中得出的最低整数。 默认值：0</p>
</li>
<li>
<p><strong>高</strong> (<em>python：int</em> )–从分布中得出的最高整数之上一个。</p>
</li>
<li>
<p><strong>大小</strong>(<em>元组</em>）–定义输出张量形状的元组。</p>
</li>
<li>
<p><strong>generator</strong> (<code>torch.Generator</code>, optional) – a pseudorandom number generator for sampling</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
<li>
<p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) – the desired data type of returned tensor. Default: if <code>None</code>, uses a global default (see <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>).</p>
</li>
<li>
<p><strong>layout</strong> (<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code>torch.layout</code></a>, optional) – the desired layout of returned Tensor. Default: <code>torch.strided</code>.</p>
</li>
<li>
<p><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) – the desired device of returned tensor. Default: if <code>None</code>, uses the current device for the default tensor type (see <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>). <code>device</code> will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.</p>
</li>
<li>
<p><strong>requires_grad</strong> (<em>bool__,</em> <em>optional</em>) – If autograd should record operations on the returned tensor. Default: <code>False</code>.</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-126-1" name="__codelineno-126-1" href="#__codelineno-126-1"></a>&gt;&gt;&gt; torch.randint(3, 5, (3,))
<a id="__codelineno-126-2" name="__codelineno-126-2" href="#__codelineno-126-2"></a>tensor([4, 3, 4])
<a id="__codelineno-126-3" name="__codelineno-126-3" href="#__codelineno-126-3"></a>
<a id="__codelineno-126-4" name="__codelineno-126-4" href="#__codelineno-126-4"></a>&gt;&gt;&gt; torch.randint(10, (2, 2))
<a id="__codelineno-126-5" name="__codelineno-126-5" href="#__codelineno-126-5"></a>tensor([[0, 2],
<a id="__codelineno-126-6" name="__codelineno-126-6" href="#__codelineno-126-6"></a>        [5, 5]])
<a id="__codelineno-126-7" name="__codelineno-126-7" href="#__codelineno-126-7"></a>
<a id="__codelineno-126-8" name="__codelineno-126-8" href="#__codelineno-126-8"></a>&gt;&gt;&gt; torch.randint(3, 10, (2, 2))
<a id="__codelineno-126-9" name="__codelineno-126-9" href="#__codelineno-126-9"></a>tensor([[4, 5],
<a id="__codelineno-126-10" name="__codelineno-126-10" href="#__codelineno-126-10"></a>        [6, 7]])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-127-1" name="__codelineno-127-1" href="#__codelineno-127-1"></a>torch.randint_like(input, low=0, high, dtype=None, layout=torch.strided, device=None, requires_grad=False) → Tensor¶
</code></pre></div>
<p>返回具有与张量<code>input</code>相同形状的张量，其中填充了在<code>low</code>(包括）和<code>high</code>(排除）之间均匀生成的随机整数。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the size of <code>input</code> will determine size of the output tensor.</p>
</li>
<li>
<p><strong>low</strong> (<em>python:int__,</em> <em>optional</em>) – Lowest integer to be drawn from the distribution. Default: 0.</p>
</li>
<li>
<p><strong>high</strong> (<em>python:int</em>) – One above the highest integer to be drawn from the distribution.</p>
</li>
<li>
<p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) – the desired data type of returned Tensor. Default: if <code>None</code>, defaults to the dtype of <code>input</code>.</p>
</li>
<li>
<p><strong>layout</strong> (<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code>torch.layout</code></a>, optional) – the desired layout of returned tensor. Default: if <code>None</code>, defaults to the layout of <code>input</code>.</p>
</li>
<li>
<p><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) – the desired device of returned tensor. Default: if <code>None</code>, defaults to the device of <code>input</code>.</p>
</li>
<li>
<p><strong>requires_grad</strong> (<em>bool__,</em> <em>optional</em>) – If autograd should record operations on the returned tensor. Default: <code>False</code>.</p>
</li>
</ul>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-128-1" name="__codelineno-128-1" href="#__codelineno-128-1"></a>torch.randn(*size, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) → Tensor¶
</code></pre></div>
<p>从平均值为 &lt;cite&gt;0&lt;/cite&gt; ，方差为 &lt;cite&gt;1&lt;/cite&gt; 的正态分布中返回一个填充有随机数的张量(也称为标准正态分布）。</p>
<p><img alt="" src="../img/6a5aeab1deaf496af03eb65c0690d32b.jpg" /></p>
<p>The shape of the tensor is defined by the variable argument <code>size</code>.</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>size</strong> (<em>python:int...</em>) – a sequence of integers defining the shape of the output tensor. Can be a variable number of arguments or a collection like a list or tuple.</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
<li>
<p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) – the desired data type of returned tensor. Default: if <code>None</code>, uses a global default (see <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>).</p>
</li>
<li>
<p><strong>layout</strong> (<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code>torch.layout</code></a>, optional) – the desired layout of returned Tensor. Default: <code>torch.strided</code>.</p>
</li>
<li>
<p><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) – the desired device of returned tensor. Default: if <code>None</code>, uses the current device for the default tensor type (see <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>). <code>device</code> will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.</p>
</li>
<li>
<p><strong>requires_grad</strong> (<em>bool__,</em> <em>optional</em>) – If autograd should record operations on the returned tensor. Default: <code>False</code>.</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-129-1" name="__codelineno-129-1" href="#__codelineno-129-1"></a>&gt;&gt;&gt; torch.randn(4)
<a id="__codelineno-129-2" name="__codelineno-129-2" href="#__codelineno-129-2"></a>tensor([-2.1436,  0.9966,  2.3426, -0.6366])
<a id="__codelineno-129-3" name="__codelineno-129-3" href="#__codelineno-129-3"></a>&gt;&gt;&gt; torch.randn(2, 3)
<a id="__codelineno-129-4" name="__codelineno-129-4" href="#__codelineno-129-4"></a>tensor([[ 1.5954,  2.8929, -1.0923],
<a id="__codelineno-129-5" name="__codelineno-129-5" href="#__codelineno-129-5"></a>        [ 1.1719, -0.4709, -0.1996]])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-130-1" name="__codelineno-130-1" href="#__codelineno-130-1"></a>torch.randn_like(input, dtype=None, layout=None, device=None, requires_grad=False) → Tensor¶
</code></pre></div>
<p>返回一个与<code>input</code>相同大小的张量，该张量由均值 0 和方差 1 的正态分布的随机数填充。<code>torch.randn_like(input)</code>等效于<code>torch.randn(input.size(), dtype=input.dtype, layout=input.layout, device=input.device)</code>。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the size of <code>input</code> will determine size of the output tensor.</p>
</li>
<li>
<p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) – the desired data type of returned Tensor. Default: if <code>None</code>, defaults to the dtype of <code>input</code>.</p>
</li>
<li>
<p><strong>layout</strong> (<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code>torch.layout</code></a>, optional) – the desired layout of returned tensor. Default: if <code>None</code>, defaults to the layout of <code>input</code>.</p>
</li>
<li>
<p><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) – the desired device of returned tensor. Default: if <code>None</code>, defaults to the device of <code>input</code>.</p>
</li>
<li>
<p><strong>requires_grad</strong> (<em>bool__,</em> <em>optional</em>) – If autograd should record operations on the returned tensor. Default: <code>False</code>.</p>
</li>
</ul>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-131-1" name="__codelineno-131-1" href="#__codelineno-131-1"></a>torch.randperm(n, out=None, dtype=torch.int64, layout=torch.strided, device=None, requires_grad=False) → LongTensor¶
</code></pre></div>
<p>返回从<code>0</code>到<code>n - 1</code>的整数的随机排列。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>n</strong>  (<em>python：int</em> )–上限(不包括）</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
<li>
<p><strong>dtype</strong>  (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a> ，可选）–返回张量的所需数据类型。 默认值：<code>torch.int64</code>。</p>
</li>
<li>
<p><strong>layout</strong> (<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code>torch.layout</code></a>, optional) – the desired layout of returned Tensor. Default: <code>torch.strided</code>.</p>
</li>
<li>
<p><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) – the desired device of returned tensor. Default: if <code>None</code>, uses the current device for the default tensor type (see <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>). <code>device</code> will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.</p>
</li>
<li>
<p><strong>requires_grad</strong> (<em>bool__,</em> <em>optional</em>) – If autograd should record operations on the returned tensor. Default: <code>False</code>.</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-132-1" name="__codelineno-132-1" href="#__codelineno-132-1"></a>&gt;&gt;&gt; torch.randperm(4)
<a id="__codelineno-132-2" name="__codelineno-132-2" href="#__codelineno-132-2"></a>tensor([2, 1, 0, 3])
</code></pre></div>
<h3 id="_6">就地随机抽样</h3>
<p>在 Tensor 上还定义了一些就地随机采样函数。 单击以查看其文档：</p>
<ul>
<li>
<p><a href="tensors.html#torch.Tensor.bernoulli_" title="torch.Tensor.bernoulli_"><code>torch.Tensor.bernoulli_()</code></a> - <a href="#torch.bernoulli" title="torch.bernoulli"><code>torch.bernoulli()</code></a> 的就地版本</p>
</li>
<li>
<p><a href="tensors.html#torch.Tensor.cauchy_" title="torch.Tensor.cauchy_"><code>torch.Tensor.cauchy_()</code></a> -从柯西分布中得出的数字</p>
</li>
<li>
<p><a href="tensors.html#torch.Tensor.exponential_" title="torch.Tensor.exponential_"><code>torch.Tensor.exponential_()</code></a> -从指数分布中得出的数字</p>
</li>
<li>
<p><a href="tensors.html#torch.Tensor.geometric_" title="torch.Tensor.geometric_"><code>torch.Tensor.geometric_()</code></a> -从几何分布中绘制的元素</p>
</li>
<li>
<p><a href="tensors.html#torch.Tensor.log_normal_" title="torch.Tensor.log_normal_"><code>torch.Tensor.log_normal_()</code></a> -来自对数正态分布的样本</p>
</li>
<li>
<p><a href="tensors.html#torch.Tensor.normal_" title="torch.Tensor.normal_"><code>torch.Tensor.normal_()</code></a> - <a href="#torch.normal" title="torch.normal"><code>torch.normal()</code></a> 的就地版本</p>
</li>
<li>
<p><a href="tensors.html#torch.Tensor.random_" title="torch.Tensor.random_"><code>torch.Tensor.random_()</code></a> -从离散均匀分布中采样的数字</p>
</li>
<li>
<p><a href="tensors.html#torch.Tensor.uniform_" title="torch.Tensor.uniform_"><code>torch.Tensor.uniform_()</code></a> -从连续均匀分布中采样的数字</p>
</li>
</ul>
<h3 id="_7">准随机抽样</h3>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-133-1" name="__codelineno-133-1" href="#__codelineno-133-1"></a>class torch.quasirandom.SobolEngine(dimension, scramble=False, seed=None)¶
</code></pre></div>
<p><a href="#torch.quasirandom.SobolEngine" title="torch.quasirandom.SobolEngine"><code>torch.quasirandom.SobolEngine</code></a> 是用于生成(加扰）Sobol 序列的引擎。 Sobol 序列是低差异准随机序列的一个示例。</p>
<p>用于 Sobol 序列的引擎的这种实现方式能够对最大维度为 1111 的序列进行采样。它使用方向编号生成这些序列，并且这些编号已从<a href="http://web.maths.unsw.edu.au/~fkuo/sobol/joe-kuo-old.1111">此处</a>改编而来。</p>
<p>参考文献</p>
<ul>
<li>
<p>Art B. Owen。 争夺 Sobol 和 Niederreiter-Xing 点。 复杂性杂志，14(4）：466-489，1998 年 12 月。</p>
</li>
<li>
<p>I. M. Sobol。 立方体中点的分布和积分的准确评估。 嗯 Vychisl。 垫。 我在。 Phys。，7：784-802，1967。</p>
</li>
</ul>
<p>Parameters</p>
<ul>
<li>
<p><strong>尺寸</strong> (<em>Int</em> )–要绘制的序列的尺寸</p>
</li>
<li>
<p><strong>扰乱</strong> (<em>bool</em> <em>，</em> <em>可选</em>）–将其设置为<code>True</code>将产生扰乱的 Sobol 序列。 加扰能够产生更好的 Sobol 序列。 默认值：<code>False</code>。</p>
</li>
<li>
<p><strong>种子</strong> (<em>Int</em> <em>，</em> <em>可选</em>）–这是加扰的种子。 如果指定，则将随机数生成器的种子设置为此。 否则，它将使用随机种子。 默认值：<code>None</code></p>
</li>
</ul>
<p>例子：</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-134-1" name="__codelineno-134-1" href="#__codelineno-134-1"></a>&gt;&gt;&gt; soboleng = torch.quasirandom.SobolEngine(dimension=5)
<a id="__codelineno-134-2" name="__codelineno-134-2" href="#__codelineno-134-2"></a>&gt;&gt;&gt; soboleng.draw(3)
<a id="__codelineno-134-3" name="__codelineno-134-3" href="#__codelineno-134-3"></a>tensor([[0.5000, 0.5000, 0.5000, 0.5000, 0.5000],
<a id="__codelineno-134-4" name="__codelineno-134-4" href="#__codelineno-134-4"></a>        [0.7500, 0.2500, 0.7500, 0.2500, 0.7500],
<a id="__codelineno-134-5" name="__codelineno-134-5" href="#__codelineno-134-5"></a>        [0.2500, 0.7500, 0.2500, 0.7500, 0.2500]])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-135-1" name="__codelineno-135-1" href="#__codelineno-135-1"></a>draw(n=1, out=None, dtype=torch.float32)¶
</code></pre></div>
<p>从 Sobol 序列中绘制<code>n</code>点序列的功能。 请注意，样本取决于先前的样本。 结果的大小为<img alt="" src="../img/4a928fac0225cfecb1270d6afba7caf9.jpg" />。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>n</strong>  (<em>Int</em> <em>，</em> <em>可选</em>）–绘制点序列的长度。 默认值：1</p>
</li>
<li>
<p><strong>out</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a> <em>，</em> <em>可选</em>）–输出张量</p>
</li>
<li>
<p><strong>dtype</strong> (<code>torch.dtype</code>，可选）–返回的张量的所需数据类型。 默认值：<code>torch.float32</code></p>
</li>
</ul>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-136-1" name="__codelineno-136-1" href="#__codelineno-136-1"></a>fast_forward(n)¶
</code></pre></div>
<p>通过<code>n</code>步骤快速前进<code>SobolEngine</code>状态的功能。 这等效于不使用样本绘制<code>n</code>样本。</p>
<p>Parameters</p>
<p><strong>n</strong>  (<em>Int</em> )–快进的步数。</p>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-137-1" name="__codelineno-137-1" href="#__codelineno-137-1"></a>reset()¶
</code></pre></div>
<p>将<code>SobolEngine</code>重置为基本状态的功能。</p>
<h2 id="_8">序列化</h2>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-138-1" name="__codelineno-138-1" href="#__codelineno-138-1"></a>torch.save(obj, f, pickle_module=&lt;module &#39;pickle&#39; from &#39;/opt/conda/lib/python3.6/pickle.py&#39;&gt;, pickle_protocol=2, _use_new_zipfile_serialization=False)¶
</code></pre></div>
<p>将对象保存到磁盘文件。</p>
<p>另请参见：<a href="notes/serialization.html#recommend-saving-models">推荐的模型保存方法</a></p>
<p>Parameters</p>
<ul>
<li>
<p><strong>obj</strong> –保存的对象</p>
</li>
<li>
<p><strong>f</strong> –类似于文件的对象(必须实现写入和刷新）或包含文件名的字符串</p>
</li>
<li>
<p><strong>pickle_module</strong> –用于腌制元数据和对象的模块</p>
</li>
<li>
<p><strong>pickle_protocol</strong> –可以指定为覆盖默认协议</p>
</li>
</ul>
<p>Warning</p>
<p>如果使用的是 Python 2，则 <a href="#torch.save" title="torch.save"><code>torch.save()</code></a> 不支持<code>StringIO.StringIO</code>作为有效的类似文件的对象。 这是因为 write 方法应返回写入的字节数； <code>StringIO.write()</code>不这样做。</p>
<p>请改用<code>io.BytesIO</code>之类的东西。</p>
<p>例</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-139-1" name="__codelineno-139-1" href="#__codelineno-139-1"></a>&gt;&gt;&gt; # Save to file
<a id="__codelineno-139-2" name="__codelineno-139-2" href="#__codelineno-139-2"></a>&gt;&gt;&gt; x = torch.tensor([0, 1, 2, 3, 4])
<a id="__codelineno-139-3" name="__codelineno-139-3" href="#__codelineno-139-3"></a>&gt;&gt;&gt; torch.save(x, &#39;tensor.pt&#39;)
<a id="__codelineno-139-4" name="__codelineno-139-4" href="#__codelineno-139-4"></a>&gt;&gt;&gt; # Save to io.BytesIO buffer
<a id="__codelineno-139-5" name="__codelineno-139-5" href="#__codelineno-139-5"></a>&gt;&gt;&gt; buffer = io.BytesIO()
<a id="__codelineno-139-6" name="__codelineno-139-6" href="#__codelineno-139-6"></a>&gt;&gt;&gt; torch.save(x, buffer)
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-140-1" name="__codelineno-140-1" href="#__codelineno-140-1"></a>torch.load(f, map_location=None, pickle_module=&lt;module &#39;pickle&#39; from &#39;/opt/conda/lib/python3.6/pickle.py&#39;&gt;, **pickle_load_args)¶
</code></pre></div>
<p>从文件加载用 <a href="#torch.save" title="torch.save"><code>torch.save()</code></a> 保存的对象。</p>
<p><a href="#torch.load" title="torch.load"><code>torch.load()</code></a> 使用 Python 的解开工具，但会特别处理位于张量之下的存储。 它们首先在 CPU 上反序列化，然后移到保存它们的设备上。 如果失败(例如，因为运行时系统没有某些设备），则会引发异常。 但是，可以使用<code>map_location</code>参数将存储动态重新映射到一组备用设备。</p>
<p>如果<code>map_location</code>是可调用的，则将为每个序列化存储调用一次，并带有两个参数：storage 和 location。 storage 参数将是驻留在 CPU 上的存储的初始反序列化。 每个序列化存储都有一个与之关联的位置标签，该标签标识了从中进行保存的设备，该标签是传递给<code>map_location</code>的第二个参数。 内置位置标签是用于 CPU 张量的<code>'cpu'</code>和用于 CUDA 张量的<code>'cuda:device_id'</code>(例如<code>'cuda:2'</code>）。 <code>map_location</code>应该返回<code>None</code>或存储。 如果<code>map_location</code>返回存储，它将用作最终反序列化的对象，已经移至正确的设备。 否则， <a href="#torch.load" title="torch.load"><code>torch.load()</code></a> 将退回到默认行为，就像未指定<code>map_location</code>一样。</p>
<p>如果<code>map_location</code>是 <a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a> 对象或与设备标签冲突的字符串，则它指示应加载所有张量的位置。</p>
<p>否则，如果<code>map_location</code>是字典，它将用于将文件(键）中出现的位置标签重新映射到指定将存储位置(值）放置的位置标签。</p>
<p>用户扩展可以使用<code>torch.serialization.register_package()</code>注册自己的位置标签以及标记和反序列化方法。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>f</strong> –类似于文件的对象(必须实现<code>read()</code>，：meth<code>readline</code>，：meth<code>tell</code>和：meth<code>seek</code>）或包含文件名的字符串</p>
</li>
<li>
<p><strong>map_location</strong> –函数， <a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a> ，字符串或指定如何重新映射存储位置的字典</p>
</li>
<li>
<p><strong>pickle_module</strong> –用于解开元数据和对象的模块(必须与用于序列化文件的<code>pickle_module</code>匹配）</p>
</li>
<li>
<p><strong>pickle_load_args</strong> –(仅适用于 Python 3）可选关键字参数传递给<code>pickle_module.load()</code>和<code>pickle_module.Unpickler()</code>，例如<code>errors=...</code>。</p>
</li>
</ul>
<p>Note</p>
<p>当您在包含 GPU 张量的文件上调用 <a href="#torch.load" title="torch.load"><code>torch.load()</code></a> 时，这些张量将默认加载到 GPU。 您可以先调用<code>torch.load(.., map_location='cpu')</code>，然后再调用<code>load_state_dict()</code>，以避免在加载模型检查点时 GPU RAM 激增。</p>
<p>Note</p>
<p>默认情况下，我们将字节字符串解码为<code>utf-8</code>。 这是为了避免在 Python 3 中加载 Python 2 保存的文件时出现常见错误情况<code>UnicodeDecodeError: 'ascii' codec can't decode byte 0x...</code>。如果此默认设置不正确，则可以使用额外的<code>encoding</code>关键字参数来指定应如何加载这些对象，例如<code>encoding='latin1'</code>使用<code>latin1</code>编码将它们解码为字符串，<code>encoding='bytes'</code>将它们保留为字节数组，以后可以使用<code>byte_array.decode(...)</code>进行解码。</p>
<p>Example</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-141-1" name="__codelineno-141-1" href="#__codelineno-141-1"></a>&gt;&gt;&gt; torch.load(&#39;tensors.pt&#39;)
<a id="__codelineno-141-2" name="__codelineno-141-2" href="#__codelineno-141-2"></a># Load all tensors onto the CPU
<a id="__codelineno-141-3" name="__codelineno-141-3" href="#__codelineno-141-3"></a>&gt;&gt;&gt; torch.load(&#39;tensors.pt&#39;, map_location=torch.device(&#39;cpu&#39;))
<a id="__codelineno-141-4" name="__codelineno-141-4" href="#__codelineno-141-4"></a># Load all tensors onto the CPU, using a function
<a id="__codelineno-141-5" name="__codelineno-141-5" href="#__codelineno-141-5"></a>&gt;&gt;&gt; torch.load(&#39;tensors.pt&#39;, map_location=lambda storage, loc: storage)
<a id="__codelineno-141-6" name="__codelineno-141-6" href="#__codelineno-141-6"></a># Load all tensors onto GPU 1
<a id="__codelineno-141-7" name="__codelineno-141-7" href="#__codelineno-141-7"></a>&gt;&gt;&gt; torch.load(&#39;tensors.pt&#39;, map_location=lambda storage, loc: storage.cuda(1))
<a id="__codelineno-141-8" name="__codelineno-141-8" href="#__codelineno-141-8"></a># Map tensors from GPU 1 to GPU 0
<a id="__codelineno-141-9" name="__codelineno-141-9" href="#__codelineno-141-9"></a>&gt;&gt;&gt; torch.load(&#39;tensors.pt&#39;, map_location={&#39;cuda:1&#39;:&#39;cuda:0&#39;})
<a id="__codelineno-141-10" name="__codelineno-141-10" href="#__codelineno-141-10"></a># Load tensor from io.BytesIO object
<a id="__codelineno-141-11" name="__codelineno-141-11" href="#__codelineno-141-11"></a>&gt;&gt;&gt; with open(&#39;tensor.pt&#39;, &#39;rb&#39;) as f:
<a id="__codelineno-141-12" name="__codelineno-141-12" href="#__codelineno-141-12"></a>        buffer = io.BytesIO(f.read())
<a id="__codelineno-141-13" name="__codelineno-141-13" href="#__codelineno-141-13"></a>&gt;&gt;&gt; torch.load(buffer)
<a id="__codelineno-141-14" name="__codelineno-141-14" href="#__codelineno-141-14"></a># Load a module with &#39;ascii&#39; encoding for unpickling
<a id="__codelineno-141-15" name="__codelineno-141-15" href="#__codelineno-141-15"></a>&gt;&gt;&gt; torch.load(&#39;module.pt&#39;, encoding=&#39;ascii&#39;)
</code></pre></div>
<h2 id="_9">平行性</h2>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-142-1" name="__codelineno-142-1" href="#__codelineno-142-1"></a>torch.get_num_threads() → int¶
</code></pre></div>
<p>返回用于并行化 CPU 操作的线程数</p>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-143-1" name="__codelineno-143-1" href="#__codelineno-143-1"></a>torch.set_num_threads(int)¶
</code></pre></div>
<p>设置用于 CPU 上的内部运算并行的线程数。 警告：为确保使用正确的线程数，必须在运行 eager，JIT 或 autograd 代码之前调用 set_num_threads。</p>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-144-1" name="__codelineno-144-1" href="#__codelineno-144-1"></a>torch.get_num_interop_threads() → int¶
</code></pre></div>
<p>返回用于 CPU 上的互操作并行的线程数(例如，在 JIT 解释器中）</p>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-145-1" name="__codelineno-145-1" href="#__codelineno-145-1"></a>torch.set_num_interop_threads(int)¶
</code></pre></div>
<p>设置用于 CPU 上的互操作并行性(例如，在 JIT 解释器中）的线程数。 警告：只能在一次操作间并行工作开始之前(例如 JIT 执行）调用一次。</p>
<h2 id="_10">局部禁用梯度计算</h2>
<p>上下文管理器<code>torch.no_grad()</code>，<code>torch.enable_grad()</code>和<code>torch.set_grad_enabled()</code>有助于局部禁用和启用梯度计算。 有关其用法的更多详细信息，请参见<a href="autograd.html#locally-disable-grad">局部禁用梯度计算</a>。 这些上下文管理器是线程本地的，因此如果您使用<code>threading</code>模块等将工作发送到另一个线程，它们将无法工作。</p>
<p>Examples:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-146-1" name="__codelineno-146-1" href="#__codelineno-146-1"></a>&gt;&gt;&gt; x = torch.zeros(1, requires_grad=True)
<a id="__codelineno-146-2" name="__codelineno-146-2" href="#__codelineno-146-2"></a>&gt;&gt;&gt; with torch.no_grad():
<a id="__codelineno-146-3" name="__codelineno-146-3" href="#__codelineno-146-3"></a>...     y = x * 2
<a id="__codelineno-146-4" name="__codelineno-146-4" href="#__codelineno-146-4"></a>&gt;&gt;&gt; y.requires_grad
<a id="__codelineno-146-5" name="__codelineno-146-5" href="#__codelineno-146-5"></a>False
<a id="__codelineno-146-6" name="__codelineno-146-6" href="#__codelineno-146-6"></a>
<a id="__codelineno-146-7" name="__codelineno-146-7" href="#__codelineno-146-7"></a>&gt;&gt;&gt; is_train = False
<a id="__codelineno-146-8" name="__codelineno-146-8" href="#__codelineno-146-8"></a>&gt;&gt;&gt; with torch.set_grad_enabled(is_train):
<a id="__codelineno-146-9" name="__codelineno-146-9" href="#__codelineno-146-9"></a>...     y = x * 2
<a id="__codelineno-146-10" name="__codelineno-146-10" href="#__codelineno-146-10"></a>&gt;&gt;&gt; y.requires_grad
<a id="__codelineno-146-11" name="__codelineno-146-11" href="#__codelineno-146-11"></a>False
<a id="__codelineno-146-12" name="__codelineno-146-12" href="#__codelineno-146-12"></a>
<a id="__codelineno-146-13" name="__codelineno-146-13" href="#__codelineno-146-13"></a>&gt;&gt;&gt; torch.set_grad_enabled(True)  # this can also be used as a function
<a id="__codelineno-146-14" name="__codelineno-146-14" href="#__codelineno-146-14"></a>&gt;&gt;&gt; y = x * 2
<a id="__codelineno-146-15" name="__codelineno-146-15" href="#__codelineno-146-15"></a>&gt;&gt;&gt; y.requires_grad
<a id="__codelineno-146-16" name="__codelineno-146-16" href="#__codelineno-146-16"></a>True
<a id="__codelineno-146-17" name="__codelineno-146-17" href="#__codelineno-146-17"></a>
<a id="__codelineno-146-18" name="__codelineno-146-18" href="#__codelineno-146-18"></a>&gt;&gt;&gt; torch.set_grad_enabled(False)
<a id="__codelineno-146-19" name="__codelineno-146-19" href="#__codelineno-146-19"></a>&gt;&gt;&gt; y = x * 2
<a id="__codelineno-146-20" name="__codelineno-146-20" href="#__codelineno-146-20"></a>&gt;&gt;&gt; y.requires_grad
<a id="__codelineno-146-21" name="__codelineno-146-21" href="#__codelineno-146-21"></a>False
</code></pre></div>
<h2 id="_11">数学运算</h2>
<h3 id="_12">逐点操作</h3>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-147-1" name="__codelineno-147-1" href="#__codelineno-147-1"></a>torch.abs(input, out=None) → Tensor¶
</code></pre></div>
<p>计算给定<code>input</code>张量的按元素的绝对值。</p>
<p><img alt="" src="../img/726d369abb9c76e751e626cbfef10220.jpg" /></p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-148-1" name="__codelineno-148-1" href="#__codelineno-148-1"></a>&gt;&gt;&gt; torch.abs(torch.tensor([-1, -2, 3]))
<a id="__codelineno-148-2" name="__codelineno-148-2" href="#__codelineno-148-2"></a>tensor([ 1,  2,  3])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-149-1" name="__codelineno-149-1" href="#__codelineno-149-1"></a>torch.acos(input, out=None) → Tensor¶
</code></pre></div>
<p>返回带有<code>input</code>元素的反余弦的新张量。</p>
<p><img alt="" src="../img/25323a5363649a36549d717d5a3cbc3e.jpg" /></p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-150-1" name="__codelineno-150-1" href="#__codelineno-150-1"></a>&gt;&gt;&gt; a = torch.randn(4)
<a id="__codelineno-150-2" name="__codelineno-150-2" href="#__codelineno-150-2"></a>&gt;&gt;&gt; a
<a id="__codelineno-150-3" name="__codelineno-150-3" href="#__codelineno-150-3"></a>tensor([ 0.3348, -0.5889,  0.2005, -0.1584])
<a id="__codelineno-150-4" name="__codelineno-150-4" href="#__codelineno-150-4"></a>&gt;&gt;&gt; torch.acos(a)
<a id="__codelineno-150-5" name="__codelineno-150-5" href="#__codelineno-150-5"></a>tensor([ 1.2294,  2.2004,  1.3690,  1.7298])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-151-1" name="__codelineno-151-1" href="#__codelineno-151-1"></a>torch.add()¶
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-152-1" name="__codelineno-152-1" href="#__codelineno-152-1"></a>torch.add(input, other, out=None)
</code></pre></div>
<p>将标量<code>other</code>添加到输入<code>input</code>的每个元素中，并返回一个新的结果张量。</p>
<p><img alt="" src="../img/1e18f010799098c127bab9465476d1fb.jpg" /></p>
<p>如果<code>input</code>的类型为 FloatTensor 或 DoubleTensor，则<code>other</code>必须为实数，否则应为整数。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>值</strong>(<em>编号</em>）–要添加到<code>input</code>每个元素的编号</p>
</li>
</ul>
<div class="highlight"><pre><span></span><code><a id="__codelineno-153-1" name="__codelineno-153-1" href="#__codelineno-153-1"></a>Keyword Arguments
</code></pre></div>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-154-1" name="__codelineno-154-1" href="#__codelineno-154-1"></a>&gt;&gt;&gt; a = torch.randn(4)
<a id="__codelineno-154-2" name="__codelineno-154-2" href="#__codelineno-154-2"></a>&gt;&gt;&gt; a
<a id="__codelineno-154-3" name="__codelineno-154-3" href="#__codelineno-154-3"></a>tensor([ 0.0202,  1.0985,  1.3506, -0.6056])
<a id="__codelineno-154-4" name="__codelineno-154-4" href="#__codelineno-154-4"></a>&gt;&gt;&gt; torch.add(a, 20)
<a id="__codelineno-154-5" name="__codelineno-154-5" href="#__codelineno-154-5"></a>tensor([ 20.0202,  21.0985,  21.3506,  19.3944])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-155-1" name="__codelineno-155-1" href="#__codelineno-155-1"></a>torch.add(input, alpha=1, other, out=None)
</code></pre></div>
<p>张量<code>other</code>的每个元素乘以标量<code>alpha</code>，然后加到张量<code>input</code>的每个元素上。 返回结果张量。</p>
<p><code>input</code>和<code>other</code>的形状必须是<a href="notes/broadcasting.html#broadcasting-semantics">可广播的</a>。</p>
<p><img alt="" src="../img/5a5bc06d446342edf134fb0e87f755ec.jpg" /></p>
<p>如果<code>other</code>的类型为 FloatTensor 或 DoubleTensor，则<code>alpha</code>必须为实数，否则应为整数。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>输入</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–第一个输入张量</p>
</li>
<li>
<p><strong>alpha</strong> (<em>数字</em>）– <code>other</code>的标量乘法器</p>
</li>
<li>
<p><strong>其他</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–第二个输入张量</p>
</li>
</ul>
<div class="highlight"><pre><span></span><code><a id="__codelineno-156-1" name="__codelineno-156-1" href="#__codelineno-156-1"></a>Keyword Arguments
</code></pre></div>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-157-1" name="__codelineno-157-1" href="#__codelineno-157-1"></a>&gt;&gt;&gt; a = torch.randn(4)
<a id="__codelineno-157-2" name="__codelineno-157-2" href="#__codelineno-157-2"></a>&gt;&gt;&gt; a
<a id="__codelineno-157-3" name="__codelineno-157-3" href="#__codelineno-157-3"></a>tensor([-0.9732, -0.3497,  0.6245,  0.4022])
<a id="__codelineno-157-4" name="__codelineno-157-4" href="#__codelineno-157-4"></a>&gt;&gt;&gt; b = torch.randn(4, 1)
<a id="__codelineno-157-5" name="__codelineno-157-5" href="#__codelineno-157-5"></a>&gt;&gt;&gt; b
<a id="__codelineno-157-6" name="__codelineno-157-6" href="#__codelineno-157-6"></a>tensor([[ 0.3743],
<a id="__codelineno-157-7" name="__codelineno-157-7" href="#__codelineno-157-7"></a>        [-1.7724],
<a id="__codelineno-157-8" name="__codelineno-157-8" href="#__codelineno-157-8"></a>        [-0.5811],
<a id="__codelineno-157-9" name="__codelineno-157-9" href="#__codelineno-157-9"></a>        [-0.8017]])
<a id="__codelineno-157-10" name="__codelineno-157-10" href="#__codelineno-157-10"></a>&gt;&gt;&gt; torch.add(a, 10, b)
<a id="__codelineno-157-11" name="__codelineno-157-11" href="#__codelineno-157-11"></a>tensor([[  2.7695,   3.3930,   4.3672,   4.1450],
<a id="__codelineno-157-12" name="__codelineno-157-12" href="#__codelineno-157-12"></a>        [-18.6971, -18.0736, -17.0994, -17.3216],
<a id="__codelineno-157-13" name="__codelineno-157-13" href="#__codelineno-157-13"></a>        [ -6.7845,  -6.1610,  -5.1868,  -5.4090],
<a id="__codelineno-157-14" name="__codelineno-157-14" href="#__codelineno-157-14"></a>        [ -8.9902,  -8.3667,  -7.3925,  -7.6147]])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-158-1" name="__codelineno-158-1" href="#__codelineno-158-1"></a>torch.addcdiv(input, value=1, tensor1, tensor2, out=None) → Tensor¶
</code></pre></div>
<p>执行<code>tensor1</code>除以<code>tensor2</code>的元素，将结果乘以标量<code>value</code>并将其加到<code>input</code>上。</p>
<p><img alt="" src="../img/69053b1e1ce2faac8aae0ee4370775b9.jpg" /></p>
<p><code>input</code>，<code>tensor1</code>和<code>tensor2</code>的形状必须是<a href="notes/broadcasting.html#broadcasting-semantics">可广播</a>。</p>
<p>对于类型为 &lt;cite&gt;FloatTensor&lt;/cite&gt; 或 &lt;cite&gt;DoubleTensor&lt;/cite&gt; 的输入，<code>value</code>必须为实数，否则为整数。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>输入</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–要添加的张量</p>
</li>
<li>
<p><strong>值</strong>(<em>编号</em> <em>，</em> <em>可选</em>）– <img alt="" src="../img/7adafc45ae24ad65f276f988f5b53f3f.jpg" />的乘数</p>
</li>
<li>
<p><strong>张量 1</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–分子张量</p>
</li>
<li>
<p><strong>张量 2</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–分母张量</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-159-1" name="__codelineno-159-1" href="#__codelineno-159-1"></a>&gt;&gt;&gt; t = torch.randn(1, 3)
<a id="__codelineno-159-2" name="__codelineno-159-2" href="#__codelineno-159-2"></a>&gt;&gt;&gt; t1 = torch.randn(3, 1)
<a id="__codelineno-159-3" name="__codelineno-159-3" href="#__codelineno-159-3"></a>&gt;&gt;&gt; t2 = torch.randn(1, 3)
<a id="__codelineno-159-4" name="__codelineno-159-4" href="#__codelineno-159-4"></a>&gt;&gt;&gt; torch.addcdiv(t, 0.1, t1, t2)
<a id="__codelineno-159-5" name="__codelineno-159-5" href="#__codelineno-159-5"></a>tensor([[-0.2312, -3.6496,  0.1312],
<a id="__codelineno-159-6" name="__codelineno-159-6" href="#__codelineno-159-6"></a>        [-1.0428,  3.4292, -0.1030],
<a id="__codelineno-159-7" name="__codelineno-159-7" href="#__codelineno-159-7"></a>        [-0.5369, -0.9829,  0.0430]])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-160-1" name="__codelineno-160-1" href="#__codelineno-160-1"></a>torch.addcmul(input, value=1, tensor1, tensor2, out=None) → Tensor¶
</code></pre></div>
<p>对<code>tensor1</code>与<code>tensor2</code>进行元素逐项乘法，将结果与标量<code>value</code>相乘，然后将其与<code>input</code>相加。</p>
<p><img alt="" src="../img/6bfdc94f1a8a13ee1a6ebc9af2d74fda.jpg" /></p>
<p><a href="#torch.tensor" title="torch.tensor"><code>tensor</code></a> ，<code>tensor1</code>和<code>tensor2</code>的形状必须是<a href="notes/broadcasting.html#broadcasting-semantics">可广播的</a>。</p>
<p>For inputs of type &lt;cite&gt;FloatTensor&lt;/cite&gt; or &lt;cite&gt;DoubleTensor&lt;/cite&gt;, <code>value</code> must be a real number, otherwise an integer.</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the tensor to be added</p>
</li>
<li>
<p><strong>值</strong>(<em>编号</em> <em>，</em> <em>可选</em>）– <img alt="" src="../img/25f6a482a153e2aed6c16040677c1f5e.jpg" />的乘数</p>
</li>
<li>
<p><strong>张量 1</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–要相乘的张量</p>
</li>
<li>
<p><strong>张量 2</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–要相乘的张量</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-161-1" name="__codelineno-161-1" href="#__codelineno-161-1"></a>&gt;&gt;&gt; t = torch.randn(1, 3)
<a id="__codelineno-161-2" name="__codelineno-161-2" href="#__codelineno-161-2"></a>&gt;&gt;&gt; t1 = torch.randn(3, 1)
<a id="__codelineno-161-3" name="__codelineno-161-3" href="#__codelineno-161-3"></a>&gt;&gt;&gt; t2 = torch.randn(1, 3)
<a id="__codelineno-161-4" name="__codelineno-161-4" href="#__codelineno-161-4"></a>&gt;&gt;&gt; torch.addcmul(t, 0.1, t1, t2)
<a id="__codelineno-161-5" name="__codelineno-161-5" href="#__codelineno-161-5"></a>tensor([[-0.8635, -0.6391,  1.6174],
<a id="__codelineno-161-6" name="__codelineno-161-6" href="#__codelineno-161-6"></a>        [-0.7617, -0.5879,  1.7388],
<a id="__codelineno-161-7" name="__codelineno-161-7" href="#__codelineno-161-7"></a>        [-0.8353, -0.6249,  1.6511]])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-162-1" name="__codelineno-162-1" href="#__codelineno-162-1"></a>torch.angle(input, out=None) → Tensor¶
</code></pre></div>
<p>计算给定<code>input</code>张量的元素方向角(以弧度为单位）。</p>
<p><img alt="" src="../img/5af8c34b3d4c490c508caf11a458d5d6.jpg" /></p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-163-1" name="__codelineno-163-1" href="#__codelineno-163-1"></a>&gt;&gt;&gt; torch.angle(torch.tensor([-1 + 1j, -2 + 2j, 3 - 3j]))*180/3.14159
<a id="__codelineno-163-2" name="__codelineno-163-2" href="#__codelineno-163-2"></a>tensor([ 135.,  135,  -45])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-164-1" name="__codelineno-164-1" href="#__codelineno-164-1"></a>torch.asin(input, out=None) → Tensor¶
</code></pre></div>
<p>返回带有<code>input</code>元素的反正弦值的新张量。</p>
<p><img alt="" src="../img/fa8ed00ae85106eea8e8a11b6b66d898.jpg" /></p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-165-1" name="__codelineno-165-1" href="#__codelineno-165-1"></a>&gt;&gt;&gt; a = torch.randn(4)
<a id="__codelineno-165-2" name="__codelineno-165-2" href="#__codelineno-165-2"></a>&gt;&gt;&gt; a
<a id="__codelineno-165-3" name="__codelineno-165-3" href="#__codelineno-165-3"></a>tensor([-0.5962,  1.4985, -0.4396,  1.4525])
<a id="__codelineno-165-4" name="__codelineno-165-4" href="#__codelineno-165-4"></a>&gt;&gt;&gt; torch.asin(a)
<a id="__codelineno-165-5" name="__codelineno-165-5" href="#__codelineno-165-5"></a>tensor([-0.6387,     nan, -0.4552,     nan])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-166-1" name="__codelineno-166-1" href="#__codelineno-166-1"></a>torch.atan(input, out=None) → Tensor¶
</code></pre></div>
<p>返回带有<code>input</code>元素的反正切的新张量。</p>
<p><img alt="" src="../img/8614369d5e7413ce21e9a5d6bcf786cb.jpg" /></p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-167-1" name="__codelineno-167-1" href="#__codelineno-167-1"></a>&gt;&gt;&gt; a = torch.randn(4)
<a id="__codelineno-167-2" name="__codelineno-167-2" href="#__codelineno-167-2"></a>&gt;&gt;&gt; a
<a id="__codelineno-167-3" name="__codelineno-167-3" href="#__codelineno-167-3"></a>tensor([ 0.2341,  0.2539, -0.6256, -0.6448])
<a id="__codelineno-167-4" name="__codelineno-167-4" href="#__codelineno-167-4"></a>&gt;&gt;&gt; torch.atan(a)
<a id="__codelineno-167-5" name="__codelineno-167-5" href="#__codelineno-167-5"></a>tensor([ 0.2299,  0.2487, -0.5591, -0.5727])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-168-1" name="__codelineno-168-1" href="#__codelineno-168-1"></a>torch.atan2(input, other, out=None) → Tensor¶
</code></pre></div>
<p>考虑象限的<img alt="" src="../img/c45e59e7ab5c2f7cff1ff37748c0d62b.jpg" />元素逐级反正切。 返回一个新的张量，其矢量<img alt="" src="../img/2cb0a971340ffd46996a0cf9eb81d376.jpg" />与矢量<img alt="" src="../img/8456c6ac83b24dbe917ff5a29a771bd7.jpg" />之间的弧度为符号角。 (请注意，第二个参数<img alt="" src="../img/4e409083ee35ea2eeab094ea4f9bf730.jpg" />是 x 坐标，而第一个参数<img alt="" src="../img/0c7468da87ed4da7f144e93eb6e7e60e.jpg" />是 y 坐标。）</p>
<p><code>input</code>和<code>other</code>的形状必须是<a href="notes/broadcasting.html#broadcasting-semantics">可广播的</a>。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the first input tensor</p>
</li>
<li>
<p><strong>other</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the second input tensor</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-169-1" name="__codelineno-169-1" href="#__codelineno-169-1"></a>&gt;&gt;&gt; a = torch.randn(4)
<a id="__codelineno-169-2" name="__codelineno-169-2" href="#__codelineno-169-2"></a>&gt;&gt;&gt; a
<a id="__codelineno-169-3" name="__codelineno-169-3" href="#__codelineno-169-3"></a>tensor([ 0.9041,  0.0196, -0.3108, -2.4423])
<a id="__codelineno-169-4" name="__codelineno-169-4" href="#__codelineno-169-4"></a>&gt;&gt;&gt; torch.atan2(a, torch.randn(4))
<a id="__codelineno-169-5" name="__codelineno-169-5" href="#__codelineno-169-5"></a>tensor([ 0.9833,  0.0811, -1.9743, -1.4151])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-170-1" name="__codelineno-170-1" href="#__codelineno-170-1"></a>torch.bitwise_not(input, out=None) → Tensor¶
</code></pre></div>
<p>计算给定输入张量的按位非。 输入张量必须是整数或布尔类型。 对于布尔张量，它计算逻辑非。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-171-1" name="__codelineno-171-1" href="#__codelineno-171-1"></a>&gt;&gt;&gt; torch.bitwise_not(torch.tensor([-1, -2, 3], dtype=torch.int8))
<a id="__codelineno-171-2" name="__codelineno-171-2" href="#__codelineno-171-2"></a>tensor([ 0,  1, -4], dtype=torch.int8)
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-172-1" name="__codelineno-172-1" href="#__codelineno-172-1"></a>torch.bitwise_xor(input, other, out=None) → Tensor¶
</code></pre></div>
<p>计算<code>input</code>和<code>other</code>的按位 XOR。 输入张量必须是整数或布尔类型。 对于布尔张量，它计算逻辑 XOR。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>输入</strong> –第一个输入张量</p>
</li>
<li>
<p><strong>其他</strong> –第二个输入张量</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-173-1" name="__codelineno-173-1" href="#__codelineno-173-1"></a>&gt;&gt;&gt; torch.bitwise_xor(torch.tensor([-1, -2, 3], dtype=torch.int8), torch.tensor([1, 0, 3], dtype=torch.int8))
<a id="__codelineno-173-2" name="__codelineno-173-2" href="#__codelineno-173-2"></a>tensor([-2, -2,  0], dtype=torch.int8)
<a id="__codelineno-173-3" name="__codelineno-173-3" href="#__codelineno-173-3"></a>&gt;&gt;&gt; torch.bitwise_xor(torch.tensor([True, True, False]), torch.tensor([False, True, False]))
<a id="__codelineno-173-4" name="__codelineno-173-4" href="#__codelineno-173-4"></a>tensor([ True, False, False])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-174-1" name="__codelineno-174-1" href="#__codelineno-174-1"></a>torch.ceil(input, out=None) → Tensor¶
</code></pre></div>
<p>返回带有<code>input</code>元素的 ceil 的新张量，该元素大于或等于每个元素的最小整数。</p>
<p><img alt="" src="../img/3425145dda63005bbc6ff0b2f0331aa7.jpg" /></p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-175-1" name="__codelineno-175-1" href="#__codelineno-175-1"></a>&gt;&gt;&gt; a = torch.randn(4)
<a id="__codelineno-175-2" name="__codelineno-175-2" href="#__codelineno-175-2"></a>&gt;&gt;&gt; a
<a id="__codelineno-175-3" name="__codelineno-175-3" href="#__codelineno-175-3"></a>tensor([-0.6341, -1.4208, -1.0900,  0.5826])
<a id="__codelineno-175-4" name="__codelineno-175-4" href="#__codelineno-175-4"></a>&gt;&gt;&gt; torch.ceil(a)
<a id="__codelineno-175-5" name="__codelineno-175-5" href="#__codelineno-175-5"></a>tensor([-0., -1., -1.,  1.])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-176-1" name="__codelineno-176-1" href="#__codelineno-176-1"></a>torch.clamp(input, min, max, out=None) → Tensor¶
</code></pre></div>
<p>将<code>input</code>中的所有元素限制在 &lt;cite&gt;[&lt;/cite&gt; <a href="#torch.min" title="torch.min"><code>min</code></a> ， <a href="#torch.max" title="torch.max"><code>max</code></a> &lt;cite&gt;]&lt;/cite&gt; 范围内，并返回结果张量：</p>
<p><img alt="" src="../img/597a5259be5b1a330eaa6858b12b8994.jpg" /></p>
<p>如果<code>input</code>的类型为 &lt;cite&gt;FloatTensor&lt;/cite&gt; 或 &lt;cite&gt;DoubleTensor&lt;/cite&gt; ，则参数 <a href="#torch.min" title="torch.min"><code>min</code></a> 和 <a href="#torch.max" title="torch.max"><code>max</code></a> 必须为实数，否则为实数 应该是整数。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>min</strong> (<em>编号</em>）–要钳制的范围的下限</p>
</li>
<li>
<p><strong>最大</strong>(<em>编号</em>）–要钳位的范围的上限</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-177-1" name="__codelineno-177-1" href="#__codelineno-177-1"></a>&gt;&gt;&gt; a = torch.randn(4)
<a id="__codelineno-177-2" name="__codelineno-177-2" href="#__codelineno-177-2"></a>&gt;&gt;&gt; a
<a id="__codelineno-177-3" name="__codelineno-177-3" href="#__codelineno-177-3"></a>tensor([-1.7120,  0.1734, -0.0478, -0.0922])
<a id="__codelineno-177-4" name="__codelineno-177-4" href="#__codelineno-177-4"></a>&gt;&gt;&gt; torch.clamp(a, min=-0.5, max=0.5)
<a id="__codelineno-177-5" name="__codelineno-177-5" href="#__codelineno-177-5"></a>tensor([-0.5000,  0.1734, -0.0478, -0.0922])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-178-1" name="__codelineno-178-1" href="#__codelineno-178-1"></a>torch.clamp(input, *, min, out=None) → Tensor
</code></pre></div>
<p>将<code>input</code>中的所有元素限制为大于或等于 <a href="#torch.min" title="torch.min"><code>min</code></a> 。</p>
<p>如果<code>input</code>的类型为 &lt;cite&gt;FloatTensor&lt;/cite&gt; 或 &lt;cite&gt;DoubleTensor&lt;/cite&gt; ，则<code>value</code>应为实数，否则应为整数。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>值</strong>(<em>编号</em>）–输出中每个元素的最小值</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-179-1" name="__codelineno-179-1" href="#__codelineno-179-1"></a>&gt;&gt;&gt; a = torch.randn(4)
<a id="__codelineno-179-2" name="__codelineno-179-2" href="#__codelineno-179-2"></a>&gt;&gt;&gt; a
<a id="__codelineno-179-3" name="__codelineno-179-3" href="#__codelineno-179-3"></a>tensor([-0.0299, -2.3184,  2.1593, -0.8883])
<a id="__codelineno-179-4" name="__codelineno-179-4" href="#__codelineno-179-4"></a>&gt;&gt;&gt; torch.clamp(a, min=0.5)
<a id="__codelineno-179-5" name="__codelineno-179-5" href="#__codelineno-179-5"></a>tensor([ 0.5000,  0.5000,  2.1593,  0.5000])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-180-1" name="__codelineno-180-1" href="#__codelineno-180-1"></a>torch.clamp(input, *, max, out=None) → Tensor
</code></pre></div>
<p>将<code>input</code>中的所有元素限制为小于或等于 <a href="#torch.max" title="torch.max"><code>max</code></a> 。</p>
<p>If <code>input</code> is of type &lt;cite&gt;FloatTensor&lt;/cite&gt; or &lt;cite&gt;DoubleTensor&lt;/cite&gt;, <code>value</code> should be a real number, otherwise it should be an integer.</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>值</strong>(<em>编号</em>）–输出中每个元素的最大值</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-181-1" name="__codelineno-181-1" href="#__codelineno-181-1"></a>&gt;&gt;&gt; a = torch.randn(4)
<a id="__codelineno-181-2" name="__codelineno-181-2" href="#__codelineno-181-2"></a>&gt;&gt;&gt; a
<a id="__codelineno-181-3" name="__codelineno-181-3" href="#__codelineno-181-3"></a>tensor([ 0.7753, -0.4702, -0.4599,  1.1899])
<a id="__codelineno-181-4" name="__codelineno-181-4" href="#__codelineno-181-4"></a>&gt;&gt;&gt; torch.clamp(a, max=0.5)
<a id="__codelineno-181-5" name="__codelineno-181-5" href="#__codelineno-181-5"></a>tensor([ 0.5000, -0.4702, -0.4599,  0.5000])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-182-1" name="__codelineno-182-1" href="#__codelineno-182-1"></a>torch.conj(input, out=None) → Tensor¶
</code></pre></div>
<p>计算给定<code>input</code>张量的逐元素共轭。</p>
<p><img alt="" src="../img/334473ca71a76cc4b45b56e05f9113be.jpg" /></p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-183-1" name="__codelineno-183-1" href="#__codelineno-183-1"></a>&gt;&gt;&gt; torch.conj(torch.tensor([-1 + 1j, -2 + 2j, 3 - 3j]))
<a id="__codelineno-183-2" name="__codelineno-183-2" href="#__codelineno-183-2"></a>tensor([-1 - 1j, -2 - 2j, 3 + 3j])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-184-1" name="__codelineno-184-1" href="#__codelineno-184-1"></a>torch.cos(input, out=None) → Tensor¶
</code></pre></div>
<p>返回带有<code>input</code>元素的余弦的新张量。</p>
<p><img alt="" src="../img/a54751040e4e9bbc318ce13170d3575a.jpg" /></p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-185-1" name="__codelineno-185-1" href="#__codelineno-185-1"></a>&gt;&gt;&gt; a = torch.randn(4)
<a id="__codelineno-185-2" name="__codelineno-185-2" href="#__codelineno-185-2"></a>&gt;&gt;&gt; a
<a id="__codelineno-185-3" name="__codelineno-185-3" href="#__codelineno-185-3"></a>tensor([ 1.4309,  1.2706, -0.8562,  0.9796])
<a id="__codelineno-185-4" name="__codelineno-185-4" href="#__codelineno-185-4"></a>&gt;&gt;&gt; torch.cos(a)
<a id="__codelineno-185-5" name="__codelineno-185-5" href="#__codelineno-185-5"></a>tensor([ 0.1395,  0.2957,  0.6553,  0.5574])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-186-1" name="__codelineno-186-1" href="#__codelineno-186-1"></a>torch.cosh(input, out=None) → Tensor¶
</code></pre></div>
<p>返回具有<code>input</code>元素的双曲余弦的新张量。</p>
<p><img alt="" src="../img/01d5c509dd9312bb70bb293dffb6ee74.jpg" /></p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-187-1" name="__codelineno-187-1" href="#__codelineno-187-1"></a>&gt;&gt;&gt; a = torch.randn(4)
<a id="__codelineno-187-2" name="__codelineno-187-2" href="#__codelineno-187-2"></a>&gt;&gt;&gt; a
<a id="__codelineno-187-3" name="__codelineno-187-3" href="#__codelineno-187-3"></a>tensor([ 0.1632,  1.1835, -0.6979, -0.7325])
<a id="__codelineno-187-4" name="__codelineno-187-4" href="#__codelineno-187-4"></a>&gt;&gt;&gt; torch.cosh(a)
<a id="__codelineno-187-5" name="__codelineno-187-5" href="#__codelineno-187-5"></a>tensor([ 1.0133,  1.7860,  1.2536,  1.2805])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-188-1" name="__codelineno-188-1" href="#__codelineno-188-1"></a>torch.div()¶
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-189-1" name="__codelineno-189-1" href="#__codelineno-189-1"></a>torch.div(input, other, out=None) → Tensor
</code></pre></div>
<p>将输入<code>input</code>的每个元素除以标量<code>other</code>，然后返回一个新的结果张量。</p>
<p><img alt="" src="../img/b67b84d63ecd57a1b4241eca13fb1661.jpg" /></p>
<p>如果<code>input</code>和<code>other</code>的 <a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a> 不同，则根据类型提升<a href="tensor_attributes.html#type-promotion-doc">文档</a>中所述的规则确定结果张量的 <a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a> 。 ]。 如果指定了<code>out</code>，则结果必须是<a href="tensor_attributes.html#type-promotion-doc">可转换为</a>到指定输出张量的 <a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a> 。 整数除以零会导致不确定的行为。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>其他</strong>(<em>编号</em>）–要划分为<code>input</code>每个元素的编号</p>
</li>
</ul>
<div class="highlight"><pre><span></span><code><a id="__codelineno-190-1" name="__codelineno-190-1" href="#__codelineno-190-1"></a>Keyword Arguments
</code></pre></div>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-191-1" name="__codelineno-191-1" href="#__codelineno-191-1"></a>&gt;&gt;&gt; a = torch.randn(5)
<a id="__codelineno-191-2" name="__codelineno-191-2" href="#__codelineno-191-2"></a>&gt;&gt;&gt; a
<a id="__codelineno-191-3" name="__codelineno-191-3" href="#__codelineno-191-3"></a>tensor([ 0.3810,  1.2774, -0.2972, -0.3719,  0.4637])
<a id="__codelineno-191-4" name="__codelineno-191-4" href="#__codelineno-191-4"></a>&gt;&gt;&gt; torch.div(a, 0.5)
<a id="__codelineno-191-5" name="__codelineno-191-5" href="#__codelineno-191-5"></a>tensor([ 0.7620,  2.5548, -0.5944, -0.7439,  0.9275])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-192-1" name="__codelineno-192-1" href="#__codelineno-192-1"></a>torch.div(input, other, out=None) → Tensor
</code></pre></div>
<p>张量<code>input</code>的每个元素除以张量<code>other</code>的每个元素。 返回结果张量。</p>
<p><img alt="" src="../img/9fd0198f0d9df7531b6ea4ae384fbd53.jpg" /></p>
<p><code>input</code>和<code>other</code>的形状必须是<a href="notes/broadcasting.html#broadcasting-semantics">可广播</a>。 如果<code>input</code>和<code>other</code>的 <a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a> 不同，则根据类型提升<a href="tensor_attributes.html#type-promotion-doc">文档</a>中描述的规则确定结果张量的 <a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a> 。 ]。 如果指定了<code>out</code>，则结果必须是<a href="tensor_attributes.html#type-promotion-doc">可转换为</a>到指定输出张量的 <a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a> 。 整数除以零会导致不确定的行为。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>输入</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–分子张量</p>
</li>
<li>
<p><strong>其他</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–分母张量</p>
</li>
</ul>
<div class="highlight"><pre><span></span><code><a id="__codelineno-193-1" name="__codelineno-193-1" href="#__codelineno-193-1"></a>Keyword Arguments
</code></pre></div>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-194-1" name="__codelineno-194-1" href="#__codelineno-194-1"></a>&gt;&gt;&gt; a = torch.randn(4, 4)
<a id="__codelineno-194-2" name="__codelineno-194-2" href="#__codelineno-194-2"></a>&gt;&gt;&gt; a
<a id="__codelineno-194-3" name="__codelineno-194-3" href="#__codelineno-194-3"></a>tensor([[-0.3711, -1.9353, -0.4605, -0.2917],
<a id="__codelineno-194-4" name="__codelineno-194-4" href="#__codelineno-194-4"></a>        [ 0.1815, -1.0111,  0.9805, -1.5923],
<a id="__codelineno-194-5" name="__codelineno-194-5" href="#__codelineno-194-5"></a>        [ 0.1062,  1.4581,  0.7759, -1.2344],
<a id="__codelineno-194-6" name="__codelineno-194-6" href="#__codelineno-194-6"></a>        [-0.1830, -0.0313,  1.1908, -1.4757]])
<a id="__codelineno-194-7" name="__codelineno-194-7" href="#__codelineno-194-7"></a>&gt;&gt;&gt; b = torch.randn(4)
<a id="__codelineno-194-8" name="__codelineno-194-8" href="#__codelineno-194-8"></a>&gt;&gt;&gt; b
<a id="__codelineno-194-9" name="__codelineno-194-9" href="#__codelineno-194-9"></a>tensor([ 0.8032,  0.2930, -0.8113, -0.2308])
<a id="__codelineno-194-10" name="__codelineno-194-10" href="#__codelineno-194-10"></a>&gt;&gt;&gt; torch.div(a, b)
<a id="__codelineno-194-11" name="__codelineno-194-11" href="#__codelineno-194-11"></a>tensor([[-0.4620, -6.6051,  0.5676,  1.2637],
<a id="__codelineno-194-12" name="__codelineno-194-12" href="#__codelineno-194-12"></a>        [ 0.2260, -3.4507, -1.2086,  6.8988],
<a id="__codelineno-194-13" name="__codelineno-194-13" href="#__codelineno-194-13"></a>        [ 0.1322,  4.9764, -0.9564,  5.3480],
<a id="__codelineno-194-14" name="__codelineno-194-14" href="#__codelineno-194-14"></a>        [-0.2278, -0.1068, -1.4678,  6.3936]])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-195-1" name="__codelineno-195-1" href="#__codelineno-195-1"></a>torch.digamma(input, out=None) → Tensor¶
</code></pre></div>
<p>计算&lt;cite&gt;输入&lt;/cite&gt;上伽马函数的对数导数。</p>
<p><img alt="" src="../img/00133aac1a20067edb1ffa26d46a0311.jpg" /></p>
<p>Parameters</p>
<p><strong>输入</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–用于计算 digamma 函数的张量</p>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-196-1" name="__codelineno-196-1" href="#__codelineno-196-1"></a>&gt;&gt;&gt; a = torch.tensor([1, 0.5])
<a id="__codelineno-196-2" name="__codelineno-196-2" href="#__codelineno-196-2"></a>&gt;&gt;&gt; torch.digamma(a)
<a id="__codelineno-196-3" name="__codelineno-196-3" href="#__codelineno-196-3"></a>tensor([-0.5772, -1.9635])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-197-1" name="__codelineno-197-1" href="#__codelineno-197-1"></a>torch.erf(input, out=None) → Tensor¶
</code></pre></div>
<p>计算每个元素的误差函数。 错误函数定义如下：</p>
<p><img alt="" src="../img/8d655f076653c2f1d5a1b07d3f7437ea.jpg" /></p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-198-1" name="__codelineno-198-1" href="#__codelineno-198-1"></a>&gt;&gt;&gt; torch.erf(torch.tensor([0, -1., 10.]))
<a id="__codelineno-198-2" name="__codelineno-198-2" href="#__codelineno-198-2"></a>tensor([ 0.0000, -0.8427,  1.0000])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-199-1" name="__codelineno-199-1" href="#__codelineno-199-1"></a>torch.erfc(input, out=None) → Tensor¶
</code></pre></div>
<p>计算<code>input</code>的每个元素的互补误差函数。 互补误差函数定义如下：</p>
<p><img alt="" src="../img/a58cfc335ee9ca516258ccbad689a9fd.jpg" /></p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-200-1" name="__codelineno-200-1" href="#__codelineno-200-1"></a>&gt;&gt;&gt; torch.erfc(torch.tensor([0, -1., 10.]))
<a id="__codelineno-200-2" name="__codelineno-200-2" href="#__codelineno-200-2"></a>tensor([ 1.0000, 1.8427,  0.0000])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-201-1" name="__codelineno-201-1" href="#__codelineno-201-1"></a>torch.erfinv(input, out=None) → Tensor¶
</code></pre></div>
<p>计算<code>input</code>的每个元素的反误差函数。 逆误差函数在<img alt="" src="../img/cef0a53226b8b835c974e021239fb845.jpg" />范围内定义为：</p>
<p><img alt="" src="../img/9e2a92a8e980b6525bc4f1a045bc6f7e.jpg" /></p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-202-1" name="__codelineno-202-1" href="#__codelineno-202-1"></a>&gt;&gt;&gt; torch.erfinv(torch.tensor([0, 0.5, -1.]))
<a id="__codelineno-202-2" name="__codelineno-202-2" href="#__codelineno-202-2"></a>tensor([ 0.0000,  0.4769,    -inf])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-203-1" name="__codelineno-203-1" href="#__codelineno-203-1"></a>torch.exp(input, out=None) → Tensor¶
</code></pre></div>
<p>返回具有输入张量<code>input</code>的元素指数的新张量。</p>
<p><img alt="" src="../img/89fdb5873c4e62755139600cbda25b05.jpg" /></p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-204-1" name="__codelineno-204-1" href="#__codelineno-204-1"></a>&gt;&gt;&gt; torch.exp(torch.tensor([0, math.log(2.)]))
<a id="__codelineno-204-2" name="__codelineno-204-2" href="#__codelineno-204-2"></a>tensor([ 1.,  2.])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-205-1" name="__codelineno-205-1" href="#__codelineno-205-1"></a>torch.expm1(input, out=None) → Tensor¶
</code></pre></div>
<p>返回一个新的张量，其元素的指数为<code>input</code>的负 1。</p>
<p><img alt="" src="../img/87b2bec9f68b2f04352bccfeb8c06838.jpg" /></p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-206-1" name="__codelineno-206-1" href="#__codelineno-206-1"></a>&gt;&gt;&gt; torch.expm1(torch.tensor([0, math.log(2.)]))
<a id="__codelineno-206-2" name="__codelineno-206-2" href="#__codelineno-206-2"></a>tensor([ 0.,  1.])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-207-1" name="__codelineno-207-1" href="#__codelineno-207-1"></a>torch.floor(input, out=None) → Tensor¶
</code></pre></div>
<p>返回一个新的张量，该张量的元素为<code>input</code>的下限，即小于或等于每个元素的最大整数。</p>
<p><img alt="" src="../img/660d522c6d1931985ef3cb2c38e0f843.jpg" /></p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-208-1" name="__codelineno-208-1" href="#__codelineno-208-1"></a>&gt;&gt;&gt; a = torch.randn(4)
<a id="__codelineno-208-2" name="__codelineno-208-2" href="#__codelineno-208-2"></a>&gt;&gt;&gt; a
<a id="__codelineno-208-3" name="__codelineno-208-3" href="#__codelineno-208-3"></a>tensor([-0.8166,  1.5308, -0.2530, -0.2091])
<a id="__codelineno-208-4" name="__codelineno-208-4" href="#__codelineno-208-4"></a>&gt;&gt;&gt; torch.floor(a)
<a id="__codelineno-208-5" name="__codelineno-208-5" href="#__codelineno-208-5"></a>tensor([-1.,  1., -1., -1.])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-209-1" name="__codelineno-209-1" href="#__codelineno-209-1"></a>torch.fmod(input, other, out=None) → Tensor¶
</code></pre></div>
<p>计算除法元素的余数。</p>
<p>被除数和除数可以同时包含整数和浮点数。 其余部分与股息<code>input</code>具有相同的符号。</p>
<p>当<code>other</code>是张量时，<code>input</code>和<code>other</code>的形状必须是<a href="notes/broadcasting.html#broadcasting-semantics">可广播的</a>。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>输入</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–股息</p>
</li>
<li>
<p><strong>其他</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a> <em>或</em> <em>python：float</em> )–除数，可以是数字或整数 与股息形状相同的张量</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-210-1" name="__codelineno-210-1" href="#__codelineno-210-1"></a>&gt;&gt;&gt; torch.fmod(torch.tensor([-3., -2, -1, 1, 2, 3]), 2)
<a id="__codelineno-210-2" name="__codelineno-210-2" href="#__codelineno-210-2"></a>tensor([-1., -0., -1.,  1.,  0.,  1.])
<a id="__codelineno-210-3" name="__codelineno-210-3" href="#__codelineno-210-3"></a>&gt;&gt;&gt; torch.fmod(torch.tensor([1., 2, 3, 4, 5]), 1.5)
<a id="__codelineno-210-4" name="__codelineno-210-4" href="#__codelineno-210-4"></a>tensor([ 1.0000,  0.5000,  0.0000,  1.0000,  0.5000])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-211-1" name="__codelineno-211-1" href="#__codelineno-211-1"></a>torch.frac(input, out=None) → Tensor¶
</code></pre></div>
<p>计算<code>input</code>中每个元素的分数部分。</p>
<p><img alt="" src="../img/fdd683e9c79ed73d211526708d082e20.jpg" /></p>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-212-1" name="__codelineno-212-1" href="#__codelineno-212-1"></a>&gt;&gt;&gt; torch.frac(torch.tensor([1, 2.5, -3.2]))
<a id="__codelineno-212-2" name="__codelineno-212-2" href="#__codelineno-212-2"></a>tensor([ 0.0000,  0.5000, -0.2000])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-213-1" name="__codelineno-213-1" href="#__codelineno-213-1"></a>torch.imag(input, out=None) → Tensor¶
</code></pre></div>
<p>计算给定<code>input</code>张量的逐元素 imag 值。</p>
<p><img alt="" src="../img/23bb4f65e036b1aa01e948c93438b146.jpg" /></p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-214-1" name="__codelineno-214-1" href="#__codelineno-214-1"></a>&gt;&gt;&gt; torch.imag(torch.tensor([-1 + 1j, -2 + 2j, 3 - 3j]))
<a id="__codelineno-214-2" name="__codelineno-214-2" href="#__codelineno-214-2"></a>tensor([ 1,  2,  -3])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-215-1" name="__codelineno-215-1" href="#__codelineno-215-1"></a>torch.lerp(input, end, weight, out=None)¶
</code></pre></div>
<p>根据标量或张量<code>weight</code>对两个张量<code>start</code>(由<code>input</code>给出）和<code>end</code>进行线性插值，并返回所得的<code>out</code>张量。</p>
<p><img alt="" src="../img/02b2be2accc587016c8fb3f0c79c2b03.jpg" /></p>
<p><code>start</code>和<code>end</code>的形状必须是<a href="notes/broadcasting.html#broadcasting-semantics">可广播的</a>。 如果<code>weight</code>是张量，则<code>weight</code>，<code>start</code>和<code>end</code>的形状必须是<a href="notes/broadcasting.html#broadcasting-semantics">可广播的</a>。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>输入</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–具有起点的张量</p>
</li>
<li>
<p><strong>末端</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–具有终点的张量</p>
</li>
<li>
<p><strong>权重</strong> (<em>python：float</em> <em>或</em> <em>tensor</em>）–插值公式的权重</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-216-1" name="__codelineno-216-1" href="#__codelineno-216-1"></a>&gt;&gt;&gt; start = torch.arange(1., 5.)
<a id="__codelineno-216-2" name="__codelineno-216-2" href="#__codelineno-216-2"></a>&gt;&gt;&gt; end = torch.empty(4).fill_(10)
<a id="__codelineno-216-3" name="__codelineno-216-3" href="#__codelineno-216-3"></a>&gt;&gt;&gt; start
<a id="__codelineno-216-4" name="__codelineno-216-4" href="#__codelineno-216-4"></a>tensor([ 1.,  2.,  3.,  4.])
<a id="__codelineno-216-5" name="__codelineno-216-5" href="#__codelineno-216-5"></a>&gt;&gt;&gt; end
<a id="__codelineno-216-6" name="__codelineno-216-6" href="#__codelineno-216-6"></a>tensor([ 10.,  10.,  10.,  10.])
<a id="__codelineno-216-7" name="__codelineno-216-7" href="#__codelineno-216-7"></a>&gt;&gt;&gt; torch.lerp(start, end, 0.5)
<a id="__codelineno-216-8" name="__codelineno-216-8" href="#__codelineno-216-8"></a>tensor([ 5.5000,  6.0000,  6.5000,  7.0000])
<a id="__codelineno-216-9" name="__codelineno-216-9" href="#__codelineno-216-9"></a>&gt;&gt;&gt; torch.lerp(start, end, torch.full_like(start, 0.5))
<a id="__codelineno-216-10" name="__codelineno-216-10" href="#__codelineno-216-10"></a>tensor([ 5.5000,  6.0000,  6.5000,  7.0000])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-217-1" name="__codelineno-217-1" href="#__codelineno-217-1"></a>torch.lgamma(input, out=None) → Tensor¶
</code></pre></div>
<p>计算<code>input</code>上伽马函数的对数。</p>
<p><img alt="" src="../img/5bc7af83352d4911e4360a1a8b2c3427.jpg" /></p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-218-1" name="__codelineno-218-1" href="#__codelineno-218-1"></a>&gt;&gt;&gt; a = torch.arange(0.5, 2, 0.5)
<a id="__codelineno-218-2" name="__codelineno-218-2" href="#__codelineno-218-2"></a>&gt;&gt;&gt; torch.lgamma(a)
<a id="__codelineno-218-3" name="__codelineno-218-3" href="#__codelineno-218-3"></a>tensor([ 0.5724,  0.0000, -0.1208])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-219-1" name="__codelineno-219-1" href="#__codelineno-219-1"></a>torch.log(input, out=None) → Tensor¶
</code></pre></div>
<p>返回具有<code>input</code>元素的自然对数的新张量。</p>
<p><img alt="" src="../img/6b7510d341734226d626b3362506887e.jpg" /></p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-220-1" name="__codelineno-220-1" href="#__codelineno-220-1"></a>&gt;&gt;&gt; a = torch.randn(5)
<a id="__codelineno-220-2" name="__codelineno-220-2" href="#__codelineno-220-2"></a>&gt;&gt;&gt; a
<a id="__codelineno-220-3" name="__codelineno-220-3" href="#__codelineno-220-3"></a>tensor([-0.7168, -0.5471, -0.8933, -1.4428, -0.1190])
<a id="__codelineno-220-4" name="__codelineno-220-4" href="#__codelineno-220-4"></a>&gt;&gt;&gt; torch.log(a)
<a id="__codelineno-220-5" name="__codelineno-220-5" href="#__codelineno-220-5"></a>tensor([ nan,  nan,  nan,  nan,  nan])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-221-1" name="__codelineno-221-1" href="#__codelineno-221-1"></a>torch.log10(input, out=None) → Tensor¶
</code></pre></div>
<p>返回以<code>input</code>元素的底数为底的对数的新张量。</p>
<p><img alt="" src="../img/585d59455ec4e6d03ca7d2f072b94ebf.jpg" /></p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-222-1" name="__codelineno-222-1" href="#__codelineno-222-1"></a>&gt;&gt;&gt; a = torch.rand(5)
<a id="__codelineno-222-2" name="__codelineno-222-2" href="#__codelineno-222-2"></a>&gt;&gt;&gt; a
<a id="__codelineno-222-3" name="__codelineno-222-3" href="#__codelineno-222-3"></a>tensor([ 0.5224,  0.9354,  0.7257,  0.1301,  0.2251])
<a id="__codelineno-222-4" name="__codelineno-222-4" href="#__codelineno-222-4"></a>
<a id="__codelineno-222-5" name="__codelineno-222-5" href="#__codelineno-222-5"></a>&gt;&gt;&gt; torch.log10(a)
<a id="__codelineno-222-6" name="__codelineno-222-6" href="#__codelineno-222-6"></a>tensor([-0.2820, -0.0290, -0.1392, -0.8857, -0.6476])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-223-1" name="__codelineno-223-1" href="#__codelineno-223-1"></a>torch.log1p(input, out=None) → Tensor¶
</code></pre></div>
<p>返回自然对数为(1 + <code>input</code>）的新张量。</p>
<p><img alt="" src="../img/a728d50025ad41634df088b6e1222f73.jpg" /></p>
<p>Note</p>
<p>对于较小的<code>input</code>值，此功能比 <a href="#torch.log" title="torch.log"><code>torch.log()</code></a> 更准确。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-224-1" name="__codelineno-224-1" href="#__codelineno-224-1"></a>&gt;&gt;&gt; a = torch.randn(5)
<a id="__codelineno-224-2" name="__codelineno-224-2" href="#__codelineno-224-2"></a>&gt;&gt;&gt; a
<a id="__codelineno-224-3" name="__codelineno-224-3" href="#__codelineno-224-3"></a>tensor([-1.0090, -0.9923,  1.0249, -0.5372,  0.2492])
<a id="__codelineno-224-4" name="__codelineno-224-4" href="#__codelineno-224-4"></a>&gt;&gt;&gt; torch.log1p(a)
<a id="__codelineno-224-5" name="__codelineno-224-5" href="#__codelineno-224-5"></a>tensor([    nan, -4.8653,  0.7055, -0.7705,  0.2225])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-225-1" name="__codelineno-225-1" href="#__codelineno-225-1"></a>torch.log2(input, out=None) → Tensor¶
</code></pre></div>
<p>返回以<code>input</code>元素的底数为对数的新张量。</p>
<p><img alt="" src="../img/893d53e193443b3650c0541772aa5216.jpg" /></p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-226-1" name="__codelineno-226-1" href="#__codelineno-226-1"></a>&gt;&gt;&gt; a = torch.rand(5)
<a id="__codelineno-226-2" name="__codelineno-226-2" href="#__codelineno-226-2"></a>&gt;&gt;&gt; a
<a id="__codelineno-226-3" name="__codelineno-226-3" href="#__codelineno-226-3"></a>tensor([ 0.8419,  0.8003,  0.9971,  0.5287,  0.0490])
<a id="__codelineno-226-4" name="__codelineno-226-4" href="#__codelineno-226-4"></a>
<a id="__codelineno-226-5" name="__codelineno-226-5" href="#__codelineno-226-5"></a>&gt;&gt;&gt; torch.log2(a)
<a id="__codelineno-226-6" name="__codelineno-226-6" href="#__codelineno-226-6"></a>tensor([-0.2483, -0.3213, -0.0042, -0.9196, -4.3504])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-227-1" name="__codelineno-227-1" href="#__codelineno-227-1"></a>torch.logical_not(input, out=None) → Tensor¶
</code></pre></div>
<p>计算给定输入张量的按元素逻辑非。 如果未指定，则输出张量将具有 bool dtype。 如果输入张量不是布尔张量，则将零视为<code>False</code>，将非零视为<code>True</code>。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-228-1" name="__codelineno-228-1" href="#__codelineno-228-1"></a>&gt;&gt;&gt; torch.logical_not(torch.tensor([True, False]))
<a id="__codelineno-228-2" name="__codelineno-228-2" href="#__codelineno-228-2"></a>tensor([ False,  True])
<a id="__codelineno-228-3" name="__codelineno-228-3" href="#__codelineno-228-3"></a>&gt;&gt;&gt; torch.logical_not(torch.tensor([0, 1, -10], dtype=torch.int8))
<a id="__codelineno-228-4" name="__codelineno-228-4" href="#__codelineno-228-4"></a>tensor([ True, False, False])
<a id="__codelineno-228-5" name="__codelineno-228-5" href="#__codelineno-228-5"></a>&gt;&gt;&gt; torch.logical_not(torch.tensor([0., 1.5, -10.], dtype=torch.double))
<a id="__codelineno-228-6" name="__codelineno-228-6" href="#__codelineno-228-6"></a>tensor([ True, False, False])
<a id="__codelineno-228-7" name="__codelineno-228-7" href="#__codelineno-228-7"></a>&gt;&gt;&gt; torch.logical_not(torch.tensor([0., 1., -10.], dtype=torch.double), out=torch.empty(3, dtype=torch.int16))
<a id="__codelineno-228-8" name="__codelineno-228-8" href="#__codelineno-228-8"></a>tensor([1, 0, 0], dtype=torch.int16)
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-229-1" name="__codelineno-229-1" href="#__codelineno-229-1"></a>torch.logical_xor(input, other, out=None) → Tensor¶
</code></pre></div>
<p>计算给定输入张量的逐元素逻辑 XOR。 零被视为<code>False</code>，非零被视为<code>True</code>。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>其他</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–用于计算 XOR 的张量</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-230-1" name="__codelineno-230-1" href="#__codelineno-230-1"></a>&gt;&gt;&gt; torch.logical_xor(torch.tensor([True, False, True]), torch.tensor([True, False, False]))
<a id="__codelineno-230-2" name="__codelineno-230-2" href="#__codelineno-230-2"></a>tensor([ False, False,  True])
<a id="__codelineno-230-3" name="__codelineno-230-3" href="#__codelineno-230-3"></a>&gt;&gt;&gt; a = torch.tensor([0, 1, 10, 0], dtype=torch.int8)
<a id="__codelineno-230-4" name="__codelineno-230-4" href="#__codelineno-230-4"></a>&gt;&gt;&gt; b = torch.tensor([4, 0, 1, 0], dtype=torch.int8)
<a id="__codelineno-230-5" name="__codelineno-230-5" href="#__codelineno-230-5"></a>&gt;&gt;&gt; torch.logical_xor(a, b)
<a id="__codelineno-230-6" name="__codelineno-230-6" href="#__codelineno-230-6"></a>tensor([ True,  True, False, False])
<a id="__codelineno-230-7" name="__codelineno-230-7" href="#__codelineno-230-7"></a>&gt;&gt;&gt; torch.logical_xor(a.double(), b.double())
<a id="__codelineno-230-8" name="__codelineno-230-8" href="#__codelineno-230-8"></a>tensor([ True,  True, False, False])
<a id="__codelineno-230-9" name="__codelineno-230-9" href="#__codelineno-230-9"></a>&gt;&gt;&gt; torch.logical_xor(a.double(), b)
<a id="__codelineno-230-10" name="__codelineno-230-10" href="#__codelineno-230-10"></a>tensor([ True,  True, False, False])
<a id="__codelineno-230-11" name="__codelineno-230-11" href="#__codelineno-230-11"></a>&gt;&gt;&gt; torch.logical_xor(a, b, out=torch.empty(4, dtype=torch.bool))
<a id="__codelineno-230-12" name="__codelineno-230-12" href="#__codelineno-230-12"></a>tensor([ True,  True, False, False])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-231-1" name="__codelineno-231-1" href="#__codelineno-231-1"></a>torch.mul()¶
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-232-1" name="__codelineno-232-1" href="#__codelineno-232-1"></a>torch.mul(input, other, out=None)
</code></pre></div>
<p>将输入<code>input</code>的每个元素与标量<code>other</code>相乘，并返回一个新的结果张量。</p>
<p><img alt="" src="../img/b9353a390a47f91fc123b9db37178994.jpg" /></p>
<p>如果<code>input</code>的类型为 &lt;cite&gt;FloatTensor&lt;/cite&gt; 或 &lt;cite&gt;DoubleTensor&lt;/cite&gt; ，则<code>other</code>应为实数，否则应为整数</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>{输入}</strong> –</p>
</li>
<li>
<p><strong>值</strong>(<em>数字</em>）–要与<code>input</code>的每个元素相乘的数字</p>
</li>
<li>
<p><strong>{out}</strong> –</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-233-1" name="__codelineno-233-1" href="#__codelineno-233-1"></a>&gt;&gt;&gt; a = torch.randn(3)
<a id="__codelineno-233-2" name="__codelineno-233-2" href="#__codelineno-233-2"></a>&gt;&gt;&gt; a
<a id="__codelineno-233-3" name="__codelineno-233-3" href="#__codelineno-233-3"></a>tensor([ 0.2015, -0.4255,  2.6087])
<a id="__codelineno-233-4" name="__codelineno-233-4" href="#__codelineno-233-4"></a>&gt;&gt;&gt; torch.mul(a, 100)
<a id="__codelineno-233-5" name="__codelineno-233-5" href="#__codelineno-233-5"></a>tensor([  20.1494,  -42.5491,  260.8663])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-234-1" name="__codelineno-234-1" href="#__codelineno-234-1"></a>torch.mul(input, other, out=None)
</code></pre></div>
<p>张量<code>input</code>的每个元素乘以张量<code>other</code>的相应元素。 返回结果张量。</p>
<p>The shapes of <code>input</code> and <code>other</code> must be <a href="notes/broadcasting.html#broadcasting-semantics">broadcastable</a>.</p>
<p><img alt="" src="../img/e7c84cbd2cee400eafabf79e53ed4a59.jpg" /></p>
<p>Parameters</p>
<ul>
<li>
<p><strong>输入</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–第一个被乘张量</p>
</li>
<li>
<p><strong>其他</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–第二个被乘张量</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-235-1" name="__codelineno-235-1" href="#__codelineno-235-1"></a>&gt;&gt;&gt; a = torch.randn(4, 1)
<a id="__codelineno-235-2" name="__codelineno-235-2" href="#__codelineno-235-2"></a>&gt;&gt;&gt; a
<a id="__codelineno-235-3" name="__codelineno-235-3" href="#__codelineno-235-3"></a>tensor([[ 1.1207],
<a id="__codelineno-235-4" name="__codelineno-235-4" href="#__codelineno-235-4"></a>        [-0.3137],
<a id="__codelineno-235-5" name="__codelineno-235-5" href="#__codelineno-235-5"></a>        [ 0.0700],
<a id="__codelineno-235-6" name="__codelineno-235-6" href="#__codelineno-235-6"></a>        [ 0.8378]])
<a id="__codelineno-235-7" name="__codelineno-235-7" href="#__codelineno-235-7"></a>&gt;&gt;&gt; b = torch.randn(1, 4)
<a id="__codelineno-235-8" name="__codelineno-235-8" href="#__codelineno-235-8"></a>&gt;&gt;&gt; b
<a id="__codelineno-235-9" name="__codelineno-235-9" href="#__codelineno-235-9"></a>tensor([[ 0.5146,  0.1216, -0.5244,  2.2382]])
<a id="__codelineno-235-10" name="__codelineno-235-10" href="#__codelineno-235-10"></a>&gt;&gt;&gt; torch.mul(a, b)
<a id="__codelineno-235-11" name="__codelineno-235-11" href="#__codelineno-235-11"></a>tensor([[ 0.5767,  0.1363, -0.5877,  2.5083],
<a id="__codelineno-235-12" name="__codelineno-235-12" href="#__codelineno-235-12"></a>        [-0.1614, -0.0382,  0.1645, -0.7021],
<a id="__codelineno-235-13" name="__codelineno-235-13" href="#__codelineno-235-13"></a>        [ 0.0360,  0.0085, -0.0367,  0.1567],
<a id="__codelineno-235-14" name="__codelineno-235-14" href="#__codelineno-235-14"></a>        [ 0.4312,  0.1019, -0.4394,  1.8753]])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-236-1" name="__codelineno-236-1" href="#__codelineno-236-1"></a>torch.mvlgamma(input, p) → Tensor¶
</code></pre></div>
<p>计算元素对数为维度<img alt="" src="../img/28f674762c8a83d24f3018848e6314d5.jpg" />的多元对数伽马函数 (<a href="https://en.wikipedia.org/wiki/Multivariate_gamma_function">[reference]</a>)，公式为</p>
<p><img alt="" src="../img/e04b3e1a381314038fdb290092d532c3.jpg" /></p>
<p>其中<img alt="" src="../img/608c3ee390b6bc4569f2210e1e011ed4.jpg" />和<img alt="" src="../img/f7fbd871ee76d8efb8317c342dd5ed9a.jpg" />是伽玛函数。</p>
<p>如果任何元素小于或等于<img alt="" src="../img/095b4338ff8c0e8670fe99e82033b339.jpg" />，那么将引发错误。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>输入</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–用于计算多元对数伽马函数的张量</p>
</li>
<li>
<p><strong>p</strong>  (<em>python：int</em> )–尺寸数</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-237-1" name="__codelineno-237-1" href="#__codelineno-237-1"></a>&gt;&gt;&gt; a = torch.empty(2, 3).uniform_(1, 2)
<a id="__codelineno-237-2" name="__codelineno-237-2" href="#__codelineno-237-2"></a>&gt;&gt;&gt; a
<a id="__codelineno-237-3" name="__codelineno-237-3" href="#__codelineno-237-3"></a>tensor([[1.6835, 1.8474, 1.1929],
<a id="__codelineno-237-4" name="__codelineno-237-4" href="#__codelineno-237-4"></a>        [1.0475, 1.7162, 1.4180]])
<a id="__codelineno-237-5" name="__codelineno-237-5" href="#__codelineno-237-5"></a>&gt;&gt;&gt; torch.mvlgamma(a, 2)
<a id="__codelineno-237-6" name="__codelineno-237-6" href="#__codelineno-237-6"></a>tensor([[0.3928, 0.4007, 0.7586],
<a id="__codelineno-237-7" name="__codelineno-237-7" href="#__codelineno-237-7"></a>        [1.0311, 0.3901, 0.5049]])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-238-1" name="__codelineno-238-1" href="#__codelineno-238-1"></a>torch.neg(input, out=None) → Tensor¶
</code></pre></div>
<p>返回带有<code>input</code>元素负数的新张量。</p>
<p><img alt="" src="../img/95cc4605d1ad5dd05793898a56b9b7c4.jpg" /></p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-239-1" name="__codelineno-239-1" href="#__codelineno-239-1"></a>&gt;&gt;&gt; a = torch.randn(5)
<a id="__codelineno-239-2" name="__codelineno-239-2" href="#__codelineno-239-2"></a>&gt;&gt;&gt; a
<a id="__codelineno-239-3" name="__codelineno-239-3" href="#__codelineno-239-3"></a>tensor([ 0.0090, -0.2262, -0.0682, -0.2866,  0.3940])
<a id="__codelineno-239-4" name="__codelineno-239-4" href="#__codelineno-239-4"></a>&gt;&gt;&gt; torch.neg(a)
<a id="__codelineno-239-5" name="__codelineno-239-5" href="#__codelineno-239-5"></a>tensor([-0.0090,  0.2262,  0.0682,  0.2866, -0.3940])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-240-1" name="__codelineno-240-1" href="#__codelineno-240-1"></a>torch.polygamma(n, input, out=None) → Tensor¶
</code></pre></div>
<p>计算<code>input</code>上的 digamma 函数的<img alt="" src="../img/e790323c220b523037f6493b45a6be2f.jpg" />导数。 <img alt="" src="../img/5131c6ffc9046f0af6229a8f78fea321.jpg" />被称为多伽玛函数的阶数。</p>
<p><img alt="" src="../img/b6aa0cf4b0167dab157397df97cd44ad.jpg" /></p>
<p>Note</p>
<p><img alt="" src="../img/f08d8837b69470f2939526fe3f47dbf5.jpg" />未实现此功能。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>n</strong>  (<em>python：int</em> )– polygamma 函数的顺序</p>
</li>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<div class="highlight"><pre><span></span><code><a id="__codelineno-241-1" name="__codelineno-241-1" href="#__codelineno-241-1"></a>Example::
</code></pre></div>
<div class="highlight"><pre><span></span><code><a id="__codelineno-242-1" name="__codelineno-242-1" href="#__codelineno-242-1"></a>&gt;&gt;&gt; a = torch.tensor([1, 0.5])
<a id="__codelineno-242-2" name="__codelineno-242-2" href="#__codelineno-242-2"></a>&gt;&gt;&gt; torch.polygamma(1, a)
<a id="__codelineno-242-3" name="__codelineno-242-3" href="#__codelineno-242-3"></a>tensor([1.64493, 4.9348])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-243-1" name="__codelineno-243-1" href="#__codelineno-243-1"></a>torch.pow()¶
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-244-1" name="__codelineno-244-1" href="#__codelineno-244-1"></a>torch.pow(input, exponent, out=None) → Tensor
</code></pre></div>
<p>用<code>exponent</code>取<code>input</code>中每个元素的幂，并返回张量与结果。</p>
<p><code>exponent</code>可以是单个<code>float</code>数字，也可以是具有与<code>input</code>相同元素数的&lt;cite&gt;张量&lt;/cite&gt;。</p>
<p>当<code>exponent</code>为标量值时，应用的运算为：</p>
<p><img alt="" src="../img/71ad83ee7a70b4a097c1393f30ffdf2f.jpg" /></p>
<p>当<code>exponent</code>是张量时，应用的运算是：</p>
<p><img alt="" src="../img/2f65f5d5f3468da870a791f8985d4dba.jpg" /></p>
<p>当<code>exponent</code>是张量时，<code>input</code>和<code>exponent</code>的形状必须是<a href="notes/broadcasting.html#broadcasting-semantics">可广播的</a>。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>指数</strong> (<em>python：float</em> <em>或</em> <em>tensor</em>）–指数值</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-245-1" name="__codelineno-245-1" href="#__codelineno-245-1"></a>&gt;&gt;&gt; a = torch.randn(4)
<a id="__codelineno-245-2" name="__codelineno-245-2" href="#__codelineno-245-2"></a>&gt;&gt;&gt; a
<a id="__codelineno-245-3" name="__codelineno-245-3" href="#__codelineno-245-3"></a>tensor([ 0.4331,  1.2475,  0.6834, -0.2791])
<a id="__codelineno-245-4" name="__codelineno-245-4" href="#__codelineno-245-4"></a>&gt;&gt;&gt; torch.pow(a, 2)
<a id="__codelineno-245-5" name="__codelineno-245-5" href="#__codelineno-245-5"></a>tensor([ 0.1875,  1.5561,  0.4670,  0.0779])
<a id="__codelineno-245-6" name="__codelineno-245-6" href="#__codelineno-245-6"></a>&gt;&gt;&gt; exp = torch.arange(1., 5.)
<a id="__codelineno-245-7" name="__codelineno-245-7" href="#__codelineno-245-7"></a>
<a id="__codelineno-245-8" name="__codelineno-245-8" href="#__codelineno-245-8"></a>&gt;&gt;&gt; a = torch.arange(1., 5.)
<a id="__codelineno-245-9" name="__codelineno-245-9" href="#__codelineno-245-9"></a>&gt;&gt;&gt; a
<a id="__codelineno-245-10" name="__codelineno-245-10" href="#__codelineno-245-10"></a>tensor([ 1.,  2.,  3.,  4.])
<a id="__codelineno-245-11" name="__codelineno-245-11" href="#__codelineno-245-11"></a>&gt;&gt;&gt; exp
<a id="__codelineno-245-12" name="__codelineno-245-12" href="#__codelineno-245-12"></a>tensor([ 1.,  2.,  3.,  4.])
<a id="__codelineno-245-13" name="__codelineno-245-13" href="#__codelineno-245-13"></a>&gt;&gt;&gt; torch.pow(a, exp)
<a id="__codelineno-245-14" name="__codelineno-245-14" href="#__codelineno-245-14"></a>tensor([   1.,    4.,   27.,  256.])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-246-1" name="__codelineno-246-1" href="#__codelineno-246-1"></a>torch.pow(self, exponent, out=None) → Tensor
</code></pre></div>
<p><code>self</code>是标量<code>float</code>值，<code>exponent</code>是张量。 返回的张量<code>out</code>与<code>exponent</code>的形状相同</p>
<p>应用的操作是：</p>
<p><img alt="" src="../img/8bfb21a8eb8a28ff6ce228b2b58f69e5.jpg" /></p>
<p>Parameters</p>
<ul>
<li>
<p><strong>自我</strong> (<em>python：float</em> )–幂运算的标量基值</p>
</li>
<li>
<p><strong>指数</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–指数张量</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-247-1" name="__codelineno-247-1" href="#__codelineno-247-1"></a>&gt;&gt;&gt; exp = torch.arange(1., 5.)
<a id="__codelineno-247-2" name="__codelineno-247-2" href="#__codelineno-247-2"></a>&gt;&gt;&gt; base = 2
<a id="__codelineno-247-3" name="__codelineno-247-3" href="#__codelineno-247-3"></a>&gt;&gt;&gt; torch.pow(base, exp)
<a id="__codelineno-247-4" name="__codelineno-247-4" href="#__codelineno-247-4"></a>tensor([  2.,   4.,   8.,  16.])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-248-1" name="__codelineno-248-1" href="#__codelineno-248-1"></a>torch.real(input, out=None) → Tensor¶
</code></pre></div>
<p>计算给定<code>input</code>张量的逐元素实数值。</p>
<p><img alt="" src="../img/831fe304d759480b4e6403bb0dce7b41.jpg" /></p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-249-1" name="__codelineno-249-1" href="#__codelineno-249-1"></a>&gt;&gt;&gt; torch.real(torch.tensor([-1 + 1j, -2 + 2j, 3 - 3j]))
<a id="__codelineno-249-2" name="__codelineno-249-2" href="#__codelineno-249-2"></a>tensor([ -1,  -2,  3])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-250-1" name="__codelineno-250-1" href="#__codelineno-250-1"></a>torch.reciprocal(input, out=None) → Tensor¶
</code></pre></div>
<p>返回带有<code>input</code>元素倒数的新张量</p>
<p><img alt="" src="../img/53b2c49d0eb01403c24924c5088215a4.jpg" /></p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-251-1" name="__codelineno-251-1" href="#__codelineno-251-1"></a>&gt;&gt;&gt; a = torch.randn(4)
<a id="__codelineno-251-2" name="__codelineno-251-2" href="#__codelineno-251-2"></a>&gt;&gt;&gt; a
<a id="__codelineno-251-3" name="__codelineno-251-3" href="#__codelineno-251-3"></a>tensor([-0.4595, -2.1219, -1.4314,  0.7298])
<a id="__codelineno-251-4" name="__codelineno-251-4" href="#__codelineno-251-4"></a>&gt;&gt;&gt; torch.reciprocal(a)
<a id="__codelineno-251-5" name="__codelineno-251-5" href="#__codelineno-251-5"></a>tensor([-2.1763, -0.4713, -0.6986,  1.3702])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-252-1" name="__codelineno-252-1" href="#__codelineno-252-1"></a>torch.remainder(input, other, out=None) → Tensor¶
</code></pre></div>
<p>Computes the element-wise remainder of division.</p>
<p>除数和除数可以同时包含整数和浮点数。 其余部分与除数的符号相同。</p>
<p>When <code>other</code> is a tensor, the shapes of <code>input</code> and <code>other</code> must be <a href="notes/broadcasting.html#broadcasting-semantics">broadcastable</a>.</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the dividend</p>
</li>
<li>
<p><strong>其他</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a> <em>或</em> <em>python：float</em> )–除数可以是数字或张量 与股息形状相同</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-253-1" name="__codelineno-253-1" href="#__codelineno-253-1"></a>&gt;&gt;&gt; torch.remainder(torch.tensor([-3., -2, -1, 1, 2, 3]), 2)
<a id="__codelineno-253-2" name="__codelineno-253-2" href="#__codelineno-253-2"></a>tensor([ 1.,  0.,  1.,  1.,  0.,  1.])
<a id="__codelineno-253-3" name="__codelineno-253-3" href="#__codelineno-253-3"></a>&gt;&gt;&gt; torch.remainder(torch.tensor([1., 2, 3, 4, 5]), 1.5)
<a id="__codelineno-253-4" name="__codelineno-253-4" href="#__codelineno-253-4"></a>tensor([ 1.0000,  0.5000,  0.0000,  1.0000,  0.5000])
</code></pre></div>
<p>也可以看看</p>
<p><a href="#torch.fmod" title="torch.fmod"><code>torch.fmod()</code></a> ，它等效于 C 库函数<code>fmod()</code>来计算元素的除法余数。</p>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-254-1" name="__codelineno-254-1" href="#__codelineno-254-1"></a>torch.round(input, out=None) → Tensor¶
</code></pre></div>
<p>返回一个新的张量，其中<code>input</code>的每个元素都舍入到最接近的整数。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-255-1" name="__codelineno-255-1" href="#__codelineno-255-1"></a>&gt;&gt;&gt; a = torch.randn(4)
<a id="__codelineno-255-2" name="__codelineno-255-2" href="#__codelineno-255-2"></a>&gt;&gt;&gt; a
<a id="__codelineno-255-3" name="__codelineno-255-3" href="#__codelineno-255-3"></a>tensor([ 0.9920,  0.6077,  0.9734, -1.0362])
<a id="__codelineno-255-4" name="__codelineno-255-4" href="#__codelineno-255-4"></a>&gt;&gt;&gt; torch.round(a)
<a id="__codelineno-255-5" name="__codelineno-255-5" href="#__codelineno-255-5"></a>tensor([ 1.,  1.,  1., -1.])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-256-1" name="__codelineno-256-1" href="#__codelineno-256-1"></a>torch.rsqrt(input, out=None) → Tensor¶
</code></pre></div>
<p>返回带有<code>input</code>的每个元素的平方根的倒数的新张量。</p>
<p><img alt="" src="../img/b203a1112169097b0c446f64347eee67.jpg" /></p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-257-1" name="__codelineno-257-1" href="#__codelineno-257-1"></a>&gt;&gt;&gt; a = torch.randn(4)
<a id="__codelineno-257-2" name="__codelineno-257-2" href="#__codelineno-257-2"></a>&gt;&gt;&gt; a
<a id="__codelineno-257-3" name="__codelineno-257-3" href="#__codelineno-257-3"></a>tensor([-0.0370,  0.2970,  1.5420, -0.9105])
<a id="__codelineno-257-4" name="__codelineno-257-4" href="#__codelineno-257-4"></a>&gt;&gt;&gt; torch.rsqrt(a)
<a id="__codelineno-257-5" name="__codelineno-257-5" href="#__codelineno-257-5"></a>tensor([    nan,  1.8351,  0.8053,     nan])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-258-1" name="__codelineno-258-1" href="#__codelineno-258-1"></a>torch.sigmoid(input, out=None) → Tensor¶
</code></pre></div>
<p>返回具有<code>input</code>元素的 S 形的新张量。</p>
<p><img alt="" src="../img/9580d259e0be66d0dc07cf0c8c19fc80.jpg" /></p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-259-1" name="__codelineno-259-1" href="#__codelineno-259-1"></a>&gt;&gt;&gt; a = torch.randn(4)
<a id="__codelineno-259-2" name="__codelineno-259-2" href="#__codelineno-259-2"></a>&gt;&gt;&gt; a
<a id="__codelineno-259-3" name="__codelineno-259-3" href="#__codelineno-259-3"></a>tensor([ 0.9213,  1.0887, -0.8858, -1.7683])
<a id="__codelineno-259-4" name="__codelineno-259-4" href="#__codelineno-259-4"></a>&gt;&gt;&gt; torch.sigmoid(a)
<a id="__codelineno-259-5" name="__codelineno-259-5" href="#__codelineno-259-5"></a>tensor([ 0.7153,  0.7481,  0.2920,  0.1458])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-260-1" name="__codelineno-260-1" href="#__codelineno-260-1"></a>torch.sign(input, out=None) → Tensor¶
</code></pre></div>
<p>返回带有<code>input</code>元素符号的新张量。</p>
<p><img alt="" src="../img/b1970c66011a7d9a1f7a3f76d56b69a5.jpg" /></p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-261-1" name="__codelineno-261-1" href="#__codelineno-261-1"></a>&gt;&gt;&gt; a = torch.tensor([0.7, -1.2, 0., 2.3])
<a id="__codelineno-261-2" name="__codelineno-261-2" href="#__codelineno-261-2"></a>&gt;&gt;&gt; a
<a id="__codelineno-261-3" name="__codelineno-261-3" href="#__codelineno-261-3"></a>tensor([ 0.7000, -1.2000,  0.0000,  2.3000])
<a id="__codelineno-261-4" name="__codelineno-261-4" href="#__codelineno-261-4"></a>&gt;&gt;&gt; torch.sign(a)
<a id="__codelineno-261-5" name="__codelineno-261-5" href="#__codelineno-261-5"></a>tensor([ 1., -1.,  0.,  1.])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-262-1" name="__codelineno-262-1" href="#__codelineno-262-1"></a>torch.sin(input, out=None) → Tensor¶
</code></pre></div>
<p>返回带有<code>input</code>元素正弦值的新张量。</p>
<p><img alt="" src="../img/b2949ef2a0b48170c3ee143bbcbf9575.jpg" /></p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-263-1" name="__codelineno-263-1" href="#__codelineno-263-1"></a>&gt;&gt;&gt; a = torch.randn(4)
<a id="__codelineno-263-2" name="__codelineno-263-2" href="#__codelineno-263-2"></a>&gt;&gt;&gt; a
<a id="__codelineno-263-3" name="__codelineno-263-3" href="#__codelineno-263-3"></a>tensor([-0.5461,  0.1347, -2.7266, -0.2746])
<a id="__codelineno-263-4" name="__codelineno-263-4" href="#__codelineno-263-4"></a>&gt;&gt;&gt; torch.sin(a)
<a id="__codelineno-263-5" name="__codelineno-263-5" href="#__codelineno-263-5"></a>tensor([-0.5194,  0.1343, -0.4032, -0.2711])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-264-1" name="__codelineno-264-1" href="#__codelineno-264-1"></a>torch.sinh(input, out=None) → Tensor¶
</code></pre></div>
<p>返回具有<code>input</code>元素的双曲正弦值的新张量。</p>
<p><img alt="" src="../img/f701007992bbd41bad0c0d47fa17cff9.jpg" /></p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-265-1" name="__codelineno-265-1" href="#__codelineno-265-1"></a>&gt;&gt;&gt; a = torch.randn(4)
<a id="__codelineno-265-2" name="__codelineno-265-2" href="#__codelineno-265-2"></a>&gt;&gt;&gt; a
<a id="__codelineno-265-3" name="__codelineno-265-3" href="#__codelineno-265-3"></a>tensor([ 0.5380, -0.8632, -0.1265,  0.9399])
<a id="__codelineno-265-4" name="__codelineno-265-4" href="#__codelineno-265-4"></a>&gt;&gt;&gt; torch.sinh(a)
<a id="__codelineno-265-5" name="__codelineno-265-5" href="#__codelineno-265-5"></a>tensor([ 0.5644, -0.9744, -0.1268,  1.0845])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-266-1" name="__codelineno-266-1" href="#__codelineno-266-1"></a>torch.sqrt(input, out=None) → Tensor¶
</code></pre></div>
<p>返回具有<code>input</code>元素平方根的新张量。</p>
<p><img alt="" src="../img/8d503a40d472f4e83ed2f136b2d59121.jpg" /></p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-267-1" name="__codelineno-267-1" href="#__codelineno-267-1"></a>&gt;&gt;&gt; a = torch.randn(4)
<a id="__codelineno-267-2" name="__codelineno-267-2" href="#__codelineno-267-2"></a>&gt;&gt;&gt; a
<a id="__codelineno-267-3" name="__codelineno-267-3" href="#__codelineno-267-3"></a>tensor([-2.0755,  1.0226,  0.0831,  0.4806])
<a id="__codelineno-267-4" name="__codelineno-267-4" href="#__codelineno-267-4"></a>&gt;&gt;&gt; torch.sqrt(a)
<a id="__codelineno-267-5" name="__codelineno-267-5" href="#__codelineno-267-5"></a>tensor([    nan,  1.0112,  0.2883,  0.6933])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-268-1" name="__codelineno-268-1" href="#__codelineno-268-1"></a>torch.tan(input, out=None) → Tensor¶
</code></pre></div>
<p>返回带有<code>input</code>元素的切线的新张量。</p>
<p><img alt="" src="../img/2b43ec9d6d179b58d128269ab29a3a9e.jpg" /></p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-269-1" name="__codelineno-269-1" href="#__codelineno-269-1"></a>&gt;&gt;&gt; a = torch.randn(4)
<a id="__codelineno-269-2" name="__codelineno-269-2" href="#__codelineno-269-2"></a>&gt;&gt;&gt; a
<a id="__codelineno-269-3" name="__codelineno-269-3" href="#__codelineno-269-3"></a>tensor([-1.2027, -1.7687,  0.4412, -1.3856])
<a id="__codelineno-269-4" name="__codelineno-269-4" href="#__codelineno-269-4"></a>&gt;&gt;&gt; torch.tan(a)
<a id="__codelineno-269-5" name="__codelineno-269-5" href="#__codelineno-269-5"></a>tensor([-2.5930,  4.9859,  0.4722, -5.3366])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-270-1" name="__codelineno-270-1" href="#__codelineno-270-1"></a>torch.tanh(input, out=None) → Tensor¶
</code></pre></div>
<p>返回具有<code>input</code>元素的双曲正切值的新张量。</p>
<p><img alt="" src="../img/7d427ae6124c99de58f1f36b2ae6d5f6.jpg" /></p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-271-1" name="__codelineno-271-1" href="#__codelineno-271-1"></a>&gt;&gt;&gt; a = torch.randn(4)
<a id="__codelineno-271-2" name="__codelineno-271-2" href="#__codelineno-271-2"></a>&gt;&gt;&gt; a
<a id="__codelineno-271-3" name="__codelineno-271-3" href="#__codelineno-271-3"></a>tensor([ 0.8986, -0.7279,  1.1745,  0.2611])
<a id="__codelineno-271-4" name="__codelineno-271-4" href="#__codelineno-271-4"></a>&gt;&gt;&gt; torch.tanh(a)
<a id="__codelineno-271-5" name="__codelineno-271-5" href="#__codelineno-271-5"></a>tensor([ 0.7156, -0.6218,  0.8257,  0.2553])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-272-1" name="__codelineno-272-1" href="#__codelineno-272-1"></a>torch.trunc(input, out=None) → Tensor¶
</code></pre></div>
<p>返回一个新的张量，该张量具有<code>input</code>元素的截断的整数值。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-273-1" name="__codelineno-273-1" href="#__codelineno-273-1"></a>&gt;&gt;&gt; a = torch.randn(4)
<a id="__codelineno-273-2" name="__codelineno-273-2" href="#__codelineno-273-2"></a>&gt;&gt;&gt; a
<a id="__codelineno-273-3" name="__codelineno-273-3" href="#__codelineno-273-3"></a>tensor([ 3.4742,  0.5466, -0.8008, -0.9079])
<a id="__codelineno-273-4" name="__codelineno-273-4" href="#__codelineno-273-4"></a>&gt;&gt;&gt; torch.trunc(a)
<a id="__codelineno-273-5" name="__codelineno-273-5" href="#__codelineno-273-5"></a>tensor([ 3.,  0., -0., -0.])
</code></pre></div>
<h3 id="_13">减少操作</h3>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-274-1" name="__codelineno-274-1" href="#__codelineno-274-1"></a>torch.argmax()¶
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-275-1" name="__codelineno-275-1" href="#__codelineno-275-1"></a>torch.argmax(input) → LongTensor
</code></pre></div>
<p>返回<code>input</code>张量中所有元素的最大值的索引。</p>
<p>这是 <a href="#torch.max" title="torch.max"><code>torch.max()</code></a> 返回的第二个值。 有关此方法的确切语义，请参见其文档。</p>
<p>Parameters</p>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-276-1" name="__codelineno-276-1" href="#__codelineno-276-1"></a>&gt;&gt;&gt; a = torch.randn(4, 4)
<a id="__codelineno-276-2" name="__codelineno-276-2" href="#__codelineno-276-2"></a>&gt;&gt;&gt; a
<a id="__codelineno-276-3" name="__codelineno-276-3" href="#__codelineno-276-3"></a>tensor([[ 1.3398,  0.2663, -0.2686,  0.2450],
<a id="__codelineno-276-4" name="__codelineno-276-4" href="#__codelineno-276-4"></a>        [-0.7401, -0.8805, -0.3402, -1.1936],
<a id="__codelineno-276-5" name="__codelineno-276-5" href="#__codelineno-276-5"></a>        [ 0.4907, -1.3948, -1.0691, -0.3132],
<a id="__codelineno-276-6" name="__codelineno-276-6" href="#__codelineno-276-6"></a>        [-1.6092,  0.5419, -0.2993,  0.3195]])
<a id="__codelineno-276-7" name="__codelineno-276-7" href="#__codelineno-276-7"></a>&gt;&gt;&gt; torch.argmax(a)
<a id="__codelineno-276-8" name="__codelineno-276-8" href="#__codelineno-276-8"></a>tensor(0)
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-277-1" name="__codelineno-277-1" href="#__codelineno-277-1"></a>torch.argmax(input, dim, keepdim=False) → LongTensor
</code></pre></div>
<p>返回一个维度上张量最大值的索引。</p>
<p>This is the second value returned by <a href="#torch.max" title="torch.max"><code>torch.max()</code></a>. See its documentation for the exact semantics of this method.</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>暗淡的</strong> (<em>python：int</em> )–缩小的尺寸。 如果<code>None</code>，则返回扁平化输入的 argmax。</p>
</li>
<li>
<p><strong>keepdim</strong>  (<em>bool</em> )–输出张量是否保留<code>dim</code>。 忽略<code>dim=None</code>。</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-278-1" name="__codelineno-278-1" href="#__codelineno-278-1"></a>&gt;&gt;&gt; a = torch.randn(4, 4)
<a id="__codelineno-278-2" name="__codelineno-278-2" href="#__codelineno-278-2"></a>&gt;&gt;&gt; a
<a id="__codelineno-278-3" name="__codelineno-278-3" href="#__codelineno-278-3"></a>tensor([[ 1.3398,  0.2663, -0.2686,  0.2450],
<a id="__codelineno-278-4" name="__codelineno-278-4" href="#__codelineno-278-4"></a>        [-0.7401, -0.8805, -0.3402, -1.1936],
<a id="__codelineno-278-5" name="__codelineno-278-5" href="#__codelineno-278-5"></a>        [ 0.4907, -1.3948, -1.0691, -0.3132],
<a id="__codelineno-278-6" name="__codelineno-278-6" href="#__codelineno-278-6"></a>        [-1.6092,  0.5419, -0.2993,  0.3195]])
<a id="__codelineno-278-7" name="__codelineno-278-7" href="#__codelineno-278-7"></a>&gt;&gt;&gt; torch.argmax(a, dim=1)
<a id="__codelineno-278-8" name="__codelineno-278-8" href="#__codelineno-278-8"></a>tensor([ 0,  2,  0,  1])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-279-1" name="__codelineno-279-1" href="#__codelineno-279-1"></a>torch.argmin()¶
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-280-1" name="__codelineno-280-1" href="#__codelineno-280-1"></a>torch.argmin(input) → LongTensor
</code></pre></div>
<p>返回<code>input</code>张量中所有元素的最小值的索引。</p>
<p>这是 <a href="#torch.min" title="torch.min"><code>torch.min()</code></a> 返回的第二个值。 有关此方法的确切语义，请参见其文档。</p>
<p>Parameters</p>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-281-1" name="__codelineno-281-1" href="#__codelineno-281-1"></a>&gt;&gt;&gt; a = torch.randn(4, 4)
<a id="__codelineno-281-2" name="__codelineno-281-2" href="#__codelineno-281-2"></a>&gt;&gt;&gt; a
<a id="__codelineno-281-3" name="__codelineno-281-3" href="#__codelineno-281-3"></a>tensor([[ 0.1139,  0.2254, -0.1381,  0.3687],
<a id="__codelineno-281-4" name="__codelineno-281-4" href="#__codelineno-281-4"></a>        [ 1.0100, -1.1975, -0.0102, -0.4732],
<a id="__codelineno-281-5" name="__codelineno-281-5" href="#__codelineno-281-5"></a>        [-0.9240,  0.1207, -0.7506, -1.0213],
<a id="__codelineno-281-6" name="__codelineno-281-6" href="#__codelineno-281-6"></a>        [ 1.7809, -1.2960,  0.9384,  0.1438]])
<a id="__codelineno-281-7" name="__codelineno-281-7" href="#__codelineno-281-7"></a>&gt;&gt;&gt; torch.argmin(a)
<a id="__codelineno-281-8" name="__codelineno-281-8" href="#__codelineno-281-8"></a>tensor(13)
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-282-1" name="__codelineno-282-1" href="#__codelineno-282-1"></a>torch.argmin(input, dim, keepdim=False, out=None) → LongTensor
</code></pre></div>
<p>返回整个维度上张量的最小值的索引。</p>
<p>This is the second value returned by <a href="#torch.min" title="torch.min"><code>torch.min()</code></a>. See its documentation for the exact semantics of this method.</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>暗淡的</strong> (<em>python：int</em> )–缩小的尺寸。 如果<code>None</code>，则返回扁平化输入的 argmin。</p>
</li>
<li>
<p><strong>keepdim</strong> (<em>bool</em>) – whether the output tensor has <code>dim</code> retained or not. Ignored if <code>dim=None</code>.</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-283-1" name="__codelineno-283-1" href="#__codelineno-283-1"></a>&gt;&gt;&gt; a = torch.randn(4, 4)
<a id="__codelineno-283-2" name="__codelineno-283-2" href="#__codelineno-283-2"></a>&gt;&gt;&gt; a
<a id="__codelineno-283-3" name="__codelineno-283-3" href="#__codelineno-283-3"></a>tensor([[ 0.1139,  0.2254, -0.1381,  0.3687],
<a id="__codelineno-283-4" name="__codelineno-283-4" href="#__codelineno-283-4"></a>        [ 1.0100, -1.1975, -0.0102, -0.4732],
<a id="__codelineno-283-5" name="__codelineno-283-5" href="#__codelineno-283-5"></a>        [-0.9240,  0.1207, -0.7506, -1.0213],
<a id="__codelineno-283-6" name="__codelineno-283-6" href="#__codelineno-283-6"></a>        [ 1.7809, -1.2960,  0.9384,  0.1438]])
<a id="__codelineno-283-7" name="__codelineno-283-7" href="#__codelineno-283-7"></a>&gt;&gt;&gt; torch.argmin(a, dim=1)
<a id="__codelineno-283-8" name="__codelineno-283-8" href="#__codelineno-283-8"></a>tensor([ 2,  1,  3,  1])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-284-1" name="__codelineno-284-1" href="#__codelineno-284-1"></a>torch.dist(input, other, p=2) → Tensor¶
</code></pre></div>
<p>返回(<code>input</code>-<code>other</code>）的 p 范数</p>
<p>The shapes of <code>input</code> and <code>other</code> must be <a href="notes/broadcasting.html#broadcasting-semantics">broadcastable</a>.</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>其他</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–右侧输入张量</p>
</li>
<li>
<p><strong>p</strong>  (<em>python：float</em> <em>，</em> <em>可选</em>）–要计算的范数</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-285-1" name="__codelineno-285-1" href="#__codelineno-285-1"></a>&gt;&gt;&gt; x = torch.randn(4)
<a id="__codelineno-285-2" name="__codelineno-285-2" href="#__codelineno-285-2"></a>&gt;&gt;&gt; x
<a id="__codelineno-285-3" name="__codelineno-285-3" href="#__codelineno-285-3"></a>tensor([-1.5393, -0.8675,  0.5916,  1.6321])
<a id="__codelineno-285-4" name="__codelineno-285-4" href="#__codelineno-285-4"></a>&gt;&gt;&gt; y = torch.randn(4)
<a id="__codelineno-285-5" name="__codelineno-285-5" href="#__codelineno-285-5"></a>&gt;&gt;&gt; y
<a id="__codelineno-285-6" name="__codelineno-285-6" href="#__codelineno-285-6"></a>tensor([ 0.0967, -1.0511,  0.6295,  0.8360])
<a id="__codelineno-285-7" name="__codelineno-285-7" href="#__codelineno-285-7"></a>&gt;&gt;&gt; torch.dist(x, y, 3.5)
<a id="__codelineno-285-8" name="__codelineno-285-8" href="#__codelineno-285-8"></a>tensor(1.6727)
<a id="__codelineno-285-9" name="__codelineno-285-9" href="#__codelineno-285-9"></a>&gt;&gt;&gt; torch.dist(x, y, 3)
<a id="__codelineno-285-10" name="__codelineno-285-10" href="#__codelineno-285-10"></a>tensor(1.6973)
<a id="__codelineno-285-11" name="__codelineno-285-11" href="#__codelineno-285-11"></a>&gt;&gt;&gt; torch.dist(x, y, 0)
<a id="__codelineno-285-12" name="__codelineno-285-12" href="#__codelineno-285-12"></a>tensor(inf)
<a id="__codelineno-285-13" name="__codelineno-285-13" href="#__codelineno-285-13"></a>&gt;&gt;&gt; torch.dist(x, y, 1)
<a id="__codelineno-285-14" name="__codelineno-285-14" href="#__codelineno-285-14"></a>tensor(2.6537)
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-286-1" name="__codelineno-286-1" href="#__codelineno-286-1"></a>torch.logsumexp(input, dim, keepdim=False, out=None)¶
</code></pre></div>
<p>返回给定维度<code>dim</code>中<code>input</code>张量的每一行的总指数对数。 该计算在数值上是稳定的。</p>
<p>对于由&lt;cite&gt;昏暗&lt;/cite&gt;给出的总和指数<img alt="" src="../img/36608d1dd28464666846576485c40a7b.jpg" />和其他指数<img alt="" src="../img/db26d1a59be5965889bd4d5533b7be61.jpg" />，结果为</p>
<blockquote>
<p><img alt="" src="../img/9a755b15a82f2dcda096a2a363073062.jpg" /></p>
</blockquote>
<p>如果<code>keepdim</code>为<code>True</code>，则输出张量的大小与<code>input</code>相同，但尺寸为<code>dim</code>的大小为 1。否则，压缩<code>dim</code>(请参见 <a href="#torch.squeeze" title="torch.squeeze"><code>torch.squeeze()</code></a>)，导致输出张量的尺寸减少 1(或<code>len(dim)</code>）。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>暗淡的</strong> (<em>python：int</em> <em>或</em> <em>python：ints</em> 的元组）–要减小的尺寸。</p>
</li>
<li>
<p><strong>keepdim</strong>  (<em>bool</em> )–输出张量是否保留<code>dim</code>。</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<div class="highlight"><pre><span></span><code><a id="__codelineno-287-1" name="__codelineno-287-1" href="#__codelineno-287-1"></a>Example::
</code></pre></div>
<div class="highlight"><pre><span></span><code><a id="__codelineno-288-1" name="__codelineno-288-1" href="#__codelineno-288-1"></a>&gt;&gt;&gt; a = torch.randn(3, 3)
<a id="__codelineno-288-2" name="__codelineno-288-2" href="#__codelineno-288-2"></a>&gt;&gt;&gt; torch.logsumexp(a, 1)
<a id="__codelineno-288-3" name="__codelineno-288-3" href="#__codelineno-288-3"></a>tensor([ 0.8442,  1.4322,  0.8711])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-289-1" name="__codelineno-289-1" href="#__codelineno-289-1"></a>torch.mean()¶
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-290-1" name="__codelineno-290-1" href="#__codelineno-290-1"></a>torch.mean(input) → Tensor
</code></pre></div>
<p>返回<code>input</code>张量中所有元素的平均值。</p>
<p>Parameters</p>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-291-1" name="__codelineno-291-1" href="#__codelineno-291-1"></a>&gt;&gt;&gt; a = torch.randn(1, 3)
<a id="__codelineno-291-2" name="__codelineno-291-2" href="#__codelineno-291-2"></a>&gt;&gt;&gt; a
<a id="__codelineno-291-3" name="__codelineno-291-3" href="#__codelineno-291-3"></a>tensor([[ 0.2294, -0.5481,  1.3288]])
<a id="__codelineno-291-4" name="__codelineno-291-4" href="#__codelineno-291-4"></a>&gt;&gt;&gt; torch.mean(a)
<a id="__codelineno-291-5" name="__codelineno-291-5" href="#__codelineno-291-5"></a>tensor(0.3367)
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-292-1" name="__codelineno-292-1" href="#__codelineno-292-1"></a>torch.mean(input, dim, keepdim=False, out=None) → Tensor
</code></pre></div>
<p>返回给定维度<code>dim</code>中<code>input</code>张量的每一行的平均值。 如果<code>dim</code>是尺寸列表，请缩小所有尺寸。</p>
<p>If <code>keepdim</code> is <code>True</code>, the output tensor is of the same size as <code>input</code> except in the dimension(s) <code>dim</code> where it is of size 1. Otherwise, <code>dim</code> is squeezed (see <a href="#torch.squeeze" title="torch.squeeze"><code>torch.squeeze()</code></a>), resulting in the output tensor having 1 (or <code>len(dim)</code>) fewer dimension(s).</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>dim</strong> (<em>python:int</em> <em>or</em> <em>tuple of python:ints</em>) – the dimension or dimensions to reduce.</p>
</li>
<li>
<p><strong>keepdim</strong> (<em>bool</em>) – whether the output tensor has <code>dim</code> retained or not.</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-293-1" name="__codelineno-293-1" href="#__codelineno-293-1"></a>&gt;&gt;&gt; a = torch.randn(4, 4)
<a id="__codelineno-293-2" name="__codelineno-293-2" href="#__codelineno-293-2"></a>&gt;&gt;&gt; a
<a id="__codelineno-293-3" name="__codelineno-293-3" href="#__codelineno-293-3"></a>tensor([[-0.3841,  0.6320,  0.4254, -0.7384],
<a id="__codelineno-293-4" name="__codelineno-293-4" href="#__codelineno-293-4"></a>        [-0.9644,  1.0131, -0.6549, -1.4279],
<a id="__codelineno-293-5" name="__codelineno-293-5" href="#__codelineno-293-5"></a>        [-0.2951, -1.3350, -0.7694,  0.5600],
<a id="__codelineno-293-6" name="__codelineno-293-6" href="#__codelineno-293-6"></a>        [ 1.0842, -0.9580,  0.3623,  0.2343]])
<a id="__codelineno-293-7" name="__codelineno-293-7" href="#__codelineno-293-7"></a>&gt;&gt;&gt; torch.mean(a, 1)
<a id="__codelineno-293-8" name="__codelineno-293-8" href="#__codelineno-293-8"></a>tensor([-0.0163, -0.5085, -0.4599,  0.1807])
<a id="__codelineno-293-9" name="__codelineno-293-9" href="#__codelineno-293-9"></a>&gt;&gt;&gt; torch.mean(a, 1, True)
<a id="__codelineno-293-10" name="__codelineno-293-10" href="#__codelineno-293-10"></a>tensor([[-0.0163],
<a id="__codelineno-293-11" name="__codelineno-293-11" href="#__codelineno-293-11"></a>        [-0.5085],
<a id="__codelineno-293-12" name="__codelineno-293-12" href="#__codelineno-293-12"></a>        [-0.4599],
<a id="__codelineno-293-13" name="__codelineno-293-13" href="#__codelineno-293-13"></a>        [ 0.1807]])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-294-1" name="__codelineno-294-1" href="#__codelineno-294-1"></a>torch.median()¶
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-295-1" name="__codelineno-295-1" href="#__codelineno-295-1"></a>torch.median(input) → Tensor
</code></pre></div>
<p>返回<code>input</code>张量中所有元素的中值。</p>
<p>Parameters</p>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-296-1" name="__codelineno-296-1" href="#__codelineno-296-1"></a>&gt;&gt;&gt; a = torch.randn(1, 3)
<a id="__codelineno-296-2" name="__codelineno-296-2" href="#__codelineno-296-2"></a>&gt;&gt;&gt; a
<a id="__codelineno-296-3" name="__codelineno-296-3" href="#__codelineno-296-3"></a>tensor([[ 1.5219, -1.5212,  0.2202]])
<a id="__codelineno-296-4" name="__codelineno-296-4" href="#__codelineno-296-4"></a>&gt;&gt;&gt; torch.median(a)
<a id="__codelineno-296-5" name="__codelineno-296-5" href="#__codelineno-296-5"></a>tensor(0.2202)
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-297-1" name="__codelineno-297-1" href="#__codelineno-297-1"></a>torch.median(input, dim=-1, keepdim=False, values=None, indices=None) -&gt; (Tensor, LongTensor)
</code></pre></div>
<p>返回一个命名元组<code>(values, indices)</code>，其中<code>values</code>是在给定维度<code>dim</code>中<code>input</code>张量的每一行的中值。 <code>indices</code>是找到的每个中值的索引位置。</p>
<p>默认情况下，<code>dim</code>是<code>input</code>张量的最后一个尺寸。</p>
<p>如果<code>keepdim</code>为<code>True</code>，则输出张量的大小与<code>input</code>相同，只是尺寸为 1 的尺寸为<code>dim</code>。否则，将压缩<code>dim</code>(请参见 <a href="#torch.squeeze" title="torch.squeeze"><code>torch.squeeze()</code></a>)，导致输出张量的尺寸比<code>input</code>小 1。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>暗淡的</strong> (<em>python：int</em> )–缩小的尺寸。</p>
</li>
<li>
<p><strong>keepdim</strong> (<em>bool</em>) – whether the output tensor has <code>dim</code> retained or not.</p>
</li>
<li>
<p><strong>值</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a> <em>，</em> <em>可选</em>）–输出张量</p>
</li>
<li>
<p><strong>索引</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a> <em>，</em> <em>可选</em>）–输出索引张量</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-298-1" name="__codelineno-298-1" href="#__codelineno-298-1"></a>&gt;&gt;&gt; a = torch.randn(4, 5)
<a id="__codelineno-298-2" name="__codelineno-298-2" href="#__codelineno-298-2"></a>&gt;&gt;&gt; a
<a id="__codelineno-298-3" name="__codelineno-298-3" href="#__codelineno-298-3"></a>tensor([[ 0.2505, -0.3982, -0.9948,  0.3518, -1.3131],
<a id="__codelineno-298-4" name="__codelineno-298-4" href="#__codelineno-298-4"></a>        [ 0.3180, -0.6993,  1.0436,  0.0438,  0.2270],
<a id="__codelineno-298-5" name="__codelineno-298-5" href="#__codelineno-298-5"></a>        [-0.2751,  0.7303,  0.2192,  0.3321,  0.2488],
<a id="__codelineno-298-6" name="__codelineno-298-6" href="#__codelineno-298-6"></a>        [ 1.0778, -1.9510,  0.7048,  0.4742, -0.7125]])
<a id="__codelineno-298-7" name="__codelineno-298-7" href="#__codelineno-298-7"></a>&gt;&gt;&gt; torch.median(a, 1)
<a id="__codelineno-298-8" name="__codelineno-298-8" href="#__codelineno-298-8"></a>torch.return_types.median(values=tensor([-0.3982,  0.2270,  0.2488,  0.4742]), indices=tensor([1, 4, 4, 3]))
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-299-1" name="__codelineno-299-1" href="#__codelineno-299-1"></a>torch.mode(input, dim=-1, keepdim=False, values=None, indices=None) -&gt; (Tensor, LongTensor)¶
</code></pre></div>
<p>返回一个命名元组<code>(values, indices)</code>，其中<code>values</code>是给定维度<code>dim</code>中<code>input</code>张量的每一行的众数值，即该行中最常出现的值，而<code>indices</code>是索引位置 找到的每个模式值。</p>
<p>By default, <code>dim</code> is the last dimension of the <code>input</code> tensor.</p>
<p>如果<code>keepdim</code>为<code>True</code>，则输出张量的大小与<code>input</code>相同，只是尺寸为 1 的尺寸为<code>dim</code>。否则，将压缩<code>dim</code>(请参见 <a href="#torch.squeeze" title="torch.squeeze"><code>torch.squeeze()</code></a>)，导致输出张量的尺寸比<code>input</code>小 1。</p>
<p>Note</p>
<p>尚未为<code>torch.cuda.Tensor</code>定义此功能。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>dim</strong> (<em>python:int</em>) – the dimension to reduce.</p>
</li>
<li>
<p><strong>keepdim</strong> (<em>bool</em>) – whether the output tensor has <code>dim</code> retained or not.</p>
</li>
<li>
<p><strong>values</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor</p>
</li>
<li>
<p><strong>indices</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output index tensor</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-300-1" name="__codelineno-300-1" href="#__codelineno-300-1"></a>&gt;&gt;&gt; a = torch.randint(10, (5,))
<a id="__codelineno-300-2" name="__codelineno-300-2" href="#__codelineno-300-2"></a>&gt;&gt;&gt; a
<a id="__codelineno-300-3" name="__codelineno-300-3" href="#__codelineno-300-3"></a>tensor([6, 5, 1, 0, 2])
<a id="__codelineno-300-4" name="__codelineno-300-4" href="#__codelineno-300-4"></a>&gt;&gt;&gt; b = a + (torch.randn(50, 1) * 5).long()
<a id="__codelineno-300-5" name="__codelineno-300-5" href="#__codelineno-300-5"></a>&gt;&gt;&gt; torch.mode(b, 0)
<a id="__codelineno-300-6" name="__codelineno-300-6" href="#__codelineno-300-6"></a>torch.return_types.mode(values=tensor([6, 5, 1, 0, 2]), indices=tensor([2, 2, 2, 2, 2]))
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-301-1" name="__codelineno-301-1" href="#__codelineno-301-1"></a>torch.norm(input, p=&#39;fro&#39;, dim=None, keepdim=False, out=None, dtype=None)¶
</code></pre></div>
<p>返回给定张量的矩阵范数或向量范数。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>输入</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–输入张量</p>
</li>
<li>
<p><strong>p</strong>  (<em>python：int</em> <em>，</em> <em>python：float</em> <em>，</em> <em>inf</em> <em>，</em> <em>-inf</em> <em>，</em> <em>'来回</em> <em>，</em> <em>'nuc'</em> <em>，</em> <em>可选</em>）–</p>
<p>规范的顺序。 默认值：<code>'fro'</code>可以计算以下规范：</p>
<p>| </p>
<p>奥德</p>
<p>| </p>
<p>矩阵范数</p>
<p>| </p>
<p>向量范数</p>
<p>|
| --- | --- | --- |
| 没有 | Frobenius 范数 | 2 范数 |
| 来回 | Frobenius norm | – |
| 'nuc' | 核规范 | – |
| 其他 | 当 dim 为 None 时作为 vec 规范 | sum(abs(x）<strong> ord）</strong>(1./ord） |</p>
</li>
<li>
<p><strong>暗淡的</strong> (<em>python：int</em> <em>，</em> <em>2 个元组的 python：ints</em> <em>，</em> <em>2 个列表 python：ints</em> <em>，</em> <em>可选</em>）–如果为 int，则将计算向量范数，如果为 int 的 2 元组，则将计算矩阵范数。 如果值为 None，则在输入张量只有二维时将计算矩阵范数，而在输入张量只有一维时将计算向量范数。 如果输入张量具有两个以上的维，则矢量范数将应用于最后一个维。</p>
</li>
<li>
<p><strong>keepdim</strong>  (<em>bool</em> <em>，</em> <em>可选</em>）–输出张量是否保留<code>dim</code>。 如果<code>dim</code> = <code>None</code>和<code>out</code> = <code>None</code>则忽略。 默认值：<code>False</code></p>
</li>
<li>
<p><strong>输出</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a> <em>，</em> <em>可选</em>）–输出张量。 如果<code>dim</code> = <code>None</code>和<code>out</code> = <code>None</code>则忽略。</p>
</li>
<li>
<p><strong>dtype</strong>  (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a> ，可选）–返回张量的所需数据类型。 如果已指定，则在执行操作时将输入张量强制转换为：attr：“ dtype”。 默认值：无。</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-302-1" name="__codelineno-302-1" href="#__codelineno-302-1"></a>&gt;&gt;&gt; import torch
<a id="__codelineno-302-2" name="__codelineno-302-2" href="#__codelineno-302-2"></a>&gt;&gt;&gt; a = torch.arange(9, dtype= torch.float) - 4
<a id="__codelineno-302-3" name="__codelineno-302-3" href="#__codelineno-302-3"></a>&gt;&gt;&gt; b = a.reshape((3, 3))
<a id="__codelineno-302-4" name="__codelineno-302-4" href="#__codelineno-302-4"></a>&gt;&gt;&gt; torch.norm(a)
<a id="__codelineno-302-5" name="__codelineno-302-5" href="#__codelineno-302-5"></a>tensor(7.7460)
<a id="__codelineno-302-6" name="__codelineno-302-6" href="#__codelineno-302-6"></a>&gt;&gt;&gt; torch.norm(b)
<a id="__codelineno-302-7" name="__codelineno-302-7" href="#__codelineno-302-7"></a>tensor(7.7460)
<a id="__codelineno-302-8" name="__codelineno-302-8" href="#__codelineno-302-8"></a>&gt;&gt;&gt; torch.norm(a, float(&#39;inf&#39;))
<a id="__codelineno-302-9" name="__codelineno-302-9" href="#__codelineno-302-9"></a>tensor(4.)
<a id="__codelineno-302-10" name="__codelineno-302-10" href="#__codelineno-302-10"></a>&gt;&gt;&gt; torch.norm(b, float(&#39;inf&#39;))
<a id="__codelineno-302-11" name="__codelineno-302-11" href="#__codelineno-302-11"></a>tensor(4.)
<a id="__codelineno-302-12" name="__codelineno-302-12" href="#__codelineno-302-12"></a>&gt;&gt;&gt; c = torch.tensor([[ 1, 2, 3],[-1, 1, 4]] , dtype= torch.float)
<a id="__codelineno-302-13" name="__codelineno-302-13" href="#__codelineno-302-13"></a>&gt;&gt;&gt; torch.norm(c, dim=0)
<a id="__codelineno-302-14" name="__codelineno-302-14" href="#__codelineno-302-14"></a>tensor([1.4142, 2.2361, 5.0000])
<a id="__codelineno-302-15" name="__codelineno-302-15" href="#__codelineno-302-15"></a>&gt;&gt;&gt; torch.norm(c, dim=1)
<a id="__codelineno-302-16" name="__codelineno-302-16" href="#__codelineno-302-16"></a>tensor([3.7417, 4.2426])
<a id="__codelineno-302-17" name="__codelineno-302-17" href="#__codelineno-302-17"></a>&gt;&gt;&gt; torch.norm(c, p=1, dim=1)
<a id="__codelineno-302-18" name="__codelineno-302-18" href="#__codelineno-302-18"></a>tensor([6., 6.])
<a id="__codelineno-302-19" name="__codelineno-302-19" href="#__codelineno-302-19"></a>&gt;&gt;&gt; d = torch.arange(8, dtype= torch.float).reshape(2,2,2)
<a id="__codelineno-302-20" name="__codelineno-302-20" href="#__codelineno-302-20"></a>&gt;&gt;&gt; torch.norm(d, dim=(1,2))
<a id="__codelineno-302-21" name="__codelineno-302-21" href="#__codelineno-302-21"></a>tensor([ 3.7417, 11.2250])
<a id="__codelineno-302-22" name="__codelineno-302-22" href="#__codelineno-302-22"></a>&gt;&gt;&gt; torch.norm(d[0, :, :]), torch.norm(d[1, :, :])
<a id="__codelineno-302-23" name="__codelineno-302-23" href="#__codelineno-302-23"></a>(tensor(3.7417), tensor(11.2250))
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-303-1" name="__codelineno-303-1" href="#__codelineno-303-1"></a>torch.prod()¶
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-304-1" name="__codelineno-304-1" href="#__codelineno-304-1"></a>torch.prod(input, dtype=None) → Tensor
</code></pre></div>
<p>返回<code>input</code>张量中所有元素的乘积。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>dtype</strong>  (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a> ，可选）–返回张量的所需数据类型。 如果指定，则在执行操作之前将输入张量转换为<code>dtype</code>。 这对于防止数据类型溢出很有用。 默认值：无。</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-305-1" name="__codelineno-305-1" href="#__codelineno-305-1"></a>&gt;&gt;&gt; a = torch.randn(1, 3)
<a id="__codelineno-305-2" name="__codelineno-305-2" href="#__codelineno-305-2"></a>&gt;&gt;&gt; a
<a id="__codelineno-305-3" name="__codelineno-305-3" href="#__codelineno-305-3"></a>tensor([[-0.8020,  0.5428, -1.5854]])
<a id="__codelineno-305-4" name="__codelineno-305-4" href="#__codelineno-305-4"></a>&gt;&gt;&gt; torch.prod(a)
<a id="__codelineno-305-5" name="__codelineno-305-5" href="#__codelineno-305-5"></a>tensor(0.6902)
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-306-1" name="__codelineno-306-1" href="#__codelineno-306-1"></a>torch.prod(input, dim, keepdim=False, dtype=None) → Tensor
</code></pre></div>
<p>返回给定维度<code>dim</code>中<code>input</code>张量的每一行的乘积。</p>
<p>如果<code>keepdim</code>为<code>True</code>，则输出张量的大小与<code>input</code>相同，但尺寸为<code>dim</code>的大小为 1。否则，将压缩<code>dim</code>(请参见 <a href="#torch.squeeze" title="torch.squeeze"><code>torch.squeeze()</code></a>)，导致输出张量的尺寸比<code>input</code>小 1。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>dim</strong> (<em>python:int</em>) – the dimension to reduce.</p>
</li>
<li>
<p><strong>keepdim</strong> (<em>bool</em>) – whether the output tensor has <code>dim</code> retained or not.</p>
</li>
<li>
<p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) – the desired data type of returned tensor. If specified, the input tensor is casted to <code>dtype</code> before the operation is performed. This is useful for preventing data type overflows. Default: None.</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-307-1" name="__codelineno-307-1" href="#__codelineno-307-1"></a>&gt;&gt;&gt; a = torch.randn(4, 2)
<a id="__codelineno-307-2" name="__codelineno-307-2" href="#__codelineno-307-2"></a>&gt;&gt;&gt; a
<a id="__codelineno-307-3" name="__codelineno-307-3" href="#__codelineno-307-3"></a>tensor([[ 0.5261, -0.3837],
<a id="__codelineno-307-4" name="__codelineno-307-4" href="#__codelineno-307-4"></a>        [ 1.1857, -0.2498],
<a id="__codelineno-307-5" name="__codelineno-307-5" href="#__codelineno-307-5"></a>        [-1.1646,  0.0705],
<a id="__codelineno-307-6" name="__codelineno-307-6" href="#__codelineno-307-6"></a>        [ 1.1131, -1.0629]])
<a id="__codelineno-307-7" name="__codelineno-307-7" href="#__codelineno-307-7"></a>&gt;&gt;&gt; torch.prod(a, 1)
<a id="__codelineno-307-8" name="__codelineno-307-8" href="#__codelineno-307-8"></a>tensor([-0.2018, -0.2962, -0.0821, -1.1831])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-308-1" name="__codelineno-308-1" href="#__codelineno-308-1"></a>torch.std()¶
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-309-1" name="__codelineno-309-1" href="#__codelineno-309-1"></a>torch.std(input, unbiased=True) → Tensor
</code></pre></div>
<p>返回<code>input</code>张量中所有元素的标准偏差。</p>
<p>如果<code>unbiased</code>为<code>False</code>，则将通过有偏估计量计算标准偏差。 否则，将使用贝塞尔的更正。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>无偏</strong> (<em>bool</em> )–是否使用无偏估计</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-310-1" name="__codelineno-310-1" href="#__codelineno-310-1"></a>&gt;&gt;&gt; a = torch.randn(1, 3)
<a id="__codelineno-310-2" name="__codelineno-310-2" href="#__codelineno-310-2"></a>&gt;&gt;&gt; a
<a id="__codelineno-310-3" name="__codelineno-310-3" href="#__codelineno-310-3"></a>tensor([[-0.8166, -1.3802, -0.3560]])
<a id="__codelineno-310-4" name="__codelineno-310-4" href="#__codelineno-310-4"></a>&gt;&gt;&gt; torch.std(a)
<a id="__codelineno-310-5" name="__codelineno-310-5" href="#__codelineno-310-5"></a>tensor(0.5130)
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-311-1" name="__codelineno-311-1" href="#__codelineno-311-1"></a>torch.std(input, dim, keepdim=False, unbiased=True, out=None) → Tensor
</code></pre></div>
<p>返回<code>input</code>张量的每一行在标准<code>dim</code>中的标准偏差。 如果<code>dim</code>是尺寸列表，请缩小所有尺寸。</p>
<p>If <code>keepdim</code> is <code>True</code>, the output tensor is of the same size as <code>input</code> except in the dimension(s) <code>dim</code> where it is of size 1. Otherwise, <code>dim</code> is squeezed (see <a href="#torch.squeeze" title="torch.squeeze"><code>torch.squeeze()</code></a>), resulting in the output tensor having 1 (or <code>len(dim)</code>) fewer dimension(s).</p>
<p>If <code>unbiased</code> is <code>False</code>, then the standard-deviation will be calculated via the biased estimator. Otherwise, Bessel's correction will be used.</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>dim</strong> (<em>python:int</em> <em>or</em> <em>tuple of python:ints</em>) – the dimension or dimensions to reduce.</p>
</li>
<li>
<p><strong>keepdim</strong> (<em>bool</em>) – whether the output tensor has <code>dim</code> retained or not.</p>
</li>
<li>
<p><strong>unbiased</strong> (<em>bool</em>) – whether to use the unbiased estimation or not</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-312-1" name="__codelineno-312-1" href="#__codelineno-312-1"></a>&gt;&gt;&gt; a = torch.randn(4, 4)
<a id="__codelineno-312-2" name="__codelineno-312-2" href="#__codelineno-312-2"></a>&gt;&gt;&gt; a
<a id="__codelineno-312-3" name="__codelineno-312-3" href="#__codelineno-312-3"></a>tensor([[ 0.2035,  1.2959,  1.8101, -0.4644],
<a id="__codelineno-312-4" name="__codelineno-312-4" href="#__codelineno-312-4"></a>        [ 1.5027, -0.3270,  0.5905,  0.6538],
<a id="__codelineno-312-5" name="__codelineno-312-5" href="#__codelineno-312-5"></a>        [-1.5745,  1.3330, -0.5596, -0.6548],
<a id="__codelineno-312-6" name="__codelineno-312-6" href="#__codelineno-312-6"></a>        [ 0.1264, -0.5080,  1.6420,  0.1992]])
<a id="__codelineno-312-7" name="__codelineno-312-7" href="#__codelineno-312-7"></a>&gt;&gt;&gt; torch.std(a, dim=1)
<a id="__codelineno-312-8" name="__codelineno-312-8" href="#__codelineno-312-8"></a>tensor([ 1.0311,  0.7477,  1.2204,  0.9087])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-313-1" name="__codelineno-313-1" href="#__codelineno-313-1"></a>torch.std_mean()¶
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-314-1" name="__codelineno-314-1" href="#__codelineno-314-1"></a>torch.std_mean(input, unbiased=True) -&gt; (Tensor, Tensor)
</code></pre></div>
<p>返回<code>input</code>张量中所有元素的标准差和均值。</p>
<p>If <code>unbiased</code> is <code>False</code>, then the standard-deviation will be calculated via the biased estimator. Otherwise, Bessel's correction will be used.</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>unbiased</strong> (<em>bool</em>) – whether to use the unbiased estimation or not</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-315-1" name="__codelineno-315-1" href="#__codelineno-315-1"></a>&gt;&gt;&gt; a = torch.randn(1, 3)
<a id="__codelineno-315-2" name="__codelineno-315-2" href="#__codelineno-315-2"></a>&gt;&gt;&gt; a
<a id="__codelineno-315-3" name="__codelineno-315-3" href="#__codelineno-315-3"></a>tensor([[0.3364, 0.3591, 0.9462]])
<a id="__codelineno-315-4" name="__codelineno-315-4" href="#__codelineno-315-4"></a>&gt;&gt;&gt; torch.std_mean(a)
<a id="__codelineno-315-5" name="__codelineno-315-5" href="#__codelineno-315-5"></a>(tensor(0.3457), tensor(0.5472))
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-316-1" name="__codelineno-316-1" href="#__codelineno-316-1"></a>torch.std(input, dim, keepdim=False, unbiased=True) -&gt; (Tensor, Tensor)
</code></pre></div>
<p>返回<code>dim</code>张量中<code>input</code>张量的每一行的标准偏差和均值。 如果<code>dim</code>是尺寸列表，请缩小所有尺寸。</p>
<p>If <code>keepdim</code> is <code>True</code>, the output tensor is of the same size as <code>input</code> except in the dimension(s) <code>dim</code> where it is of size 1. Otherwise, <code>dim</code> is squeezed (see <a href="#torch.squeeze" title="torch.squeeze"><code>torch.squeeze()</code></a>), resulting in the output tensor having 1 (or <code>len(dim)</code>) fewer dimension(s).</p>
<p>If <code>unbiased</code> is <code>False</code>, then the standard-deviation will be calculated via the biased estimator. Otherwise, Bessel's correction will be used.</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>dim</strong> (<em>python:int</em> <em>or</em> <em>tuple of python:ints</em>) – the dimension or dimensions to reduce.</p>
</li>
<li>
<p><strong>keepdim</strong> (<em>bool</em>) – whether the output tensor has <code>dim</code> retained or not.</p>
</li>
<li>
<p><strong>unbiased</strong> (<em>bool</em>) – whether to use the unbiased estimation or not</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-317-1" name="__codelineno-317-1" href="#__codelineno-317-1"></a>&gt;&gt;&gt; a = torch.randn(4, 4)
<a id="__codelineno-317-2" name="__codelineno-317-2" href="#__codelineno-317-2"></a>&gt;&gt;&gt; a
<a id="__codelineno-317-3" name="__codelineno-317-3" href="#__codelineno-317-3"></a>tensor([[ 0.5648, -0.5984, -1.2676, -1.4471],
<a id="__codelineno-317-4" name="__codelineno-317-4" href="#__codelineno-317-4"></a>        [ 0.9267,  1.0612,  1.1050, -0.6014],
<a id="__codelineno-317-5" name="__codelineno-317-5" href="#__codelineno-317-5"></a>        [ 0.0154,  1.9301,  0.0125, -1.0904],
<a id="__codelineno-317-6" name="__codelineno-317-6" href="#__codelineno-317-6"></a>        [-1.9711, -0.7748, -1.3840,  0.5067]])
<a id="__codelineno-317-7" name="__codelineno-317-7" href="#__codelineno-317-7"></a>&gt;&gt;&gt; torch.std_mean(a, 1)
<a id="__codelineno-317-8" name="__codelineno-317-8" href="#__codelineno-317-8"></a>(tensor([0.9110, 0.8197, 1.2552, 1.0608]), tensor([-0.6871,  0.6229,  0.2169, -0.9058]))
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-318-1" name="__codelineno-318-1" href="#__codelineno-318-1"></a>torch.sum()¶
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-319-1" name="__codelineno-319-1" href="#__codelineno-319-1"></a>torch.sum(input, dtype=None) → Tensor
</code></pre></div>
<p>返回<code>input</code>张量中所有元素的总和。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) – the desired data type of returned tensor. If specified, the input tensor is casted to <code>dtype</code> before the operation is performed. This is useful for preventing data type overflows. Default: None.</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-320-1" name="__codelineno-320-1" href="#__codelineno-320-1"></a>&gt;&gt;&gt; a = torch.randn(1, 3)
<a id="__codelineno-320-2" name="__codelineno-320-2" href="#__codelineno-320-2"></a>&gt;&gt;&gt; a
<a id="__codelineno-320-3" name="__codelineno-320-3" href="#__codelineno-320-3"></a>tensor([[ 0.1133, -0.9567,  0.2958]])
<a id="__codelineno-320-4" name="__codelineno-320-4" href="#__codelineno-320-4"></a>&gt;&gt;&gt; torch.sum(a)
<a id="__codelineno-320-5" name="__codelineno-320-5" href="#__codelineno-320-5"></a>tensor(-0.5475)
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-321-1" name="__codelineno-321-1" href="#__codelineno-321-1"></a>torch.sum(input, dim, keepdim=False, dtype=None) → Tensor
</code></pre></div>
<p>返回给定维度<code>dim</code>中<code>input</code>张量的每一行的总和。 如果<code>dim</code>是尺寸列表，请缩小所有尺寸。</p>
<p>If <code>keepdim</code> is <code>True</code>, the output tensor is of the same size as <code>input</code> except in the dimension(s) <code>dim</code> where it is of size 1. Otherwise, <code>dim</code> is squeezed (see <a href="#torch.squeeze" title="torch.squeeze"><code>torch.squeeze()</code></a>), resulting in the output tensor having 1 (or <code>len(dim)</code>) fewer dimension(s).</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>dim</strong> (<em>python:int</em> <em>or</em> <em>tuple of python:ints</em>) – the dimension or dimensions to reduce.</p>
</li>
<li>
<p><strong>keepdim</strong> (<em>bool</em>) – whether the output tensor has <code>dim</code> retained or not.</p>
</li>
<li>
<p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) – the desired data type of returned tensor. If specified, the input tensor is casted to <code>dtype</code> before the operation is performed. This is useful for preventing data type overflows. Default: None.</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-322-1" name="__codelineno-322-1" href="#__codelineno-322-1"></a>&gt;&gt;&gt; a = torch.randn(4, 4)
<a id="__codelineno-322-2" name="__codelineno-322-2" href="#__codelineno-322-2"></a>&gt;&gt;&gt; a
<a id="__codelineno-322-3" name="__codelineno-322-3" href="#__codelineno-322-3"></a>tensor([[ 0.0569, -0.2475,  0.0737, -0.3429],
<a id="__codelineno-322-4" name="__codelineno-322-4" href="#__codelineno-322-4"></a>        [-0.2993,  0.9138,  0.9337, -1.6864],
<a id="__codelineno-322-5" name="__codelineno-322-5" href="#__codelineno-322-5"></a>        [ 0.1132,  0.7892, -0.1003,  0.5688],
<a id="__codelineno-322-6" name="__codelineno-322-6" href="#__codelineno-322-6"></a>        [ 0.3637, -0.9906, -0.4752, -1.5197]])
<a id="__codelineno-322-7" name="__codelineno-322-7" href="#__codelineno-322-7"></a>&gt;&gt;&gt; torch.sum(a, 1)
<a id="__codelineno-322-8" name="__codelineno-322-8" href="#__codelineno-322-8"></a>tensor([-0.4598, -0.1381,  1.3708, -2.6217])
<a id="__codelineno-322-9" name="__codelineno-322-9" href="#__codelineno-322-9"></a>&gt;&gt;&gt; b = torch.arange(4 * 5 * 6).view(4, 5, 6)
<a id="__codelineno-322-10" name="__codelineno-322-10" href="#__codelineno-322-10"></a>&gt;&gt;&gt; torch.sum(b, (2, 1))
<a id="__codelineno-322-11" name="__codelineno-322-11" href="#__codelineno-322-11"></a>tensor([  435.,  1335.,  2235.,  3135.])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-323-1" name="__codelineno-323-1" href="#__codelineno-323-1"></a>torch.unique(input, sorted=True, return_inverse=False, return_counts=False, dim=None)¶
</code></pre></div>
<p>返回输入张量的唯一元素。</p>
<p>Note</p>
<p>此功能与 <a href="#torch.unique_consecutive" title="torch.unique_consecutive"><code>torch.unique_consecutive()</code></a> 不同，因为该功能还消除了非连续的重复值。</p>
<p>Note</p>
<p>当前，在 CUDA 实现和 CPU 实现中，当指定 dim 时，无论 &lt;cite&gt;sort&lt;/cite&gt; 参数如何， &lt;cite&gt;torch.unique&lt;/cite&gt; 始终在开始时对张量进行排序。 排序可能会很慢，因此如果您的输入张量已被排序，建议使用 <a href="#torch.unique_consecutive" title="torch.unique_consecutive"><code>torch.unique_consecutive()</code></a> 以避免排序。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor</p>
</li>
<li>
<p><strong>排序的</strong> (<em>bool</em> )–在返回为输出之前是否按升序对唯一元素进行排序。</p>
</li>
<li>
<p><strong>return_inverse</strong>  (<em>bool</em> )–是否还返回原始输入中元素在返回的唯一列表中所处位置的索引。</p>
</li>
<li>
<p><strong>return_counts</strong>  (<em>bool</em> )–是否还返回每个唯一元素的计数。</p>
</li>
<li>
<p><strong>暗淡的</strong> (<em>python：int</em> )–应用唯一尺寸。 如果<code>None</code>，则返回拼合输入的唯一性。 默认值：<code>None</code></p>
</li>
</ul>
<p>Returns</p>
<p>一个张量或张量的元组包含</p>
<blockquote>
<ul>
<li>
<p><strong>输出</strong>(<em>tensor</em>）：唯一标量元素的输出列表。</p>
</li>
<li>
<p><strong>inverse_indices</strong> (<em>tensor</em>）：(可选）如果<code>return_inverse</code>为 True，将有一个额外的返回张量(形状与输入相同）表示原始输入中元素所在位置的索引 映射到输出中； 否则，此函数将仅返回单个张量。</p>
</li>
<li>
<p><strong>计数为</strong>(<em>tensor</em>）：(可选）如果<code>return_counts</code>为 True，则将有一个额外的返回张量(与 output 或 output.size(dim）相同的形状，如果 dim 为 代表每个唯一值或张量的出现次数。</p>
</li>
</ul>
</blockquote>
<p>Return type</p>
<p>(<a href="tensors.html#torch.Tensor" title="torch.Tensor">张量</a>，<a href="tensors.html#torch.Tensor" title="torch.Tensor">张量</a>(可选），<a href="tensors.html#torch.Tensor" title="torch.Tensor">张量</a>(可选））</p>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-324-1" name="__codelineno-324-1" href="#__codelineno-324-1"></a>&gt;&gt;&gt; output = torch.unique(torch.tensor([1, 3, 2, 3], dtype=torch.long))
<a id="__codelineno-324-2" name="__codelineno-324-2" href="#__codelineno-324-2"></a>&gt;&gt;&gt; output
<a id="__codelineno-324-3" name="__codelineno-324-3" href="#__codelineno-324-3"></a>tensor([ 2,  3,  1])
<a id="__codelineno-324-4" name="__codelineno-324-4" href="#__codelineno-324-4"></a>
<a id="__codelineno-324-5" name="__codelineno-324-5" href="#__codelineno-324-5"></a>&gt;&gt;&gt; output, inverse_indices = torch.unique(
<a id="__codelineno-324-6" name="__codelineno-324-6" href="#__codelineno-324-6"></a>        torch.tensor([1, 3, 2, 3], dtype=torch.long), sorted=True, return_inverse=True)
<a id="__codelineno-324-7" name="__codelineno-324-7" href="#__codelineno-324-7"></a>&gt;&gt;&gt; output
<a id="__codelineno-324-8" name="__codelineno-324-8" href="#__codelineno-324-8"></a>tensor([ 1,  2,  3])
<a id="__codelineno-324-9" name="__codelineno-324-9" href="#__codelineno-324-9"></a>&gt;&gt;&gt; inverse_indices
<a id="__codelineno-324-10" name="__codelineno-324-10" href="#__codelineno-324-10"></a>tensor([ 0,  2,  1,  2])
<a id="__codelineno-324-11" name="__codelineno-324-11" href="#__codelineno-324-11"></a>
<a id="__codelineno-324-12" name="__codelineno-324-12" href="#__codelineno-324-12"></a>&gt;&gt;&gt; output, inverse_indices = torch.unique(
<a id="__codelineno-324-13" name="__codelineno-324-13" href="#__codelineno-324-13"></a>        torch.tensor([[1, 3], [2, 3]], dtype=torch.long), sorted=True, return_inverse=True)
<a id="__codelineno-324-14" name="__codelineno-324-14" href="#__codelineno-324-14"></a>&gt;&gt;&gt; output
<a id="__codelineno-324-15" name="__codelineno-324-15" href="#__codelineno-324-15"></a>tensor([ 1,  2,  3])
<a id="__codelineno-324-16" name="__codelineno-324-16" href="#__codelineno-324-16"></a>&gt;&gt;&gt; inverse_indices
<a id="__codelineno-324-17" name="__codelineno-324-17" href="#__codelineno-324-17"></a>tensor([[ 0,  2],
<a id="__codelineno-324-18" name="__codelineno-324-18" href="#__codelineno-324-18"></a>        [ 1,  2]])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-325-1" name="__codelineno-325-1" href="#__codelineno-325-1"></a>torch.unique_consecutive(input, return_inverse=False, return_counts=False, dim=None)¶
</code></pre></div>
<p>从每个连续的等效元素组中除去除第一个元素外的所有元素。</p>
<p>Note</p>
<p>在此功能仅消除连续重复值的意义上，此功能与 <a href="#torch.unique" title="torch.unique"><code>torch.unique()</code></a> 不同。 此语义类似于 C ++中的 &lt;cite&gt;std :: unique&lt;/cite&gt; 。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor</p>
</li>
<li>
<p><strong>return_inverse</strong> (<em>bool</em>) – Whether to also return the indices for where elements in the original input ended up in the returned unique list.</p>
</li>
<li>
<p><strong>return_counts</strong> (<em>bool</em>) – Whether to also return the counts for each unique element.</p>
</li>
<li>
<p><strong>dim</strong> (<em>python:int</em>) – the dimension to apply unique. If <code>None</code>, the unique of the flattened input is returned. default: <code>None</code></p>
</li>
</ul>
<p>Returns</p>
<p>A tensor or a tuple of tensors containing</p>
<blockquote>
<ul>
<li>
<p><strong>output</strong> (<em>Tensor</em>): the output list of unique scalar elements.</p>
</li>
<li>
<p><strong>inverse_indices</strong> (<em>Tensor</em>): (optional) if <code>return_inverse</code> is True, there will be an additional returned tensor (same shape as input) representing the indices for where elements in the original input map to in the output; otherwise, this function will only return a single tensor.</p>
</li>
<li>
<p><strong>counts</strong> (<em>Tensor</em>): (optional) if <code>return_counts</code> is True, there will be an additional returned tensor (same shape as output or output.size(dim), if dim was specified) representing the number of occurrences for each unique value or tensor.</p>
</li>
</ul>
</blockquote>
<p>Return type</p>
<p>(<a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a>, <a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a> (optional), <a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a> (optional))</p>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-326-1" name="__codelineno-326-1" href="#__codelineno-326-1"></a>&gt;&gt;&gt; x = torch.tensor([1, 1, 2, 2, 3, 1, 1, 2])
<a id="__codelineno-326-2" name="__codelineno-326-2" href="#__codelineno-326-2"></a>&gt;&gt;&gt; output = torch.unique_consecutive(x)
<a id="__codelineno-326-3" name="__codelineno-326-3" href="#__codelineno-326-3"></a>&gt;&gt;&gt; output
<a id="__codelineno-326-4" name="__codelineno-326-4" href="#__codelineno-326-4"></a>tensor([1, 2, 3, 1, 2])
<a id="__codelineno-326-5" name="__codelineno-326-5" href="#__codelineno-326-5"></a>
<a id="__codelineno-326-6" name="__codelineno-326-6" href="#__codelineno-326-6"></a>&gt;&gt;&gt; output, inverse_indices = torch.unique_consecutive(x, return_inverse=True)
<a id="__codelineno-326-7" name="__codelineno-326-7" href="#__codelineno-326-7"></a>&gt;&gt;&gt; output
<a id="__codelineno-326-8" name="__codelineno-326-8" href="#__codelineno-326-8"></a>tensor([1, 2, 3, 1, 2])
<a id="__codelineno-326-9" name="__codelineno-326-9" href="#__codelineno-326-9"></a>&gt;&gt;&gt; inverse_indices
<a id="__codelineno-326-10" name="__codelineno-326-10" href="#__codelineno-326-10"></a>tensor([0, 0, 1, 1, 2, 3, 3, 4])
<a id="__codelineno-326-11" name="__codelineno-326-11" href="#__codelineno-326-11"></a>
<a id="__codelineno-326-12" name="__codelineno-326-12" href="#__codelineno-326-12"></a>&gt;&gt;&gt; output, counts = torch.unique_consecutive(x, return_counts=True)
<a id="__codelineno-326-13" name="__codelineno-326-13" href="#__codelineno-326-13"></a>&gt;&gt;&gt; output
<a id="__codelineno-326-14" name="__codelineno-326-14" href="#__codelineno-326-14"></a>tensor([1, 2, 3, 1, 2])
<a id="__codelineno-326-15" name="__codelineno-326-15" href="#__codelineno-326-15"></a>&gt;&gt;&gt; counts
<a id="__codelineno-326-16" name="__codelineno-326-16" href="#__codelineno-326-16"></a>tensor([2, 2, 1, 2, 1])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-327-1" name="__codelineno-327-1" href="#__codelineno-327-1"></a>torch.var()¶
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-328-1" name="__codelineno-328-1" href="#__codelineno-328-1"></a>torch.var(input, unbiased=True) → Tensor
</code></pre></div>
<p>返回<code>input</code>张量中所有元素的方差。</p>
<p>如果<code>unbiased</code>为<code>False</code>，则将通过有偏估计量计算方差。 否则，将使用贝塞尔的更正。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>unbiased</strong> (<em>bool</em>) – whether to use the unbiased estimation or not</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-329-1" name="__codelineno-329-1" href="#__codelineno-329-1"></a>&gt;&gt;&gt; a = torch.randn(1, 3)
<a id="__codelineno-329-2" name="__codelineno-329-2" href="#__codelineno-329-2"></a>&gt;&gt;&gt; a
<a id="__codelineno-329-3" name="__codelineno-329-3" href="#__codelineno-329-3"></a>tensor([[-0.3425, -1.2636, -0.4864]])
<a id="__codelineno-329-4" name="__codelineno-329-4" href="#__codelineno-329-4"></a>&gt;&gt;&gt; torch.var(a)
<a id="__codelineno-329-5" name="__codelineno-329-5" href="#__codelineno-329-5"></a>tensor(0.2455)
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-330-1" name="__codelineno-330-1" href="#__codelineno-330-1"></a>torch.var(input, dim, keepdim=False, unbiased=True, out=None) → Tensor
</code></pre></div>
<p>返回给定维度<code>dim</code>中<code>input</code>张量的每一行的方差。</p>
<p>If <code>keepdim</code> is <code>True</code>, the output tensor is of the same size as <code>input</code> except in the dimension(s) <code>dim</code> where it is of size 1. Otherwise, <code>dim</code> is squeezed (see <a href="#torch.squeeze" title="torch.squeeze"><code>torch.squeeze()</code></a>), resulting in the output tensor having 1 (or <code>len(dim)</code>) fewer dimension(s).</p>
<p>If <code>unbiased</code> is <code>False</code>, then the variance will be calculated via the biased estimator. Otherwise, Bessel's correction will be used.</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>dim</strong> (<em>python:int</em> <em>or</em> <em>tuple of python:ints</em>) – the dimension or dimensions to reduce.</p>
</li>
<li>
<p><strong>keepdim</strong> (<em>bool</em>) – whether the output tensor has <code>dim</code> retained or not.</p>
</li>
<li>
<p><strong>unbiased</strong> (<em>bool</em>) – whether to use the unbiased estimation or not</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-331-1" name="__codelineno-331-1" href="#__codelineno-331-1"></a>&gt;&gt;&gt; a = torch.randn(4, 4)
<a id="__codelineno-331-2" name="__codelineno-331-2" href="#__codelineno-331-2"></a>&gt;&gt;&gt; a
<a id="__codelineno-331-3" name="__codelineno-331-3" href="#__codelineno-331-3"></a>tensor([[-0.3567,  1.7385, -1.3042,  0.7423],
<a id="__codelineno-331-4" name="__codelineno-331-4" href="#__codelineno-331-4"></a>        [ 1.3436, -0.1015, -0.9834, -0.8438],
<a id="__codelineno-331-5" name="__codelineno-331-5" href="#__codelineno-331-5"></a>        [ 0.6056,  0.1089, -0.3112, -1.4085],
<a id="__codelineno-331-6" name="__codelineno-331-6" href="#__codelineno-331-6"></a>        [-0.7700,  0.6074, -0.1469,  0.7777]])
<a id="__codelineno-331-7" name="__codelineno-331-7" href="#__codelineno-331-7"></a>&gt;&gt;&gt; torch.var(a, 1)
<a id="__codelineno-331-8" name="__codelineno-331-8" href="#__codelineno-331-8"></a>tensor([ 1.7444,  1.1363,  0.7356,  0.5112])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-332-1" name="__codelineno-332-1" href="#__codelineno-332-1"></a>torch.var_mean()¶
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-333-1" name="__codelineno-333-1" href="#__codelineno-333-1"></a>torch.var_mean(input, unbiased=True) -&gt; (Tensor, Tensor)
</code></pre></div>
<p>返回<code>input</code>张量中所有元素的方差和均值。</p>
<p>If <code>unbiased</code> is <code>False</code>, then the variance will be calculated via the biased estimator. Otherwise, Bessel's correction will be used.</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>unbiased</strong> (<em>bool</em>) – whether to use the unbiased estimation or not</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-334-1" name="__codelineno-334-1" href="#__codelineno-334-1"></a>&gt;&gt;&gt; a = torch.randn(1, 3)
<a id="__codelineno-334-2" name="__codelineno-334-2" href="#__codelineno-334-2"></a>&gt;&gt;&gt; a
<a id="__codelineno-334-3" name="__codelineno-334-3" href="#__codelineno-334-3"></a>tensor([[0.0146, 0.4258, 0.2211]])
<a id="__codelineno-334-4" name="__codelineno-334-4" href="#__codelineno-334-4"></a>&gt;&gt;&gt; torch.var_mean(a)
<a id="__codelineno-334-5" name="__codelineno-334-5" href="#__codelineno-334-5"></a>(tensor(0.0423), tensor(0.2205))
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-335-1" name="__codelineno-335-1" href="#__codelineno-335-1"></a>torch.var_mean(input, dim, keepdim=False, unbiased=True) -&gt; (Tensor, Tensor)
</code></pre></div>
<p>返回给定维度<code>dim</code>中<code>input</code>张量的每一行的方差和均值。</p>
<p>If <code>keepdim</code> is <code>True</code>, the output tensor is of the same size as <code>input</code> except in the dimension(s) <code>dim</code> where it is of size 1. Otherwise, <code>dim</code> is squeezed (see <a href="#torch.squeeze" title="torch.squeeze"><code>torch.squeeze()</code></a>), resulting in the output tensor having 1 (or <code>len(dim)</code>) fewer dimension(s).</p>
<p>If <code>unbiased</code> is <code>False</code>, then the variance will be calculated via the biased estimator. Otherwise, Bessel's correction will be used.</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>dim</strong> (<em>python:int</em> <em>or</em> <em>tuple of python:ints</em>) – the dimension or dimensions to reduce.</p>
</li>
<li>
<p><strong>keepdim</strong> (<em>bool</em>) – whether the output tensor has <code>dim</code> retained or not.</p>
</li>
<li>
<p><strong>unbiased</strong> (<em>bool</em>) – whether to use the unbiased estimation or not</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-336-1" name="__codelineno-336-1" href="#__codelineno-336-1"></a>&gt;&gt;&gt; a = torch.randn(4, 4)
<a id="__codelineno-336-2" name="__codelineno-336-2" href="#__codelineno-336-2"></a>&gt;&gt;&gt; a
<a id="__codelineno-336-3" name="__codelineno-336-3" href="#__codelineno-336-3"></a>tensor([[-1.5650,  2.0415, -0.1024, -0.5790],
<a id="__codelineno-336-4" name="__codelineno-336-4" href="#__codelineno-336-4"></a>        [ 0.2325, -2.6145, -1.6428, -0.3537],
<a id="__codelineno-336-5" name="__codelineno-336-5" href="#__codelineno-336-5"></a>        [-0.2159, -1.1069,  1.2882, -1.3265],
<a id="__codelineno-336-6" name="__codelineno-336-6" href="#__codelineno-336-6"></a>        [-0.6706, -1.5893,  0.6827,  1.6727]])
<a id="__codelineno-336-7" name="__codelineno-336-7" href="#__codelineno-336-7"></a>&gt;&gt;&gt; torch.var_mean(a, 1)
<a id="__codelineno-336-8" name="__codelineno-336-8" href="#__codelineno-336-8"></a>(tensor([2.3174, 1.6403, 1.4092, 2.0791]), tensor([-0.0512, -1.0946, -0.3403,  0.0239]))
</code></pre></div>
<h3 id="_14">比较行动</h3>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-337-1" name="__codelineno-337-1" href="#__codelineno-337-1"></a>torch.allclose(input, other, rtol=1e-05, atol=1e-08, equal_nan=False) → bool¶
</code></pre></div>
<p>此函数检查<code>input</code>和<code>other</code>是否都满足以下条件：</p>
<p><img alt="" src="../img/cbd6149aa59b9b03eadc2c023182fe68.jpg" /></p>
<p>对于<code>input</code>和<code>other</code>的所有元素，都是逐元素的。 此函数的行为类似于 <a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.allclose.html">numpy.allclose</a></p>
<p>Parameters</p>
<ul>
<li>
<p><strong>输入</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–比较的第一个张量</p>
</li>
<li>
<p><strong>其他</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–要比较的第二张量</p>
</li>
<li>
<p><strong>atol</strong>  (<em>python：float</em> <em>，</em> <em>可选</em>）–绝对公差。 默认值：1e-08</p>
</li>
<li>
<p><strong>rtol</strong>  (<em>python：float</em> <em>，</em> <em>可选</em>）–相对公差。 默认值：1e-05</p>
</li>
<li>
<p><strong>equal_nan</strong>  (<em>bool</em> <em>，</em> <em>可选</em>）–如果<code>True</code>，则将两个<code>NaN</code> s 相等。 默认值：<code>False</code></p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-338-1" name="__codelineno-338-1" href="#__codelineno-338-1"></a>&gt;&gt;&gt; torch.allclose(torch.tensor([10000., 1e-07]), torch.tensor([10000.1, 1e-08]))
<a id="__codelineno-338-2" name="__codelineno-338-2" href="#__codelineno-338-2"></a>False
<a id="__codelineno-338-3" name="__codelineno-338-3" href="#__codelineno-338-3"></a>&gt;&gt;&gt; torch.allclose(torch.tensor([10000., 1e-08]), torch.tensor([10000.1, 1e-09]))
<a id="__codelineno-338-4" name="__codelineno-338-4" href="#__codelineno-338-4"></a>True
<a id="__codelineno-338-5" name="__codelineno-338-5" href="#__codelineno-338-5"></a>&gt;&gt;&gt; torch.allclose(torch.tensor([1.0, float(&#39;nan&#39;)]), torch.tensor([1.0, float(&#39;nan&#39;)]))
<a id="__codelineno-338-6" name="__codelineno-338-6" href="#__codelineno-338-6"></a>False
<a id="__codelineno-338-7" name="__codelineno-338-7" href="#__codelineno-338-7"></a>&gt;&gt;&gt; torch.allclose(torch.tensor([1.0, float(&#39;nan&#39;)]), torch.tensor([1.0, float(&#39;nan&#39;)]), equal_nan=True)
<a id="__codelineno-338-8" name="__codelineno-338-8" href="#__codelineno-338-8"></a>True
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-339-1" name="__codelineno-339-1" href="#__codelineno-339-1"></a>torch.argsort(input, dim=-1, descending=False, out=None) → LongTensor¶
</code></pre></div>
<p>返回按值升序对给定维度上的张量排序的索引。</p>
<p>这是 <a href="#torch.sort" title="torch.sort"><code>torch.sort()</code></a> 返回的第二个值。 有关此方法的确切语义，请参见其文档。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>暗淡的</strong> (<em>python：int</em> <em>，</em> <em>可选</em>）–要排序的维度</p>
</li>
<li>
<p><strong>降序</strong> (<em>bool</em> <em>，</em> <em>可选</em>）–控制排序顺序(升序或降序）</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-340-1" name="__codelineno-340-1" href="#__codelineno-340-1"></a>&gt;&gt;&gt; a = torch.randn(4, 4)
<a id="__codelineno-340-2" name="__codelineno-340-2" href="#__codelineno-340-2"></a>&gt;&gt;&gt; a
<a id="__codelineno-340-3" name="__codelineno-340-3" href="#__codelineno-340-3"></a>tensor([[ 0.0785,  1.5267, -0.8521,  0.4065],
<a id="__codelineno-340-4" name="__codelineno-340-4" href="#__codelineno-340-4"></a>        [ 0.1598,  0.0788, -0.0745, -1.2700],
<a id="__codelineno-340-5" name="__codelineno-340-5" href="#__codelineno-340-5"></a>        [ 1.2208,  1.0722, -0.7064,  1.2564],
<a id="__codelineno-340-6" name="__codelineno-340-6" href="#__codelineno-340-6"></a>        [ 0.0669, -0.2318, -0.8229, -0.9280]])
<a id="__codelineno-340-7" name="__codelineno-340-7" href="#__codelineno-340-7"></a>
<a id="__codelineno-340-8" name="__codelineno-340-8" href="#__codelineno-340-8"></a>&gt;&gt;&gt; torch.argsort(a, dim=1)
<a id="__codelineno-340-9" name="__codelineno-340-9" href="#__codelineno-340-9"></a>tensor([[2, 0, 3, 1],
<a id="__codelineno-340-10" name="__codelineno-340-10" href="#__codelineno-340-10"></a>        [3, 2, 1, 0],
<a id="__codelineno-340-11" name="__codelineno-340-11" href="#__codelineno-340-11"></a>        [2, 1, 0, 3],
<a id="__codelineno-340-12" name="__codelineno-340-12" href="#__codelineno-340-12"></a>        [3, 2, 1, 0]])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-341-1" name="__codelineno-341-1" href="#__codelineno-341-1"></a>torch.eq(input, other, out=None) → Tensor¶
</code></pre></div>
<p>计算按元素相等</p>
<p>第二个参数可以是数字或张量，其形状可以与第一个参数一起广播为的<a href="notes/broadcasting.html#broadcasting-semantics">。</a></p>
<p>Parameters</p>
<ul>
<li>
<p><strong>输入</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–要比较的张量</p>
</li>
<li>
<p><strong>其他</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a> <em>或</em> <em>python：float</em> )–要比较的张量或值</p>
</li>
<li>
<p><strong>输出</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a> <em>，</em> <em>可选</em>）–输出张量。 必须是 &lt;cite&gt;ByteTensor&lt;/cite&gt;</p>
</li>
</ul>
<p>Returns</p>
<p><code>torch.BoolTensor</code>在每个比较为真的位置包含一个真</p>
<p>Return type</p>
<p><a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-342-1" name="__codelineno-342-1" href="#__codelineno-342-1"></a>&gt;&gt;&gt; torch.eq(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))
<a id="__codelineno-342-2" name="__codelineno-342-2" href="#__codelineno-342-2"></a>tensor([[ 1,  0],
<a id="__codelineno-342-3" name="__codelineno-342-3" href="#__codelineno-342-3"></a>        [ 0,  1]], dtype=torch.uint8)
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-343-1" name="__codelineno-343-1" href="#__codelineno-343-1"></a>torch.equal(input, other) → bool¶
</code></pre></div>
<p>如果两个张量具有相同的大小和元素，则为<code>True</code>，否则为<code>False</code>。</p>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-344-1" name="__codelineno-344-1" href="#__codelineno-344-1"></a>&gt;&gt;&gt; torch.equal(torch.tensor([1, 2]), torch.tensor([1, 2]))
<a id="__codelineno-344-2" name="__codelineno-344-2" href="#__codelineno-344-2"></a>True
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-345-1" name="__codelineno-345-1" href="#__codelineno-345-1"></a>torch.ge(input, other, out=None) → Tensor¶
</code></pre></div>
<p>逐元素计算<img alt="" src="../img/5807a6d54e9791f46e3bc29daa0ee7a1.jpg" />。</p>
<p>The second argument can be a number or a tensor whose shape is <a href="notes/broadcasting.html#broadcasting-semantics">broadcastable</a> with the first argument.</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the tensor to compare</p>
</li>
<li>
<p><strong>other</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a> <em>or</em> <em>python:float</em>) – the tensor or value to compare</p>
</li>
<li>
<p><strong>输出</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a> <em>，</em> <em>可选</em>）–输出张量必须为 &lt;cite&gt;BoolTensor&lt;/cite&gt;</p>
</li>
</ul>
<p>Returns</p>
<p>A <code>torch.BoolTensor</code> containing a True at each location where comparison is true</p>
<p>Return type</p>
<p><a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-346-1" name="__codelineno-346-1" href="#__codelineno-346-1"></a>&gt;&gt;&gt; torch.ge(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))
<a id="__codelineno-346-2" name="__codelineno-346-2" href="#__codelineno-346-2"></a>tensor([[True, True], [False, True]])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-347-1" name="__codelineno-347-1" href="#__codelineno-347-1"></a>torch.gt(input, other, out=None) → Tensor¶
</code></pre></div>
<p>逐元素计算<img alt="" src="../img/0f05b585a740804201541c969a0fcf12.jpg" />。</p>
<p>The second argument can be a number or a tensor whose shape is <a href="notes/broadcasting.html#broadcasting-semantics">broadcastable</a> with the first argument.</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the tensor to compare</p>
</li>
<li>
<p><strong>other</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a> <em>or</em> <em>python:float</em>) – the tensor or value to compare</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor that must be a &lt;cite&gt;BoolTensor&lt;/cite&gt;</p>
</li>
</ul>
<p>Returns</p>
<p>A <code>torch.BoolTensor</code> containing a True at each location where comparison is true</p>
<p>Return type</p>
<p><a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-348-1" name="__codelineno-348-1" href="#__codelineno-348-1"></a>&gt;&gt;&gt; torch.gt(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))
<a id="__codelineno-348-2" name="__codelineno-348-2" href="#__codelineno-348-2"></a>tensor([[False, True], [False, False]])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-349-1" name="__codelineno-349-1" href="#__codelineno-349-1"></a>torch.isfinite()¶
</code></pre></div>
<p>返回带有布尔元素的新张量，布尔元素表示每个元素是否为&lt;cite&gt;有限&lt;/cite&gt;。</p>
<blockquote>
<div class="highlight"><pre><span></span><code><a id="__codelineno-350-1" name="__codelineno-350-1" href="#__codelineno-350-1"></a>Arguments:
</code></pre></div>
<p>张量(张量）：要检查的张量</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-351-1" name="__codelineno-351-1" href="#__codelineno-351-1"></a>Returns:
</code></pre></div>
<p>张量：<code>A torch.Tensor with dtype torch.bool</code>在有限元素的每个位置均包含 True，否则包含 False</p>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-352-1" name="__codelineno-352-1" href="#__codelineno-352-1"></a>&amp;gt;&amp;gt;&amp;gt; torch.isfinite(torch.tensor([1, float(&#39;inf&#39;), 2, float(&#39;-inf&#39;), float(&#39;nan&#39;)]))
<a id="__codelineno-352-2" name="__codelineno-352-2" href="#__codelineno-352-2"></a>tensor([True,  False,  True,  False,  False])
</code></pre></div>
</blockquote>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-353-1" name="__codelineno-353-1" href="#__codelineno-353-1"></a>torch.isinf(tensor)¶
</code></pre></div>
<p>返回带有布尔元素的新张量，该布尔元素表示每个元素是否为 &lt;cite&gt;+/- INF&lt;/cite&gt; 。</p>
<p>Parameters</p>
<p><strong>张量</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–要检查的张量</p>
<p>Returns</p>
<p><code>A torch.Tensor with dtype torch.bool</code>在 &lt;cite&gt;+/- INF&lt;/cite&gt; 元素的每个位置均包含 True，否则包含 False</p>
<p>Return type</p>
<p><a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-354-1" name="__codelineno-354-1" href="#__codelineno-354-1"></a>&gt;&gt;&gt; torch.isinf(torch.tensor([1, float(&#39;inf&#39;), 2, float(&#39;-inf&#39;), float(&#39;nan&#39;)]))
<a id="__codelineno-354-2" name="__codelineno-354-2" href="#__codelineno-354-2"></a>tensor([False,  True,  False,  True,  False])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-355-1" name="__codelineno-355-1" href="#__codelineno-355-1"></a>torch.isnan()¶
</code></pre></div>
<p>返回带有布尔元素的新张量，布尔元素表示每个元素是否为 &lt;cite&gt;NaN&lt;/cite&gt; 。</p>
<p>Parameters</p>
<p><strong>输入</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–要检查的张量</p>
<p>Returns</p>
<p>在 &lt;cite&gt;NaN&lt;/cite&gt; 元素的每个位置包含 True 的<code>torch.BoolTensor</code>。</p>
<p>Return type</p>
<p><a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-356-1" name="__codelineno-356-1" href="#__codelineno-356-1"></a>&gt;&gt;&gt; torch.isnan(torch.tensor([1, float(&#39;nan&#39;), 2]))
<a id="__codelineno-356-2" name="__codelineno-356-2" href="#__codelineno-356-2"></a>tensor([False, True, False])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-357-1" name="__codelineno-357-1" href="#__codelineno-357-1"></a>torch.kthvalue(input, k, dim=None, keepdim=False, out=None) -&gt; (Tensor, LongTensor)¶
</code></pre></div>
<p>返回一个命名元组<code>(values, indices)</code>，其中<code>values</code>是在给定维度<code>dim</code>中<code>input</code>张量的每一行的第<code>k</code>个最小元素。 <code>indices</code>是找到的每个元素的索引位置。</p>
<p>如果未提供<code>dim</code>，则选择&lt;cite&gt;输入&lt;/cite&gt;的最后尺寸。</p>
<p>如果<code>keepdim</code>为<code>True</code>，则<code>values</code>和<code>indices</code>张量与<code>input</code>的大小相同，但尺寸为<code>dim</code>的张量为 1。否则，<code>dim</code>会受到挤压 (参见 <a href="#torch.squeeze" title="torch.squeeze"><code>torch.squeeze()</code></a>)，导致<code>values</code>和<code>indices</code>张量的尺寸都比<code>input</code>张量小 1。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>k</strong>  (<em>python：int</em> )–第 k 个最小元素的 k</p>
</li>
<li>
<p><strong>暗淡的</strong> (<em>python：int</em> <em>，</em> <em>可选</em>）–沿第 k 个值查找尺寸</p>
</li>
<li>
<p><strong>keepdim</strong> (<em>bool</em>) – whether the output tensor has <code>dim</code> retained or not.</p>
</li>
<li>
<p><strong>out</strong> (<em>元组</em> <em>，</em> <em>可选</em>）–(Tensor，LongTensor）的输出元组可以可选地用作输出缓冲区</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-358-1" name="__codelineno-358-1" href="#__codelineno-358-1"></a>&gt;&gt;&gt; x = torch.arange(1., 6.)
<a id="__codelineno-358-2" name="__codelineno-358-2" href="#__codelineno-358-2"></a>&gt;&gt;&gt; x
<a id="__codelineno-358-3" name="__codelineno-358-3" href="#__codelineno-358-3"></a>tensor([ 1.,  2.,  3.,  4.,  5.])
<a id="__codelineno-358-4" name="__codelineno-358-4" href="#__codelineno-358-4"></a>&gt;&gt;&gt; torch.kthvalue(x, 4)
<a id="__codelineno-358-5" name="__codelineno-358-5" href="#__codelineno-358-5"></a>torch.return_types.kthvalue(values=tensor(4.), indices=tensor(3))
<a id="__codelineno-358-6" name="__codelineno-358-6" href="#__codelineno-358-6"></a>
<a id="__codelineno-358-7" name="__codelineno-358-7" href="#__codelineno-358-7"></a>&gt;&gt;&gt; x=torch.arange(1.,7.).resize_(2,3)
<a id="__codelineno-358-8" name="__codelineno-358-8" href="#__codelineno-358-8"></a>&gt;&gt;&gt; x
<a id="__codelineno-358-9" name="__codelineno-358-9" href="#__codelineno-358-9"></a>tensor([[ 1.,  2.,  3.],
<a id="__codelineno-358-10" name="__codelineno-358-10" href="#__codelineno-358-10"></a>        [ 4.,  5.,  6.]])
<a id="__codelineno-358-11" name="__codelineno-358-11" href="#__codelineno-358-11"></a>&gt;&gt;&gt; torch.kthvalue(x, 2, 0, True)
<a id="__codelineno-358-12" name="__codelineno-358-12" href="#__codelineno-358-12"></a>torch.return_types.kthvalue(values=tensor([[4., 5., 6.]]), indices=tensor([[1, 1, 1]]))
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-359-1" name="__codelineno-359-1" href="#__codelineno-359-1"></a>torch.le(input, other, out=None) → Tensor¶
</code></pre></div>
<p>逐元素计算<img alt="" src="../img/155c37e00e2691f1d70e95a9eb0156f1.jpg" />。</p>
<p>The second argument can be a number or a tensor whose shape is <a href="notes/broadcasting.html#broadcasting-semantics">broadcastable</a> with the first argument.</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the tensor to compare</p>
</li>
<li>
<p><strong>other</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a> <em>or</em> <em>python:float</em>) – the tensor or value to compare</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor that must be a &lt;cite&gt;BoolTensor&lt;/cite&gt;</p>
</li>
</ul>
<p>Returns</p>
<p>A <code>torch.BoolTensor</code> containing a True at each location where comparison is true</p>
<p>Return type</p>
<p><a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-360-1" name="__codelineno-360-1" href="#__codelineno-360-1"></a>&gt;&gt;&gt; torch.le(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))
<a id="__codelineno-360-2" name="__codelineno-360-2" href="#__codelineno-360-2"></a>tensor([[True, False], [True, True]])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-361-1" name="__codelineno-361-1" href="#__codelineno-361-1"></a>torch.lt(input, other, out=None) → Tensor¶
</code></pre></div>
<p>逐元素计算<img alt="" src="../img/6953e508ba80b2b7609d4e21f205f593.jpg" />。</p>
<p>The second argument can be a number or a tensor whose shape is <a href="notes/broadcasting.html#broadcasting-semantics">broadcastable</a> with the first argument.</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the tensor to compare</p>
</li>
<li>
<p><strong>other</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a> <em>or</em> <em>python:float</em>) – the tensor or value to compare</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor that must be a &lt;cite&gt;BoolTensor&lt;/cite&gt;</p>
</li>
</ul>
<p>Returns</p>
<p>在每个比较为真的位置处包含“真”的 &lt;cite&gt;Torch.BoolTensor&lt;/cite&gt;</p>
<p>Return type</p>
<p><a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-362-1" name="__codelineno-362-1" href="#__codelineno-362-1"></a>&gt;&gt;&gt; torch.lt(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))
<a id="__codelineno-362-2" name="__codelineno-362-2" href="#__codelineno-362-2"></a>tensor([[False, False], [True, False]])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-363-1" name="__codelineno-363-1" href="#__codelineno-363-1"></a>torch.max()¶
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-364-1" name="__codelineno-364-1" href="#__codelineno-364-1"></a>torch.max(input) → Tensor
</code></pre></div>
<p>返回<code>input</code>张量中所有元素的最大值。</p>
<p>Parameters</p>
<p><strong>{input}</strong> –</p>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-365-1" name="__codelineno-365-1" href="#__codelineno-365-1"></a>&gt;&gt;&gt; a = torch.randn(1, 3)
<a id="__codelineno-365-2" name="__codelineno-365-2" href="#__codelineno-365-2"></a>&gt;&gt;&gt; a
<a id="__codelineno-365-3" name="__codelineno-365-3" href="#__codelineno-365-3"></a>tensor([[ 0.6763,  0.7445, -2.2369]])
<a id="__codelineno-365-4" name="__codelineno-365-4" href="#__codelineno-365-4"></a>&gt;&gt;&gt; torch.max(a)
<a id="__codelineno-365-5" name="__codelineno-365-5" href="#__codelineno-365-5"></a>tensor(0.7445)
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-366-1" name="__codelineno-366-1" href="#__codelineno-366-1"></a>torch.max(input, dim, keepdim=False, out=None) -&gt; (Tensor, LongTensor)
</code></pre></div>
<p>返回一个命名元组<code>(values, indices)</code>，其中<code>values</code>是在给定维度<code>dim</code>中<code>input</code>张量的每一行的最大值。 <code>indices</code>是找到的每个最大值(argmax）的索引位置。</p>
<p>如果<code>keepdim</code>为<code>True</code>，则输出张量的大小与<code>input</code>相同，只是尺寸为 1 的尺寸为<code>dim</code>。否则，将压缩<code>dim</code>(请参见 <a href="#torch.squeeze" title="torch.squeeze"><code>torch.squeeze()</code></a>)，导致输出张量的尺寸比<code>input</code>小 1。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>{input}</strong> –</p>
</li>
<li>
<p><strong>{dim}</strong> –</p>
</li>
<li>
<p><strong>默认</strong> (<em>{keepdim}</em> )– <code>False</code>。</p>
</li>
<li>
<p><strong>输出</strong>(<em>元组</em> <em>，</em> <em>可选</em>）–两个输出张量的结果元组(max，max_indices）</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-367-1" name="__codelineno-367-1" href="#__codelineno-367-1"></a>&gt;&gt;&gt; a = torch.randn(4, 4)
<a id="__codelineno-367-2" name="__codelineno-367-2" href="#__codelineno-367-2"></a>&gt;&gt;&gt; a
<a id="__codelineno-367-3" name="__codelineno-367-3" href="#__codelineno-367-3"></a>tensor([[-1.2360, -0.2942, -0.1222,  0.8475],
<a id="__codelineno-367-4" name="__codelineno-367-4" href="#__codelineno-367-4"></a>        [ 1.1949, -1.1127, -2.2379, -0.6702],
<a id="__codelineno-367-5" name="__codelineno-367-5" href="#__codelineno-367-5"></a>        [ 1.5717, -0.9207,  0.1297, -1.8768],
<a id="__codelineno-367-6" name="__codelineno-367-6" href="#__codelineno-367-6"></a>        [-0.6172,  1.0036, -0.6060, -0.2432]])
<a id="__codelineno-367-7" name="__codelineno-367-7" href="#__codelineno-367-7"></a>&gt;&gt;&gt; torch.max(a, 1)
<a id="__codelineno-367-8" name="__codelineno-367-8" href="#__codelineno-367-8"></a>torch.return_types.max(values=tensor([0.8475, 1.1949, 1.5717, 1.0036]), indices=tensor([3, 0, 0, 1]))
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-368-1" name="__codelineno-368-1" href="#__codelineno-368-1"></a>torch.max(input, other, out=None) → Tensor
</code></pre></div>
<p>将张量<code>input</code>的每个元素与张量<code>other</code>的对应元素进行比较，并获得逐个元素的最大值。</p>
<p><code>input</code>和<code>other</code>的形状不需要匹配，但它们必须是<a href="notes/broadcasting.html#broadcasting-semantics">可广播的</a>。</p>
<p><img alt="" src="../img/6b8f532893b7a5323f14fb2e70fe152e.jpg" /></p>
<p>Note</p>
<p>当形状不匹配时，返回的输出张量的形状遵循<a href="notes/broadcasting.html#broadcasting-semantics">广播规则</a>。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>other</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the second input tensor</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-369-1" name="__codelineno-369-1" href="#__codelineno-369-1"></a>&gt;&gt;&gt; a = torch.randn(4)
<a id="__codelineno-369-2" name="__codelineno-369-2" href="#__codelineno-369-2"></a>&gt;&gt;&gt; a
<a id="__codelineno-369-3" name="__codelineno-369-3" href="#__codelineno-369-3"></a>tensor([ 0.2942, -0.7416,  0.2653, -0.1584])
<a id="__codelineno-369-4" name="__codelineno-369-4" href="#__codelineno-369-4"></a>&gt;&gt;&gt; b = torch.randn(4)
<a id="__codelineno-369-5" name="__codelineno-369-5" href="#__codelineno-369-5"></a>&gt;&gt;&gt; b
<a id="__codelineno-369-6" name="__codelineno-369-6" href="#__codelineno-369-6"></a>tensor([ 0.8722, -1.7421, -0.4141, -0.5055])
<a id="__codelineno-369-7" name="__codelineno-369-7" href="#__codelineno-369-7"></a>&gt;&gt;&gt; torch.max(a, b)
<a id="__codelineno-369-8" name="__codelineno-369-8" href="#__codelineno-369-8"></a>tensor([ 0.8722, -0.7416,  0.2653, -0.1584])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-370-1" name="__codelineno-370-1" href="#__codelineno-370-1"></a>torch.min()¶
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-371-1" name="__codelineno-371-1" href="#__codelineno-371-1"></a>torch.min(input) → Tensor
</code></pre></div>
<p>返回<code>input</code>张量中所有元素的最小值。</p>
<p>Parameters</p>
<p><strong>{input}</strong> –</p>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-372-1" name="__codelineno-372-1" href="#__codelineno-372-1"></a>&gt;&gt;&gt; a = torch.randn(1, 3)
<a id="__codelineno-372-2" name="__codelineno-372-2" href="#__codelineno-372-2"></a>&gt;&gt;&gt; a
<a id="__codelineno-372-3" name="__codelineno-372-3" href="#__codelineno-372-3"></a>tensor([[ 0.6750,  1.0857,  1.7197]])
<a id="__codelineno-372-4" name="__codelineno-372-4" href="#__codelineno-372-4"></a>&gt;&gt;&gt; torch.min(a)
<a id="__codelineno-372-5" name="__codelineno-372-5" href="#__codelineno-372-5"></a>tensor(0.6750)
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-373-1" name="__codelineno-373-1" href="#__codelineno-373-1"></a>torch.min(input, dim, keepdim=False, out=None) -&gt; (Tensor, LongTensor)
</code></pre></div>
<p>返回一个命名元组<code>(values, indices)</code>，其中<code>values</code>是在给定维度<code>dim</code>中<code>input</code>张量的每一行的最小值。 <code>indices</code>是找到的每个最小值的索引位置(argmin）。</p>
<p>如果<code>keepdim</code>为<code>True</code>，则输出张量的大小与<code>input</code>相同，只是尺寸为 1 的尺寸为<code>dim</code>。否则，将压缩<code>dim</code>(请参见 <a href="#torch.squeeze" title="torch.squeeze"><code>torch.squeeze()</code></a>)，导致输出张量的尺寸比<code>input</code>小 1。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>{input}</strong> –</p>
</li>
<li>
<p><strong>{dim}</strong> –</p>
</li>
<li>
<p><strong>{keepdim}</strong> –</p>
</li>
<li>
<p><strong>输出</strong>(<em>元组</em> <em>，</em> <em>可选</em>）–两个输出张量的元组(min，min_indices）</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-374-1" name="__codelineno-374-1" href="#__codelineno-374-1"></a>&gt;&gt;&gt; a = torch.randn(4, 4)
<a id="__codelineno-374-2" name="__codelineno-374-2" href="#__codelineno-374-2"></a>&gt;&gt;&gt; a
<a id="__codelineno-374-3" name="__codelineno-374-3" href="#__codelineno-374-3"></a>tensor([[-0.6248,  1.1334, -1.1899, -0.2803],
<a id="__codelineno-374-4" name="__codelineno-374-4" href="#__codelineno-374-4"></a>        [-1.4644, -0.2635, -0.3651,  0.6134],
<a id="__codelineno-374-5" name="__codelineno-374-5" href="#__codelineno-374-5"></a>        [ 0.2457,  0.0384,  1.0128,  0.7015],
<a id="__codelineno-374-6" name="__codelineno-374-6" href="#__codelineno-374-6"></a>        [-0.1153,  2.9849,  2.1458,  0.5788]])
<a id="__codelineno-374-7" name="__codelineno-374-7" href="#__codelineno-374-7"></a>&gt;&gt;&gt; torch.min(a, 1)
<a id="__codelineno-374-8" name="__codelineno-374-8" href="#__codelineno-374-8"></a>torch.return_types.min(values=tensor([-1.1899, -1.4644,  0.0384, -0.1153]), indices=tensor([2, 0, 1, 0]))
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-375-1" name="__codelineno-375-1" href="#__codelineno-375-1"></a>torch.min(input, other, out=None) → Tensor
</code></pre></div>
<p>将张量<code>input</code>的每个元素与张量<code>other</code>的对应元素进行比较，并按元素取最小值。 返回结果张量。</p>
<p>The shapes of <code>input</code> and <code>other</code> don't need to match, but they must be <a href="notes/broadcasting.html#broadcasting-semantics">broadcastable</a>.</p>
<p><img alt="" src="../img/f033730f350678f87f2f08ee833a82fe.jpg" /></p>
<p>Note</p>
<p>When the shapes do not match, the shape of the returned output tensor follows the <a href="notes/broadcasting.html#broadcasting-semantics">broadcasting rules</a>.</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>other</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the second input tensor</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-376-1" name="__codelineno-376-1" href="#__codelineno-376-1"></a>&gt;&gt;&gt; a = torch.randn(4)
<a id="__codelineno-376-2" name="__codelineno-376-2" href="#__codelineno-376-2"></a>&gt;&gt;&gt; a
<a id="__codelineno-376-3" name="__codelineno-376-3" href="#__codelineno-376-3"></a>tensor([ 0.8137, -1.1740, -0.6460,  0.6308])
<a id="__codelineno-376-4" name="__codelineno-376-4" href="#__codelineno-376-4"></a>&gt;&gt;&gt; b = torch.randn(4)
<a id="__codelineno-376-5" name="__codelineno-376-5" href="#__codelineno-376-5"></a>&gt;&gt;&gt; b
<a id="__codelineno-376-6" name="__codelineno-376-6" href="#__codelineno-376-6"></a>tensor([-0.1369,  0.1555,  0.4019, -0.1929])
<a id="__codelineno-376-7" name="__codelineno-376-7" href="#__codelineno-376-7"></a>&gt;&gt;&gt; torch.min(a, b)
<a id="__codelineno-376-8" name="__codelineno-376-8" href="#__codelineno-376-8"></a>tensor([-0.1369, -1.1740, -0.6460, -0.1929])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-377-1" name="__codelineno-377-1" href="#__codelineno-377-1"></a>torch.ne(input, other, out=None) → Tensor¶
</code></pre></div>
<p>逐元素计算<img alt="" src="../img/efbfcdce474b4ae10bb5dacb46481266.jpg" />。</p>
<p>The second argument can be a number or a tensor whose shape is <a href="notes/broadcasting.html#broadcasting-semantics">broadcastable</a> with the first argument.</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the tensor to compare</p>
</li>
<li>
<p><strong>other</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a> <em>or</em> <em>python:float</em>) – the tensor or value to compare</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor that must be a &lt;cite&gt;BoolTensor&lt;/cite&gt;</p>
</li>
</ul>
<p>Returns</p>
<p><code>torch.BoolTensor</code>在比较为真的每个位置都包含“真”。</p>
<p>Return type</p>
<p><a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-378-1" name="__codelineno-378-1" href="#__codelineno-378-1"></a>&gt;&gt;&gt; torch.ne(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))
<a id="__codelineno-378-2" name="__codelineno-378-2" href="#__codelineno-378-2"></a>tensor([[False, True], [True, False]])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-379-1" name="__codelineno-379-1" href="#__codelineno-379-1"></a>torch.sort(input, dim=-1, descending=False, out=None) -&gt; (Tensor, LongTensor)¶
</code></pre></div>
<p>沿给定维度按值升序对<code>input</code>张量的元素进行排序。</p>
<p>If <code>dim</code> is not given, the last dimension of the &lt;cite&gt;input&lt;/cite&gt; is chosen.</p>
<p>如果<code>descending</code>为<code>True</code>，则元素将按值降序排序。</p>
<p>返回一个(值，索引）的命名元组，其中&lt;cite&gt;值&lt;/cite&gt;是排序的值，&lt;cite&gt;索引&lt;/cite&gt;是原始&lt;cite&gt;输入&lt;/cite&gt;张量中元素的索引。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>dim</strong> (<em>python:int__,</em> <em>optional</em>) – the dimension to sort along</p>
</li>
<li>
<p><strong>descending</strong> (<em>bool__,</em> <em>optional</em>) – controls the sorting order (ascending or descending)</p>
</li>
<li>
<p><strong>输出</strong>(<em>元组</em> <em>，</em> <em>可选</em>）–(&lt;cite&gt;张量&lt;/cite&gt;， &lt;cite&gt;LongTensor&lt;/cite&gt; )，可以选择将其用作输出缓冲区</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-380-1" name="__codelineno-380-1" href="#__codelineno-380-1"></a>&gt;&gt;&gt; x = torch.randn(3, 4)
<a id="__codelineno-380-2" name="__codelineno-380-2" href="#__codelineno-380-2"></a>&gt;&gt;&gt; sorted, indices = torch.sort(x)
<a id="__codelineno-380-3" name="__codelineno-380-3" href="#__codelineno-380-3"></a>&gt;&gt;&gt; sorted
<a id="__codelineno-380-4" name="__codelineno-380-4" href="#__codelineno-380-4"></a>tensor([[-0.2162,  0.0608,  0.6719,  2.3332],
<a id="__codelineno-380-5" name="__codelineno-380-5" href="#__codelineno-380-5"></a>        [-0.5793,  0.0061,  0.6058,  0.9497],
<a id="__codelineno-380-6" name="__codelineno-380-6" href="#__codelineno-380-6"></a>        [-0.5071,  0.3343,  0.9553,  1.0960]])
<a id="__codelineno-380-7" name="__codelineno-380-7" href="#__codelineno-380-7"></a>&gt;&gt;&gt; indices
<a id="__codelineno-380-8" name="__codelineno-380-8" href="#__codelineno-380-8"></a>tensor([[ 1,  0,  2,  3],
<a id="__codelineno-380-9" name="__codelineno-380-9" href="#__codelineno-380-9"></a>        [ 3,  1,  0,  2],
<a id="__codelineno-380-10" name="__codelineno-380-10" href="#__codelineno-380-10"></a>        [ 0,  3,  1,  2]])
<a id="__codelineno-380-11" name="__codelineno-380-11" href="#__codelineno-380-11"></a>
<a id="__codelineno-380-12" name="__codelineno-380-12" href="#__codelineno-380-12"></a>&gt;&gt;&gt; sorted, indices = torch.sort(x, 0)
<a id="__codelineno-380-13" name="__codelineno-380-13" href="#__codelineno-380-13"></a>&gt;&gt;&gt; sorted
<a id="__codelineno-380-14" name="__codelineno-380-14" href="#__codelineno-380-14"></a>tensor([[-0.5071, -0.2162,  0.6719, -0.5793],
<a id="__codelineno-380-15" name="__codelineno-380-15" href="#__codelineno-380-15"></a>        [ 0.0608,  0.0061,  0.9497,  0.3343],
<a id="__codelineno-380-16" name="__codelineno-380-16" href="#__codelineno-380-16"></a>        [ 0.6058,  0.9553,  1.0960,  2.3332]])
<a id="__codelineno-380-17" name="__codelineno-380-17" href="#__codelineno-380-17"></a>&gt;&gt;&gt; indices
<a id="__codelineno-380-18" name="__codelineno-380-18" href="#__codelineno-380-18"></a>tensor([[ 2,  0,  0,  1],
<a id="__codelineno-380-19" name="__codelineno-380-19" href="#__codelineno-380-19"></a>        [ 0,  1,  1,  2],
<a id="__codelineno-380-20" name="__codelineno-380-20" href="#__codelineno-380-20"></a>        [ 1,  2,  2,  0]])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-381-1" name="__codelineno-381-1" href="#__codelineno-381-1"></a>torch.topk(input, k, dim=None, largest=True, sorted=True, out=None) -&gt; (Tensor, LongTensor)¶
</code></pre></div>
<p>返回沿给定维度的给定<code>input</code>张量的<code>k</code>最大元素。</p>
<p>If <code>dim</code> is not given, the last dimension of the &lt;cite&gt;input&lt;/cite&gt; is chosen.</p>
<p>如果<code>largest</code>为<code>False</code>，则返回 &lt;cite&gt;k&lt;/cite&gt; 个最小的元素。</p>
<p>返回&lt;cite&gt;(值，索引）&lt;/cite&gt;的命名元组，其中&lt;cite&gt;索引&lt;/cite&gt;是原始&lt;cite&gt;输入&lt;/cite&gt;张量中元素的索引。</p>
<p>布尔选项<code>sorted</code>如果为<code>True</code>，将确保返回的 &lt;cite&gt;k&lt;/cite&gt; 元素本身已排序</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>k</strong>  (<em>python：int</em> )–“ top-k”中的 k</p>
</li>
<li>
<p><strong>dim</strong> (<em>python:int__,</em> <em>optional</em>) – the dimension to sort along</p>
</li>
<li>
<p><strong>最大的</strong> (<em>bool</em> <em>，</em> <em>可选</em>）–控制是返回最大还是最小元素</p>
</li>
<li>
<p><strong>排序的</strong> (<em>bool</em> <em>，</em> <em>可选</em>）–控制是否按排序顺序返回元素</p>
</li>
<li>
<p><strong>out</strong> (<em>元组</em> <em>，</em> <em>可选</em>）–可以选择提供(Tensor，LongTensor）的输出元组 缓冲区</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-382-1" name="__codelineno-382-1" href="#__codelineno-382-1"></a>&gt;&gt;&gt; x = torch.arange(1., 6.)
<a id="__codelineno-382-2" name="__codelineno-382-2" href="#__codelineno-382-2"></a>&gt;&gt;&gt; x
<a id="__codelineno-382-3" name="__codelineno-382-3" href="#__codelineno-382-3"></a>tensor([ 1.,  2.,  3.,  4.,  5.])
<a id="__codelineno-382-4" name="__codelineno-382-4" href="#__codelineno-382-4"></a>&gt;&gt;&gt; torch.topk(x, 3)
<a id="__codelineno-382-5" name="__codelineno-382-5" href="#__codelineno-382-5"></a>torch.return_types.topk(values=tensor([5., 4., 3.]), indices=tensor([4, 3, 2]))
</code></pre></div>
<h3 id="_15">光谱操作</h3>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-383-1" name="__codelineno-383-1" href="#__codelineno-383-1"></a>torch.fft(input, signal_ndim, normalized=False) → Tensor¶
</code></pre></div>
<p>复数到复数离散傅立叶变换</p>
<p>此方法计算复数到复数离散傅里叶变换。 忽略批次尺寸，它将计算以下表达式：</p>
<p><img alt="" src="../img/f1d658cd4fac30eca4c9293d39d0d2fe.jpg" /></p>
<p>其中<img alt="" src="../img/95d07e221a9b945feb93422a44ac3d42.jpg" /> = <code>signal_ndim</code>是信号尺寸的数量，<img alt="" src="../img/8d61ccc06c1a60ca6e3c19f12aee4f33.jpg" />是信号尺寸<img alt="" src="../img/db26d1a59be5965889bd4d5533b7be61.jpg" />的尺寸。</p>
<p>此方法支持<code>signal_ndim</code>指示的 1D，2D 和 3D 复数到复数转换。 <code>input</code>必须是张量，其最后一个尺寸为 2，代表复数的实部和虚部，并且至少应具有<code>signal_ndim + 1</code>个尺寸，并可以选择任意数量的前批尺寸。 如果<code>normalized</code>设置为<code>True</code>，则通过将结果除以<img alt="" src="../img/ed8d1a6498daf3d5b9174f9c1535b2bd.jpg" />来对结果进行归一化，以使运算符为一元。</p>
<p>将实部和虚部一起返回为<code>input</code>形状相同的一个张量。</p>
<p>此函数的反函数为 <a href="#torch.ifft" title="torch.ifft"><code>ifft()</code></a> 。</p>
<p>Note</p>
<p>对于 CUDA 张量，LRU 缓存用于 cuFFT 计划，以加快在具有相同配置的相同几何形状的张量上重复运行 FFT 方法的速度。 有关如何监视和控制缓存的更多详细信息，请参见 <a href="notes/cuda.html#cufft-plan-cache">cuFFT 计划缓存</a>。</p>
<p>Warning</p>
<p>对于 CPU 张量，此方法当前仅适用于 MKL。 使用<code>torch.backends.mkl.is_available()</code>检查是否安装了 MKL。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>输入</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–至少<code>signal_ndim</code> <code>+ 1</code>尺寸的输入张量</p>
</li>
<li>
<p><strong>signal_ndim</strong>  (<em>python：int</em> )–每个信号中的维数。 <code>signal_ndim</code>只能是 1、2 或 3</p>
</li>
<li>
<p><strong>标准化的</strong> (<em>bool</em> <em>，</em> <em>可选</em>）–控制是否返回标准化结果。 默认值：<code>False</code></p>
</li>
</ul>
<p>Returns</p>
<p>包含复数到复数傅里叶变换结果的张量</p>
<p>Return type</p>
<p><a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-384-1" name="__codelineno-384-1" href="#__codelineno-384-1"></a>&gt;&gt;&gt; # unbatched 2D FFT
<a id="__codelineno-384-2" name="__codelineno-384-2" href="#__codelineno-384-2"></a>&gt;&gt;&gt; x = torch.randn(4, 3, 2)
<a id="__codelineno-384-3" name="__codelineno-384-3" href="#__codelineno-384-3"></a>&gt;&gt;&gt; torch.fft(x, 2)
<a id="__codelineno-384-4" name="__codelineno-384-4" href="#__codelineno-384-4"></a>tensor([[[-0.0876,  1.7835],
<a id="__codelineno-384-5" name="__codelineno-384-5" href="#__codelineno-384-5"></a>         [-2.0399, -2.9754],
<a id="__codelineno-384-6" name="__codelineno-384-6" href="#__codelineno-384-6"></a>         [ 4.4773, -5.0119]],
<a id="__codelineno-384-7" name="__codelineno-384-7" href="#__codelineno-384-7"></a>
<a id="__codelineno-384-8" name="__codelineno-384-8" href="#__codelineno-384-8"></a>        [[-1.5716,  2.7631],
<a id="__codelineno-384-9" name="__codelineno-384-9" href="#__codelineno-384-9"></a>         [-3.8846,  5.2652],
<a id="__codelineno-384-10" name="__codelineno-384-10" href="#__codelineno-384-10"></a>         [ 0.2046, -0.7088]],
<a id="__codelineno-384-11" name="__codelineno-384-11" href="#__codelineno-384-11"></a>
<a id="__codelineno-384-12" name="__codelineno-384-12" href="#__codelineno-384-12"></a>        [[ 1.9938, -0.5901],
<a id="__codelineno-384-13" name="__codelineno-384-13" href="#__codelineno-384-13"></a>         [ 6.5637,  6.4556],
<a id="__codelineno-384-14" name="__codelineno-384-14" href="#__codelineno-384-14"></a>         [ 2.9865,  4.9318]],
<a id="__codelineno-384-15" name="__codelineno-384-15" href="#__codelineno-384-15"></a>
<a id="__codelineno-384-16" name="__codelineno-384-16" href="#__codelineno-384-16"></a>        [[ 7.0193,  1.1742],
<a id="__codelineno-384-17" name="__codelineno-384-17" href="#__codelineno-384-17"></a>         [-1.3717, -2.1084],
<a id="__codelineno-384-18" name="__codelineno-384-18" href="#__codelineno-384-18"></a>         [ 2.0289,  2.9357]]])
<a id="__codelineno-384-19" name="__codelineno-384-19" href="#__codelineno-384-19"></a>&gt;&gt;&gt; # batched 1D FFT
<a id="__codelineno-384-20" name="__codelineno-384-20" href="#__codelineno-384-20"></a>&gt;&gt;&gt; torch.fft(x, 1)
<a id="__codelineno-384-21" name="__codelineno-384-21" href="#__codelineno-384-21"></a>tensor([[[ 1.8385,  1.2827],
<a id="__codelineno-384-22" name="__codelineno-384-22" href="#__codelineno-384-22"></a>         [-0.1831,  1.6593],
<a id="__codelineno-384-23" name="__codelineno-384-23" href="#__codelineno-384-23"></a>         [ 2.4243,  0.5367]],
<a id="__codelineno-384-24" name="__codelineno-384-24" href="#__codelineno-384-24"></a>
<a id="__codelineno-384-25" name="__codelineno-384-25" href="#__codelineno-384-25"></a>        [[-0.9176, -1.5543],
<a id="__codelineno-384-26" name="__codelineno-384-26" href="#__codelineno-384-26"></a>         [-3.9943, -2.9860],
<a id="__codelineno-384-27" name="__codelineno-384-27" href="#__codelineno-384-27"></a>         [ 1.2838, -2.9420]],
<a id="__codelineno-384-28" name="__codelineno-384-28" href="#__codelineno-384-28"></a>
<a id="__codelineno-384-29" name="__codelineno-384-29" href="#__codelineno-384-29"></a>        [[-0.8854, -0.6860],
<a id="__codelineno-384-30" name="__codelineno-384-30" href="#__codelineno-384-30"></a>         [ 2.4450,  0.0808],
<a id="__codelineno-384-31" name="__codelineno-384-31" href="#__codelineno-384-31"></a>         [ 1.3076, -0.5768]],
<a id="__codelineno-384-32" name="__codelineno-384-32" href="#__codelineno-384-32"></a>
<a id="__codelineno-384-33" name="__codelineno-384-33" href="#__codelineno-384-33"></a>        [[-0.1231,  2.7411],
<a id="__codelineno-384-34" name="__codelineno-384-34" href="#__codelineno-384-34"></a>         [-0.3075, -1.7295],
<a id="__codelineno-384-35" name="__codelineno-384-35" href="#__codelineno-384-35"></a>         [-0.5384, -2.0299]]])
<a id="__codelineno-384-36" name="__codelineno-384-36" href="#__codelineno-384-36"></a>&gt;&gt;&gt; # arbitrary number of batch dimensions, 2D FFT
<a id="__codelineno-384-37" name="__codelineno-384-37" href="#__codelineno-384-37"></a>&gt;&gt;&gt; x = torch.randn(3, 3, 5, 5, 2)
<a id="__codelineno-384-38" name="__codelineno-384-38" href="#__codelineno-384-38"></a>&gt;&gt;&gt; y = torch.fft(x, 2)
<a id="__codelineno-384-39" name="__codelineno-384-39" href="#__codelineno-384-39"></a>&gt;&gt;&gt; y.shape
<a id="__codelineno-384-40" name="__codelineno-384-40" href="#__codelineno-384-40"></a>torch.Size([3, 3, 5, 5, 2])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-385-1" name="__codelineno-385-1" href="#__codelineno-385-1"></a>torch.ifft(input, signal_ndim, normalized=False) → Tensor¶
</code></pre></div>
<p>复数到逆离散傅立叶逆变换</p>
<p>此方法计算复杂到复杂的逆离散傅里叶变换。 忽略批次尺寸，它将计算以下表达式：</p>
<p><img alt="" src="../img/5040603503ecec3ee3de71bfe027769c.jpg" /></p>
<p>where <img alt="" src="../img/95d07e221a9b945feb93422a44ac3d42.jpg" /> = <code>signal_ndim</code> is number of dimensions for the signal, and <img alt="" src="../img/8d61ccc06c1a60ca6e3c19f12aee4f33.jpg" /> is the size of signal dimension <img alt="" src="../img/db26d1a59be5965889bd4d5533b7be61.jpg" />.</p>
<p>参数规范几乎与 <a href="#torch.fft" title="torch.fft"><code>fft()</code></a> 相同。 但是，如果将<code>normalized</code>设置为<code>True</code>，它将返回结果乘以<img alt="" src="../img/77177756d6e74e1e1a97289dfeea062c.jpg" />的结果，从而成为一元运算符。 因此，要反转 <a href="#torch.fft" title="torch.fft"><code>fft()</code></a> ，应为 <a href="#torch.fft" title="torch.fft"><code>fft()</code></a> 设置<code>normalized</code>自变量。</p>
<p>Returns the real and the imaginary parts together as one tensor of the same shape of <code>input</code>.</p>
<p>此函数的反函数为 <a href="#torch.fft" title="torch.fft"><code>fft()</code></a> 。</p>
<p>Note</p>
<p>For CUDA tensors, an LRU cache is used for cuFFT plans to speed up repeatedly running FFT methods on tensors of same geometry with same configuration. See <a href="notes/cuda.html#cufft-plan-cache">cuFFT plan cache</a> for more details on how to monitor and control the cache.</p>
<p>Warning</p>
<p>For CPU tensors, this method is currently only available with MKL. Use <code>torch.backends.mkl.is_available()</code> to check if MKL is installed.</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor of at least <code>signal_ndim</code> <code>+ 1</code> dimensions</p>
</li>
<li>
<p><strong>signal_ndim</strong> (<em>python:int</em>) – the number of dimensions in each signal. <code>signal_ndim</code> can only be 1, 2 or 3</p>
</li>
<li>
<p><strong>normalized</strong> (<em>bool__,</em> <em>optional</em>) – controls whether to return normalized results. Default: <code>False</code></p>
</li>
</ul>
<p>Returns</p>
<p>包含复数到复数傅立叶逆变换结果的张量</p>
<p>Return type</p>
<p><a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-386-1" name="__codelineno-386-1" href="#__codelineno-386-1"></a>&gt;&gt;&gt; x = torch.randn(3, 3, 2)
<a id="__codelineno-386-2" name="__codelineno-386-2" href="#__codelineno-386-2"></a>&gt;&gt;&gt; x
<a id="__codelineno-386-3" name="__codelineno-386-3" href="#__codelineno-386-3"></a>tensor([[[ 1.2766,  1.3680],
<a id="__codelineno-386-4" name="__codelineno-386-4" href="#__codelineno-386-4"></a>         [-0.8337,  2.0251],
<a id="__codelineno-386-5" name="__codelineno-386-5" href="#__codelineno-386-5"></a>         [ 0.9465, -1.4390]],
<a id="__codelineno-386-6" name="__codelineno-386-6" href="#__codelineno-386-6"></a>
<a id="__codelineno-386-7" name="__codelineno-386-7" href="#__codelineno-386-7"></a>        [[-0.1890,  1.6010],
<a id="__codelineno-386-8" name="__codelineno-386-8" href="#__codelineno-386-8"></a>         [ 1.1034, -1.9230],
<a id="__codelineno-386-9" name="__codelineno-386-9" href="#__codelineno-386-9"></a>         [-0.9482,  1.0775]],
<a id="__codelineno-386-10" name="__codelineno-386-10" href="#__codelineno-386-10"></a>
<a id="__codelineno-386-11" name="__codelineno-386-11" href="#__codelineno-386-11"></a>        [[-0.7708, -0.8176],
<a id="__codelineno-386-12" name="__codelineno-386-12" href="#__codelineno-386-12"></a>         [-0.1843, -0.2287],
<a id="__codelineno-386-13" name="__codelineno-386-13" href="#__codelineno-386-13"></a>         [-1.9034, -0.2196]]])
<a id="__codelineno-386-14" name="__codelineno-386-14" href="#__codelineno-386-14"></a>&gt;&gt;&gt; y = torch.fft(x, 2)
<a id="__codelineno-386-15" name="__codelineno-386-15" href="#__codelineno-386-15"></a>&gt;&gt;&gt; torch.ifft(y, 2)  # recover x
<a id="__codelineno-386-16" name="__codelineno-386-16" href="#__codelineno-386-16"></a>tensor([[[ 1.2766,  1.3680],
<a id="__codelineno-386-17" name="__codelineno-386-17" href="#__codelineno-386-17"></a>         [-0.8337,  2.0251],
<a id="__codelineno-386-18" name="__codelineno-386-18" href="#__codelineno-386-18"></a>         [ 0.9465, -1.4390]],
<a id="__codelineno-386-19" name="__codelineno-386-19" href="#__codelineno-386-19"></a>
<a id="__codelineno-386-20" name="__codelineno-386-20" href="#__codelineno-386-20"></a>        [[-0.1890,  1.6010],
<a id="__codelineno-386-21" name="__codelineno-386-21" href="#__codelineno-386-21"></a>         [ 1.1034, -1.9230],
<a id="__codelineno-386-22" name="__codelineno-386-22" href="#__codelineno-386-22"></a>         [-0.9482,  1.0775]],
<a id="__codelineno-386-23" name="__codelineno-386-23" href="#__codelineno-386-23"></a>
<a id="__codelineno-386-24" name="__codelineno-386-24" href="#__codelineno-386-24"></a>        [[-0.7708, -0.8176],
<a id="__codelineno-386-25" name="__codelineno-386-25" href="#__codelineno-386-25"></a>         [-0.1843, -0.2287],
<a id="__codelineno-386-26" name="__codelineno-386-26" href="#__codelineno-386-26"></a>         [-1.9034, -0.2196]]])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-387-1" name="__codelineno-387-1" href="#__codelineno-387-1"></a>torch.rfft(input, signal_ndim, normalized=False, onesided=True) → Tensor¶
</code></pre></div>
<p>实数到复杂离散傅里叶变换</p>
<p>此方法计算实数到复杂的离散傅里叶变换。 它在数学上等效于 <a href="#torch.fft" title="torch.fft"><code>fft()</code></a> ，只是输入和输出格式不同。</p>
<p>此方法支持 1D，2D 和 3D 实数到复杂的变换，由<code>signal_ndim</code>指示。 <code>input</code>必须是至少具有<code>signal_ndim</code>尺寸且可以选择任意数量的前导批尺寸的张量。 如果<code>normalized</code>设置为<code>True</code>，则通过将结果除以<img alt="" src="../img/ed8d1a6498daf3d5b9174f9c1535b2bd.jpg" />来标准化结果，以使运算符为 the，其中<img alt="" src="../img/8d61ccc06c1a60ca6e3c19f12aee4f33.jpg" />是信号维度<img alt="" src="../img/db26d1a59be5965889bd4d5533b7be61.jpg" />的大小。</p>
<p>实数到复杂的傅立叶变换结果遵循共轭对称性：</p>
<p><img alt="" src="../img/eacabf6004703215e86dc3522380c6ed.jpg" /></p>
<p>其中索引算术是计算模量的相应尺寸的大小，<img alt="" src="../img/f1deebf6650afaf555a88b62fc3992f0.jpg" />是共轭算子，<img alt="" src="../img/95d07e221a9b945feb93422a44ac3d42.jpg" /> = <code>signal_ndim</code>。 <code>onesided</code>标志控制是否避免输出结果中的冗余。 如果设置为<code>True</code>(默认值），输出将不是形状<img alt="" src="../img/f6dde7278b3a3d1ca525c45b7ad2b155.jpg" />的完全复杂结果，其中<img alt="" src="../img/dcef98688866c0d5a21137cf53bf228d.jpg" />是<code>input</code>的形状，但是最后一个尺寸将是<img alt="" src="../img/ab88558f86cd940a2c036983b4a17bc0.jpg" />尺寸的一半 。</p>
<p>此函数的反函数为 <a href="#torch.irfft" title="torch.irfft"><code>irfft()</code></a> 。</p>
<p>Note</p>
<p>For CUDA tensors, an LRU cache is used for cuFFT plans to speed up repeatedly running FFT methods on tensors of same geometry with same configuration. See <a href="notes/cuda.html#cufft-plan-cache">cuFFT plan cache</a> for more details on how to monitor and control the cache.</p>
<p>Warning</p>
<p>For CPU tensors, this method is currently only available with MKL. Use <code>torch.backends.mkl.is_available()</code> to check if MKL is installed.</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>输入</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–至少<code>signal_ndim</code>尺寸的输入张量</p>
</li>
<li>
<p><strong>signal_ndim</strong> (<em>python:int</em>) – the number of dimensions in each signal. <code>signal_ndim</code> can only be 1, 2 or 3</p>
</li>
<li>
<p><strong>normalized</strong> (<em>bool__,</em> <em>optional</em>) – controls whether to return normalized results. Default: <code>False</code></p>
</li>
<li>
<p><strong>单面</strong> (<em>bool</em> <em>，</em> <em>可选</em>）–控制是否返回一半结果以避免重复。 默认值：<code>True</code></p>
</li>
</ul>
<p>Returns</p>
<p>包含实数到复数傅立叶变换结果的张量</p>
<p>Return type</p>
<p><a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-388-1" name="__codelineno-388-1" href="#__codelineno-388-1"></a>&gt;&gt;&gt; x = torch.randn(5, 5)
<a id="__codelineno-388-2" name="__codelineno-388-2" href="#__codelineno-388-2"></a>&gt;&gt;&gt; torch.rfft(x, 2).shape
<a id="__codelineno-388-3" name="__codelineno-388-3" href="#__codelineno-388-3"></a>torch.Size([5, 3, 2])
<a id="__codelineno-388-4" name="__codelineno-388-4" href="#__codelineno-388-4"></a>&gt;&gt;&gt; torch.rfft(x, 2, onesided=False).shape
<a id="__codelineno-388-5" name="__codelineno-388-5" href="#__codelineno-388-5"></a>torch.Size([5, 5, 2])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-389-1" name="__codelineno-389-1" href="#__codelineno-389-1"></a>torch.irfft(input, signal_ndim, normalized=False, onesided=True, signal_sizes=None) → Tensor¶
</code></pre></div>
<p>复数到实数离散傅里叶逆变换</p>
<p>此方法计算复数到实数的离散傅里叶逆变换。 它在数学上等效于 <a href="#torch.ifft" title="torch.ifft"><code>ifft()</code></a> ，只是输入和输出格式不同。</p>
<p>参数规范几乎与 <a href="#torch.ifft" title="torch.ifft"><code>ifft()</code></a> 相同。 与 <a href="#torch.ifft" title="torch.ifft"><code>ifft()</code></a> 相似，如果<code>normalized</code>设置为<code>True</code>，则通过将结果与<img alt="" src="../img/ed8d1a6498daf3d5b9174f9c1535b2bd.jpg" />相乘来对结果进行归一化，以使运算符为 ary，其中<img alt="" src="../img/8d61ccc06c1a60ca6e3c19f12aee4f33.jpg" />是信号的大小 尺寸<img alt="" src="../img/db26d1a59be5965889bd4d5533b7be61.jpg" />。</p>
<p>Note</p>
<p>由于共轭对称性，<code>input</code>不需要包含完整的复数频率值。 大约一半的值就足够了，就像 <a href="#torch.rfft" title="torch.rfft"><code>rfft()</code></a> 和<code>rfft(signal, onesided=True)</code>给出<code>input</code>的情况一样。 在这种情况下，请将此方法的<code>onesided</code>参数设置为<code>True</code>。 此外，原始信号形状信息有时会丢失，可以选择将<code>signal_sizes</code>设置为原始信号的大小(如果处于批处理模式，则没有批处理尺寸），以恢复正确的形状。</p>
<p>因此，要反转 <a href="#torch.rfft" title="torch.rfft"><code>rfft()</code></a> ，应为 <a href="#torch.irfft" title="torch.irfft"><code>irfft()</code></a> 设置<code>normalized</code>和<code>onesided</code>自变量，并且最好使用<code>signal_sizes</code>以避免大小 不匹配。 有关大小不匹配的情况，请参见下面的示例。</p>
<p>有关共轭对称性的详细信息，请参见 <a href="#torch.rfft" title="torch.rfft"><code>rfft()</code></a> 。</p>
<p>此函数的反函数为 <a href="#torch.rfft" title="torch.rfft"><code>rfft()</code></a> 。</p>
<p>Warning</p>
<p>通常，此函数的输入应包含遵循共轭对称性的值。 请注意，即使<code>onesided</code>为<code>True</code>，仍然经常需要在某些部分上保持对称。 当不满足此要求时， <a href="#torch.irfft" title="torch.irfft"><code>irfft()</code></a> 的行为是不确定的。 由于 <a href="autograd.html#torch.autograd.gradcheck" title="torch.autograd.gradcheck"><code>torch.autograd.gradcheck()</code></a> 估计带有点扰动的数字雅可比行列式，因此 <a href="#torch.irfft" title="torch.irfft"><code>irfft()</code></a> 几乎肯定会失败。</p>
<p>Note</p>
<p>For CUDA tensors, an LRU cache is used for cuFFT plans to speed up repeatedly running FFT methods on tensors of same geometry with same configuration. See <a href="notes/cuda.html#cufft-plan-cache">cuFFT plan cache</a> for more details on how to monitor and control the cache.</p>
<p>Warning</p>
<p>For CPU tensors, this method is currently only available with MKL. Use <code>torch.backends.mkl.is_available()</code> to check if MKL is installed.</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor of at least <code>signal_ndim</code> <code>+ 1</code> dimensions</p>
</li>
<li>
<p><strong>signal_ndim</strong> (<em>python:int</em>) – the number of dimensions in each signal. <code>signal_ndim</code> can only be 1, 2 or 3</p>
</li>
<li>
<p><strong>normalized</strong> (<em>bool__,</em> <em>optional</em>) – controls whether to return normalized results. Default: <code>False</code></p>
</li>
<li>
<p><strong>单面</strong> (<em>bool</em> <em>，</em> <em>可选</em>）–控制<code>input</code>是否对半以避免冗余，例如通过 <a href="#torch.rfft" title="torch.rfft"><code>rfft()</code></a> 。 默认值：<code>True</code></p>
</li>
<li>
<p><strong>signal_sizes</strong> (列表或<code>torch.Size</code>，可选）–原始信号的大小(无批次尺寸）。 默认值：<code>None</code></p>
</li>
</ul>
<p>Returns</p>
<p>包含复数到实数傅立叶逆变换结果的张量</p>
<p>Return type</p>
<p><a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-390-1" name="__codelineno-390-1" href="#__codelineno-390-1"></a>&gt;&gt;&gt; x = torch.randn(4, 4)
<a id="__codelineno-390-2" name="__codelineno-390-2" href="#__codelineno-390-2"></a>&gt;&gt;&gt; torch.rfft(x, 2, onesided=True).shape
<a id="__codelineno-390-3" name="__codelineno-390-3" href="#__codelineno-390-3"></a>torch.Size([4, 3, 2])
<a id="__codelineno-390-4" name="__codelineno-390-4" href="#__codelineno-390-4"></a>&gt;&gt;&gt;
<a id="__codelineno-390-5" name="__codelineno-390-5" href="#__codelineno-390-5"></a>&gt;&gt;&gt; # notice that with onesided=True, output size does not determine the original signal size
<a id="__codelineno-390-6" name="__codelineno-390-6" href="#__codelineno-390-6"></a>&gt;&gt;&gt; x = torch.randn(4, 5)
<a id="__codelineno-390-7" name="__codelineno-390-7" href="#__codelineno-390-7"></a>
<a id="__codelineno-390-8" name="__codelineno-390-8" href="#__codelineno-390-8"></a>&gt;&gt;&gt; torch.rfft(x, 2, onesided=True).shape
<a id="__codelineno-390-9" name="__codelineno-390-9" href="#__codelineno-390-9"></a>torch.Size([4, 3, 2])
<a id="__codelineno-390-10" name="__codelineno-390-10" href="#__codelineno-390-10"></a>&gt;&gt;&gt;
<a id="__codelineno-390-11" name="__codelineno-390-11" href="#__codelineno-390-11"></a>&gt;&gt;&gt; # now we use the original shape to recover x
<a id="__codelineno-390-12" name="__codelineno-390-12" href="#__codelineno-390-12"></a>&gt;&gt;&gt; x
<a id="__codelineno-390-13" name="__codelineno-390-13" href="#__codelineno-390-13"></a>tensor([[-0.8992,  0.6117, -1.6091, -0.4155, -0.8346],
<a id="__codelineno-390-14" name="__codelineno-390-14" href="#__codelineno-390-14"></a>        [-2.1596, -0.0853,  0.7232,  0.1941, -0.0789],
<a id="__codelineno-390-15" name="__codelineno-390-15" href="#__codelineno-390-15"></a>        [-2.0329,  1.1031,  0.6869, -0.5042,  0.9895],
<a id="__codelineno-390-16" name="__codelineno-390-16" href="#__codelineno-390-16"></a>        [-0.1884,  0.2858, -1.5831,  0.9917, -0.8356]])
<a id="__codelineno-390-17" name="__codelineno-390-17" href="#__codelineno-390-17"></a>&gt;&gt;&gt; y = torch.rfft(x, 2, onesided=True)
<a id="__codelineno-390-18" name="__codelineno-390-18" href="#__codelineno-390-18"></a>&gt;&gt;&gt; torch.irfft(y, 2, onesided=True, signal_sizes=x.shape)  # recover x
<a id="__codelineno-390-19" name="__codelineno-390-19" href="#__codelineno-390-19"></a>tensor([[-0.8992,  0.6117, -1.6091, -0.4155, -0.8346],
<a id="__codelineno-390-20" name="__codelineno-390-20" href="#__codelineno-390-20"></a>        [-2.1596, -0.0853,  0.7232,  0.1941, -0.0789],
<a id="__codelineno-390-21" name="__codelineno-390-21" href="#__codelineno-390-21"></a>        [-2.0329,  1.1031,  0.6869, -0.5042,  0.9895],
<a id="__codelineno-390-22" name="__codelineno-390-22" href="#__codelineno-390-22"></a>        [-0.1884,  0.2858, -1.5831,  0.9917, -0.8356]])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-391-1" name="__codelineno-391-1" href="#__codelineno-391-1"></a>torch.stft(input, n_fft, hop_length=None, win_length=None, window=None, center=True, pad_mode=&#39;reflect&#39;, normalized=False, onesided=True)¶
</code></pre></div>
<p>短时傅立叶变换(STFT）。</p>
<p>忽略可选的批处理维，此方法将计算以下表达式：</p>
<p><img alt="" src="../img/33a339c5713e9db708c9eed0a7969d63.jpg" /></p>
<p>其中<img alt="" src="../img/03327e7b697db30918e96b2209927929.jpg" />是滑动窗口的索引，<img alt="" src="../img/47bff0d0b1fe99458172eae026405291.jpg" />是<img alt="" src="../img/c913af4e6c1176b6e9846f5f85011461.jpg" />的频率。 当<code>onesided</code>为默认值<code>True</code>时，</p>
<ul>
<li>
<p><code>input</code>必须是一维时间序列或二维时间序列批次。</p>
</li>
<li>
<p>如果<code>hop_length</code>为<code>None</code>(默认值），则将其视为等于<code>floor(n_fft / 4)</code>。</p>
</li>
<li>
<p>如果<code>win_length</code>为<code>None</code>(默认值），则将其视为等于<code>n_fft</code>。</p>
</li>
<li>
<p><code>window</code>可以是大小为<code>win_length</code>的一维张量，例如来自 <a href="#torch.hann_window" title="torch.hann_window"><code>torch.hann_window()</code></a> 。 如果<code>window</code>为<code>None</code>(默认），则将其视为窗口中到处都有<img alt="" src="../img/21c5bc8d40ee5ab2e18d64aaba8359c7.jpg" />。 如果使用<img alt="" src="../img/3579be09fb723df705e009678d545d64.jpg" />，则将<code>window</code>的两面填充长度<code>n_fft</code>，然后再进行应用。</p>
</li>
<li>
<p>如果<code>center</code>为<code>True</code>(默认值），则将在两边都填充<code>input</code>，以使第<img alt="" src="../img/ea198b23d3d3f891bcc7684b8659b3e4.jpg" />帧位于时间<img alt="" src="../img/33e0acbd6c2b37b3ef3123f4dc18a1c0.jpg" />的中心。 否则，第<img alt="" src="../img/ea198b23d3d3f891bcc7684b8659b3e4.jpg" />帧在时间<img alt="" src="../img/33e0acbd6c2b37b3ef3123f4dc18a1c0.jpg" />开始。</p>
</li>
<li>
<p><code>pad_mode</code>确定当<code>center</code>为<code>True</code>时在<code>input</code>上使用的填充方法。 有关所有可用选项，请参见 <a href="nn.functional.html#torch.nn.functional.pad" title="torch.nn.functional.pad"><code>torch.nn.functional.pad()</code></a> 。 默认值为<code>"reflect"</code>。</p>
</li>
<li>
<p>如果<code>onesided</code>为<code>True</code>(默认值），则仅返回<img alt="" src="../img/c15755e95a374ed10ce009af8ac323f5.jpg" />中<img alt="" src="../img/47bff0d0b1fe99458172eae026405291.jpg" />的值，因为实数到复数傅里叶变换满足共轭对称性，即<img alt="" src="../img/da5553744a960ad6822ef967ba0ea46f.jpg" />。</p>
</li>
<li>
<p>如果<code>normalized</code>为<code>True</code>(默认为<code>False</code>），则该函数返回归一化的 STFT 结果，即乘以<img alt="" src="../img/3a83bac3c29283a2eb3fcc60e15f37aa.jpg" />。</p>
</li>
</ul>
<p>将实部和虚部一起返回为一个大小为<img alt="" src="../img/1d33f2bcea40c743acb3c7e78a69ca62.jpg" />的张量，其中<img alt="" src="../img/dcef98688866c0d5a21137cf53bf228d.jpg" />是可选的<code>input</code>批大小，<img alt="" src="../img/7ea0d6aa290d37c8bc57957206bafe80.jpg" />是应用 STFT 的频率数，<img alt="" src="../img/e76349865309323eafcc6a541e3073ae.jpg" />是总数 所使用的帧数，最后一维中的每一对代表一个复数，作为实部和虚部。</p>
<p>Warning</p>
<p>此功能在版本 0.4.1 更改了签名。 使用前一个签名进行调用可能会导致错误或返回错误的结果。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor</p>
</li>
<li>
<p><strong>n_fft</strong>  (<em>python：int</em> )–傅立叶变换的大小</p>
</li>
<li>
<p><strong>hop_length</strong>  (<em>python：int</em> <em>，</em> <em>可选</em>）–相邻滑动窗口框架之间的距离。 默认值：<code>None</code>(等同于<code>floor(n_fft / 4)</code>）</p>
</li>
<li>
<p><strong>win_length</strong>  (<em>python：int</em> <em>，</em> <em>可选</em>）–窗口框架和 STFT 过滤器的大小。 默认值：<code>None</code>(等同于<code>n_fft</code>）</p>
</li>
<li>
<p><strong>窗口</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a> <em>，</em> <em>可选</em>）–可选窗口功能。 默认值：<code>None</code>(被视为所有<img alt="" src="../img/21c5bc8d40ee5ab2e18d64aaba8359c7.jpg" />的窗口）</p>
</li>
<li>
<p><strong>中心</strong> (<em>bool</em> <em>，</em> <em>可选</em>）–是否在两侧都填充<code>input</code>以便第<img alt="" src="../img/ea198b23d3d3f891bcc7684b8659b3e4.jpg" />个帧位于 集中在时间<img alt="" src="../img/33e0acbd6c2b37b3ef3123f4dc18a1c0.jpg" />上。 默认值：<code>True</code></p>
</li>
<li>
<p><strong>pad_mode</strong> (<em>字符串</em> <em>，</em> <em>可选</em>）–控制当<code>center</code>为<code>True</code>时使用的填充方法。 默认值：<code>"reflect"</code></p>
</li>
<li>
<p><strong>规范化的</strong> (<em>bool</em> <em>，</em> <em>可选</em>）–控制是否返回规范化的 STFT 结果默认：<code>False</code></p>
</li>
<li>
<p><strong>单面</strong> (<em>bool</em> <em>，</em> <em>可选</em>）–控制是否返回一半结果以避免冗余默认值：<code>True</code></p>
</li>
</ul>
<p>Returns</p>
<p>包含 STFT 结果的张量具有上述形状</p>
<p>Return type</p>
<p><a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-392-1" name="__codelineno-392-1" href="#__codelineno-392-1"></a>torch.bartlett_window(window_length, periodic=True, dtype=None, layout=torch.strided, device=None, requires_grad=False) → Tensor¶
</code></pre></div>
<p>Bartlett 窗口功能。</p>
<p><img alt="" src="../img/7a8f3ed9f6703292e22f8787927206ae.jpg" /></p>
<p>其中<img alt="" src="../img/7ea0d6aa290d37c8bc57957206bafe80.jpg" />是整个窗口的大小。</p>
<p>输入<code>window_length</code>是控制返回的窗口大小的正整数。 <code>periodic</code>标志确定返回的窗口是否从对称窗口中修剪掉最后一个重复值，并准备好用作具有 <a href="#torch.stft" title="torch.stft"><code>torch.stft()</code></a> 之类的周期性窗口。 因此，如果<code>periodic</code>为真，则上式中的<img alt="" src="../img/7ea0d6aa290d37c8bc57957206bafe80.jpg" />实际上为<img alt="" src="../img/1cfea2e0a42e6ef5aee44b22f2bf2ab4.jpg" />。 另外，我们总是<code>torch.bartlett_window(L, periodic=True)</code>等于<code>torch.bartlett_window(L + 1, periodic=False)[:-1])</code>。</p>
<p>Note</p>
<p>如果<code>window_length</code> <img alt="" src="../img/193049f37e5b8049aa1c2f98d7bf7b2c.jpg" />，则返回的窗口包含单个值 1。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>window_length</strong>  (<em>python：int</em> )–返回窗口的大小</p>
</li>
<li>
<p><strong>周期性</strong> (<em>bool</em> <em>，</em> <em>可选</em>）–如果为 True，则返回用作周期性函数的窗口。 如果为 False，则返回一个对称窗口。</p>
</li>
<li>
<p><strong>dtype</strong>  (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a> ，可选）–返回张量的所需数据类型。 默认值：如果<code>None</code>使用全局默认值(请参见 <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>)。 仅支持浮点类型。</p>
</li>
<li>
<p><strong>布局</strong> (<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code>torch.layout</code></a> ，可选）–返回的窗口张量的所需布局。 仅支持<code>torch.strided</code>(密集布局）。</p>
</li>
<li>
<p><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) – the desired device of returned tensor. Default: if <code>None</code>, uses the current device for the default tensor type (see <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>). <code>device</code> will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.</p>
</li>
<li>
<p><strong>requires_grad</strong> (<em>bool__,</em> <em>optional</em>) – If autograd should record operations on the returned tensor. Default: <code>False</code>.</p>
</li>
</ul>
<p>Returns</p>
<p>包含窗口的大小为<img alt="" src="../img/ba92c6cf7c07822f0e23d550a6cf1108.jpg" />的一维张量</p>
<p>Return type</p>
<p><a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-393-1" name="__codelineno-393-1" href="#__codelineno-393-1"></a>torch.blackman_window(window_length, periodic=True, dtype=None, layout=torch.strided, device=None, requires_grad=False) → Tensor¶
</code></pre></div>
<p>布莱克曼窗口功能。</p>
<p><img alt="" src="../img/ee8023c13513826303e160aa8135b175.jpg" /></p>
<p>where <img alt="" src="../img/7ea0d6aa290d37c8bc57957206bafe80.jpg" /> is the full window size.</p>
<p>输入<code>window_length</code>是控制返回的窗口大小的正整数。 <code>periodic</code>标志确定返回的窗口是否从对称窗口中修剪掉最后一个重复值，并准备好用作具有 <a href="#torch.stft" title="torch.stft"><code>torch.stft()</code></a> 之类的周期性窗口。 因此，如果<code>periodic</code>为真，则上式中的<img alt="" src="../img/7ea0d6aa290d37c8bc57957206bafe80.jpg" />实际上为<img alt="" src="../img/1cfea2e0a42e6ef5aee44b22f2bf2ab4.jpg" />。 另外，我们总是<code>torch.blackman_window(L, periodic=True)</code>等于<code>torch.blackman_window(L + 1, periodic=False)[:-1])</code>。</p>
<p>Note</p>
<p>If <code>window_length</code> <img alt="" src="../img/193049f37e5b8049aa1c2f98d7bf7b2c.jpg" />, the returned window contains a single value 1.</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>window_length</strong> (<em>python:int</em>) – the size of returned window</p>
</li>
<li>
<p><strong>periodic</strong> (<em>bool__,</em> <em>optional</em>) – If True, returns a window to be used as periodic function. If False, return a symmetric window.</p>
</li>
<li>
<p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) – the desired data type of returned tensor. Default: if <code>None</code>, uses a global default (see <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>). Only floating point types are supported.</p>
</li>
<li>
<p><strong>layout</strong> (<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code>torch.layout</code></a>, optional) – the desired layout of returned window tensor. Only <code>torch.strided</code> (dense layout) is supported.</p>
</li>
<li>
<p><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) – the desired device of returned tensor. Default: if <code>None</code>, uses the current device for the default tensor type (see <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>). <code>device</code> will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.</p>
</li>
<li>
<p><strong>requires_grad</strong> (<em>bool__,</em> <em>optional</em>) – If autograd should record operations on the returned tensor. Default: <code>False</code>.</p>
</li>
</ul>
<p>Returns</p>
<p>A 1-D tensor of size <img alt="" src="../img/ba92c6cf7c07822f0e23d550a6cf1108.jpg" /> containing the window</p>
<p>Return type</p>
<p><a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-394-1" name="__codelineno-394-1" href="#__codelineno-394-1"></a>torch.hamming_window(window_length, periodic=True, alpha=0.54, beta=0.46, dtype=None, layout=torch.strided, device=None, requires_grad=False) → Tensor¶
</code></pre></div>
<p>汉明窗功能。</p>
<p><img alt="" src="../img/5fe81b7bd5bd7c7e122962f6739b7c8e.jpg" /></p>
<p>where <img alt="" src="../img/7ea0d6aa290d37c8bc57957206bafe80.jpg" /> is the full window size.</p>
<p>输入<code>window_length</code>是控制返回的窗口大小的正整数。 <code>periodic</code>标志确定返回的窗口是否从对称窗口中修剪掉最后一个重复值，并准备好用作具有 <a href="#torch.stft" title="torch.stft"><code>torch.stft()</code></a> 之类的周期性窗口。 因此，如果<code>periodic</code>为真，则上式中的<img alt="" src="../img/7ea0d6aa290d37c8bc57957206bafe80.jpg" />实际上为<img alt="" src="../img/1cfea2e0a42e6ef5aee44b22f2bf2ab4.jpg" />。 另外，我们总是<code>torch.hamming_window(L, periodic=True)</code>等于<code>torch.hamming_window(L + 1, periodic=False)[:-1])</code>。</p>
<p>Note</p>
<p>If <code>window_length</code> <img alt="" src="../img/193049f37e5b8049aa1c2f98d7bf7b2c.jpg" />, the returned window contains a single value 1.</p>
<p>Note</p>
<p>这是 <a href="#torch.hann_window" title="torch.hann_window"><code>torch.hann_window()</code></a> 的通用版本。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>window_length</strong> (<em>python:int</em>) – the size of returned window</p>
</li>
<li>
<p><strong>periodic</strong> (<em>bool__,</em> <em>optional</em>) – If True, returns a window to be used as periodic function. If False, return a symmetric window.</p>
</li>
<li>
<p><strong>alpha</strong>  (<em>python：float</em> <em>，</em> <em>可选</em>）–上式中的系数<img alt="" src="../img/5b9866fb35b01c553ed3e738e3972ae9.jpg" /></p>
</li>
<li>
<p><strong>beta</strong>  (<em>python：float</em> <em>，</em> <em>可选</em>）–上式中的系数<img alt="" src="../img/53a496ec7d546e2af9595a7055dd6a7e.jpg" /></p>
</li>
<li>
<p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) – the desired data type of returned tensor. Default: if <code>None</code>, uses a global default (see <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>). Only floating point types are supported.</p>
</li>
<li>
<p><strong>layout</strong> (<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code>torch.layout</code></a>, optional) – the desired layout of returned window tensor. Only <code>torch.strided</code> (dense layout) is supported.</p>
</li>
<li>
<p><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) – the desired device of returned tensor. Default: if <code>None</code>, uses the current device for the default tensor type (see <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>). <code>device</code> will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.</p>
</li>
<li>
<p><strong>requires_grad</strong> (<em>bool__,</em> <em>optional</em>) – If autograd should record operations on the returned tensor. Default: <code>False</code>.</p>
</li>
</ul>
<p>Returns</p>
<p>A 1-D tensor of size <img alt="" src="../img/ba92c6cf7c07822f0e23d550a6cf1108.jpg" /> containing the window</p>
<p>Return type</p>
<p><a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-395-1" name="__codelineno-395-1" href="#__codelineno-395-1"></a>torch.hann_window(window_length, periodic=True, dtype=None, layout=torch.strided, device=None, requires_grad=False) → Tensor¶
</code></pre></div>
<p>汉恩窗口功能。</p>
<p><img alt="" src="../img/05f672670a77d8bc009f743a19461201.jpg" /></p>
<p>where <img alt="" src="../img/7ea0d6aa290d37c8bc57957206bafe80.jpg" /> is the full window size.</p>
<p>输入<code>window_length</code>是控制返回的窗口大小的正整数。 <code>periodic</code>标志确定返回的窗口是否从对称窗口中修剪掉最后一个重复值，并准备好用作具有 <a href="#torch.stft" title="torch.stft"><code>torch.stft()</code></a> 之类的周期性窗口。 因此，如果<code>periodic</code>为真，则上式中的<img alt="" src="../img/7ea0d6aa290d37c8bc57957206bafe80.jpg" />实际上为<img alt="" src="../img/1cfea2e0a42e6ef5aee44b22f2bf2ab4.jpg" />。 另外，我们总是<code>torch.hann_window(L, periodic=True)</code>等于<code>torch.hann_window(L + 1, periodic=False)[:-1])</code>。</p>
<p>Note</p>
<p>If <code>window_length</code> <img alt="" src="../img/193049f37e5b8049aa1c2f98d7bf7b2c.jpg" />, the returned window contains a single value 1.</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>window_length</strong> (<em>python:int</em>) – the size of returned window</p>
</li>
<li>
<p><strong>periodic</strong> (<em>bool__,</em> <em>optional</em>) – If True, returns a window to be used as periodic function. If False, return a symmetric window.</p>
</li>
<li>
<p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) – the desired data type of returned tensor. Default: if <code>None</code>, uses a global default (see <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>). Only floating point types are supported.</p>
</li>
<li>
<p><strong>layout</strong> (<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code>torch.layout</code></a>, optional) – the desired layout of returned window tensor. Only <code>torch.strided</code> (dense layout) is supported.</p>
</li>
<li>
<p><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) – the desired device of returned tensor. Default: if <code>None</code>, uses the current device for the default tensor type (see <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>). <code>device</code> will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.</p>
</li>
<li>
<p><strong>requires_grad</strong> (<em>bool__,</em> <em>optional</em>) – If autograd should record operations on the returned tensor. Default: <code>False</code>.</p>
</li>
</ul>
<p>Returns</p>
<p>A 1-D tensor of size <img alt="" src="../img/ba92c6cf7c07822f0e23d550a6cf1108.jpg" /> containing the window</p>
<p>Return type</p>
<p><a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
<h3 id="_16">其他作业</h3>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-396-1" name="__codelineno-396-1" href="#__codelineno-396-1"></a>torch.bincount(input, weights=None, minlength=0) → Tensor¶
</code></pre></div>
<p>计算非负整数数组中每个值的频率。</p>
<p>除非<code>input</code>为空，否则 bin 的数量(大小 1）比<code>input</code>中的最大值大一个，在这种情况下，结果是大小为 0 的张量。如果指定<code>minlength</code>，则 bin 的数量为 至少<code>minlength</code>为空，如果<code>input</code>为空，则结果为大小为<code>minlength</code>的张量填充零。 如果<code>n</code>是位置<code>i</code>的值，则如果指定了<code>weights</code>则为<code>out[n] += weights[i]</code>，否则指定<code>out[n] += 1</code>。</p>
<p>Note</p>
<p>使用 CUDA 后端时，此操作可能会导致不确定的行为，不容易关闭。 有关背景，请参见<a href="notes/randomness.html">重现性</a>的注释。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>输入</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)– 1-d int 张量</p>
</li>
<li>
<p><strong>权重</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–可选，输入张量中每个值的权重。 应具有与输入张量相同的大小。</p>
</li>
<li>
<p><strong>minlength</strong>  (<em>python：int</em> )–可选，最小存储箱数。 应该是非负的。</p>
</li>
</ul>
<p>Returns</p>
<p>如果<code>input</code>为非空，则为<code>Size([max(input) + 1])</code>形状的张量，否则为<code>Size(0)</code></p>
<p>Return type</p>
<p>输出(<a href="tensors.html#torch.Tensor" title="torch.Tensor">张量</a>）</p>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-397-1" name="__codelineno-397-1" href="#__codelineno-397-1"></a>&gt;&gt;&gt; input = torch.randint(0, 8, (5,), dtype=torch.int64)
<a id="__codelineno-397-2" name="__codelineno-397-2" href="#__codelineno-397-2"></a>&gt;&gt;&gt; weights = torch.linspace(0, 1, steps=5)
<a id="__codelineno-397-3" name="__codelineno-397-3" href="#__codelineno-397-3"></a>&gt;&gt;&gt; input, weights
<a id="__codelineno-397-4" name="__codelineno-397-4" href="#__codelineno-397-4"></a>(tensor([4, 3, 6, 3, 4]),
<a id="__codelineno-397-5" name="__codelineno-397-5" href="#__codelineno-397-5"></a> tensor([ 0.0000,  0.2500,  0.5000,  0.7500,  1.0000])
<a id="__codelineno-397-6" name="__codelineno-397-6" href="#__codelineno-397-6"></a>
<a id="__codelineno-397-7" name="__codelineno-397-7" href="#__codelineno-397-7"></a>&gt;&gt;&gt; torch.bincount(input)
<a id="__codelineno-397-8" name="__codelineno-397-8" href="#__codelineno-397-8"></a>tensor([0, 0, 0, 2, 2, 0, 1])
<a id="__codelineno-397-9" name="__codelineno-397-9" href="#__codelineno-397-9"></a>
<a id="__codelineno-397-10" name="__codelineno-397-10" href="#__codelineno-397-10"></a>&gt;&gt;&gt; input.bincount(weights)
<a id="__codelineno-397-11" name="__codelineno-397-11" href="#__codelineno-397-11"></a>tensor([0.0000, 0.0000, 0.0000, 1.0000, 1.0000, 0.0000, 0.5000])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-398-1" name="__codelineno-398-1" href="#__codelineno-398-1"></a>torch.broadcast_tensors(*tensors) → List of Tensors¶
</code></pre></div>
<p>根据<a href="notes/broadcasting.html#broadcasting-semantics">广播语义</a>广播给定张量。</p>
<p>Parameters</p>
<p><strong>*张量</strong> –任意数量的相同类型的张量</p>
<p>Warning</p>
<p>广播张量的一个以上元素可以引用单个存储位置。 结果，就地操作(尤其是矢量化的操作）可能会导致错误的行为。 如果需要写张量，请先克隆它们。</p>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-399-1" name="__codelineno-399-1" href="#__codelineno-399-1"></a>&gt;&gt;&gt; x = torch.arange(3).view(1, 3)
<a id="__codelineno-399-2" name="__codelineno-399-2" href="#__codelineno-399-2"></a>&gt;&gt;&gt; y = torch.arange(2).view(2, 1)
<a id="__codelineno-399-3" name="__codelineno-399-3" href="#__codelineno-399-3"></a>&gt;&gt;&gt; a, b = torch.broadcast_tensors(x, y)
<a id="__codelineno-399-4" name="__codelineno-399-4" href="#__codelineno-399-4"></a>&gt;&gt;&gt; a.size()
<a id="__codelineno-399-5" name="__codelineno-399-5" href="#__codelineno-399-5"></a>torch.Size([2, 3])
<a id="__codelineno-399-6" name="__codelineno-399-6" href="#__codelineno-399-6"></a>&gt;&gt;&gt; a
<a id="__codelineno-399-7" name="__codelineno-399-7" href="#__codelineno-399-7"></a>tensor([[0, 1, 2],
<a id="__codelineno-399-8" name="__codelineno-399-8" href="#__codelineno-399-8"></a>        [0, 1, 2]])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-400-1" name="__codelineno-400-1" href="#__codelineno-400-1"></a>torch.cartesian_prod(*tensors)¶
</code></pre></div>
<p>给定张量序列的笛卡尔积。 行为类似于 python 的 &lt;cite&gt;itertools.product&lt;/cite&gt; 。</p>
<p>Parameters</p>
<p><strong>*张量</strong> –任意数量的一维张量。</p>
<p>Returns</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-401-1" name="__codelineno-401-1" href="#__codelineno-401-1"></a>A tensor equivalent to converting all the input tensors into lists,
</code></pre></div>
<p>在这些列表上执行 &lt;cite&gt;itertools.product&lt;/cite&gt; ，最后将结果列表转换为张量。</p>
<p>Return type</p>
<p><a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-402-1" name="__codelineno-402-1" href="#__codelineno-402-1"></a>&gt;&gt;&gt; a = [1, 2, 3]
<a id="__codelineno-402-2" name="__codelineno-402-2" href="#__codelineno-402-2"></a>&gt;&gt;&gt; b = [4, 5]
<a id="__codelineno-402-3" name="__codelineno-402-3" href="#__codelineno-402-3"></a>&gt;&gt;&gt; list(itertools.product(a, b))
<a id="__codelineno-402-4" name="__codelineno-402-4" href="#__codelineno-402-4"></a>[(1, 4), (1, 5), (2, 4), (2, 5), (3, 4), (3, 5)]
<a id="__codelineno-402-5" name="__codelineno-402-5" href="#__codelineno-402-5"></a>&gt;&gt;&gt; tensor_a = torch.tensor(a)
<a id="__codelineno-402-6" name="__codelineno-402-6" href="#__codelineno-402-6"></a>&gt;&gt;&gt; tensor_b = torch.tensor(b)
<a id="__codelineno-402-7" name="__codelineno-402-7" href="#__codelineno-402-7"></a>&gt;&gt;&gt; torch.cartesian_prod(tensor_a, tensor_b)
<a id="__codelineno-402-8" name="__codelineno-402-8" href="#__codelineno-402-8"></a>tensor([[1, 4],
<a id="__codelineno-402-9" name="__codelineno-402-9" href="#__codelineno-402-9"></a>        [1, 5],
<a id="__codelineno-402-10" name="__codelineno-402-10" href="#__codelineno-402-10"></a>        [2, 4],
<a id="__codelineno-402-11" name="__codelineno-402-11" href="#__codelineno-402-11"></a>        [2, 5],
<a id="__codelineno-402-12" name="__codelineno-402-12" href="#__codelineno-402-12"></a>        [3, 4],
<a id="__codelineno-402-13" name="__codelineno-402-13" href="#__codelineno-402-13"></a>        [3, 5]])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-403-1" name="__codelineno-403-1" href="#__codelineno-403-1"></a>torch.cdist(x1, x2, p=2, compute_mode=&#39;use_mm_for_euclid_dist_if_necessary&#39;)¶
</code></pre></div>
<p>计算批处理行向量的两个集合的每对之间的 p 范数距离。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>x1</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–形状为<img alt="" src="../img/b34d73ac200b58cfa9bc0608c6e1658a.jpg" />的输入张量。</p>
</li>
<li>
<p><strong>x2</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–形状为<img alt="" src="../img/a77a440e5ab82d234d8ad3c6f8c5dc73.jpg" />的输入张量。</p>
</li>
<li>
<p><strong>p</strong> -p 范数距离的 p 值，以计算每个向量对<img alt="" src="../img/2ed3340d16bd283f671129abab392a0b.jpg" />之间的距离。</p>
</li>
<li>
<p><strong>compute_mode</strong> –'use_mm_for_euclid_dist_if_necessary'-如果 P &gt; 25 或 R &gt; 25'use_mm_for_euclid_dist'-将始终使用矩阵乘法方法来计算欧几里德距离(p = 2） 欧式距离(p = 2）'donot_use_mm_for_euclid_dist'-永远不会使用矩阵乘法方法来计算欧式距离(p = 2）默认值：use_mm_for_euclid_dist_if_necessary。</p>
</li>
</ul>
<p>如果 x1 具有形状<img alt="" src="../img/b34d73ac200b58cfa9bc0608c6e1658a.jpg" />，而 x2 具有形状<img alt="" src="../img/a77a440e5ab82d234d8ad3c6f8c5dc73.jpg" />，则输出将具有形状<img alt="" src="../img/48db404261f2466cb86616d6f920e894.jpg" />。</p>
<p>如果<img alt="" src="../img/5323d331671967548fee2dd052f584b9.jpg" />，则此函数等效于 &lt;cite&gt;scipy.spatial.distance.cdist(input，'minkowski'，p = p）&lt;/cite&gt;。 当<img alt="" src="../img/0ed455324c64d9801ef75fe54be6b565.jpg" />等于 &lt;cite&gt;scipy.spatial.distance.cdist(input，'hamming'）* M&lt;/cite&gt; 。 当<img alt="" src="../img/2a680efba9b8e37cfffde015324eb07e.jpg" />时，最接近的 scipy 函数是 &lt;cite&gt;scipy.spatial.distance.cdist(xn，lambda x，y：np.abs(x-y）.max(））&lt;/cite&gt;。</p>
<p>Example</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-404-1" name="__codelineno-404-1" href="#__codelineno-404-1"></a>&gt;&gt;&gt; a = torch.tensor([[0.9041,  0.0196], [-0.3108, -2.4423], [-0.4821,  1.059]])
<a id="__codelineno-404-2" name="__codelineno-404-2" href="#__codelineno-404-2"></a>&gt;&gt;&gt; a
<a id="__codelineno-404-3" name="__codelineno-404-3" href="#__codelineno-404-3"></a>tensor([[ 0.9041,  0.0196],
<a id="__codelineno-404-4" name="__codelineno-404-4" href="#__codelineno-404-4"></a>        [-0.3108, -2.4423],
<a id="__codelineno-404-5" name="__codelineno-404-5" href="#__codelineno-404-5"></a>        [-0.4821,  1.0590]])
<a id="__codelineno-404-6" name="__codelineno-404-6" href="#__codelineno-404-6"></a>&gt;&gt;&gt; b = torch.tensor([[-2.1763, -0.4713], [-0.6986,  1.3702]])
<a id="__codelineno-404-7" name="__codelineno-404-7" href="#__codelineno-404-7"></a>&gt;&gt;&gt; b
<a id="__codelineno-404-8" name="__codelineno-404-8" href="#__codelineno-404-8"></a>tensor([[-2.1763, -0.4713],
<a id="__codelineno-404-9" name="__codelineno-404-9" href="#__codelineno-404-9"></a>        [-0.6986,  1.3702]])
<a id="__codelineno-404-10" name="__codelineno-404-10" href="#__codelineno-404-10"></a>&gt;&gt;&gt; torch.cdist(a, b, p=2)
<a id="__codelineno-404-11" name="__codelineno-404-11" href="#__codelineno-404-11"></a>tensor([[3.1193, 2.0959],
<a id="__codelineno-404-12" name="__codelineno-404-12" href="#__codelineno-404-12"></a>        [2.7138, 3.8322],
<a id="__codelineno-404-13" name="__codelineno-404-13" href="#__codelineno-404-13"></a>        [2.2830, 0.3791]])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-405-1" name="__codelineno-405-1" href="#__codelineno-405-1"></a>torch.combinations(input, r=2, with_replacement=False) → seq¶
</code></pre></div>
<p>计算给定张量的长度<img alt="" src="../img/a892400c6ed0045af4aedcc03b3e5fd5.jpg" />的组合。 当 &lt;cite&gt;with_replacement&lt;/cite&gt; 设置为 &lt;cite&gt;False&lt;/cite&gt; 时，该行为类似于 python 的 &lt;cite&gt;itertools.combinations&lt;/cite&gt; ；当 &lt;cite&gt;with_replacement&lt;/cite&gt; 设置为时，该行为与 &lt;cite&gt;itertools.combinations_with_replacement&lt;/cite&gt; 相似。 HTG10]设置为 &lt;cite&gt;True&lt;/cite&gt; 。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>输入</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–一维矢量。</p>
</li>
<li>
<p><strong>r</strong>  (<em>python：int</em> <em>，</em> <em>可选</em>）–要组合的元素数</p>
</li>
<li>
<p><strong>with_replacement</strong> (<em>布尔值</em> <em>，</em> <em>可选</em>）–是否允许重复复制</p>
</li>
</ul>
<p>Returns</p>
<p>等于将所有输入张量转换为列表的张量，对这些列表执行 &lt;cite&gt;itertools.combinations&lt;/cite&gt; 或 &lt;cite&gt;itertools.combinations_with_replacement&lt;/cite&gt; ，最后将结果列表转换为张量。</p>
<p>Return type</p>
<p><a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-406-1" name="__codelineno-406-1" href="#__codelineno-406-1"></a>&gt;&gt;&gt; a = [1, 2, 3]
<a id="__codelineno-406-2" name="__codelineno-406-2" href="#__codelineno-406-2"></a>&gt;&gt;&gt; list(itertools.combinations(a, r=2))
<a id="__codelineno-406-3" name="__codelineno-406-3" href="#__codelineno-406-3"></a>[(1, 2), (1, 3), (2, 3)]
<a id="__codelineno-406-4" name="__codelineno-406-4" href="#__codelineno-406-4"></a>&gt;&gt;&gt; list(itertools.combinations(a, r=3))
<a id="__codelineno-406-5" name="__codelineno-406-5" href="#__codelineno-406-5"></a>[(1, 2, 3)]
<a id="__codelineno-406-6" name="__codelineno-406-6" href="#__codelineno-406-6"></a>&gt;&gt;&gt; list(itertools.combinations_with_replacement(a, r=2))
<a id="__codelineno-406-7" name="__codelineno-406-7" href="#__codelineno-406-7"></a>[(1, 1), (1, 2), (1, 3), (2, 2), (2, 3), (3, 3)]
<a id="__codelineno-406-8" name="__codelineno-406-8" href="#__codelineno-406-8"></a>&gt;&gt;&gt; tensor_a = torch.tensor(a)
<a id="__codelineno-406-9" name="__codelineno-406-9" href="#__codelineno-406-9"></a>&gt;&gt;&gt; torch.combinations(tensor_a)
<a id="__codelineno-406-10" name="__codelineno-406-10" href="#__codelineno-406-10"></a>tensor([[1, 2],
<a id="__codelineno-406-11" name="__codelineno-406-11" href="#__codelineno-406-11"></a>        [1, 3],
<a id="__codelineno-406-12" name="__codelineno-406-12" href="#__codelineno-406-12"></a>        [2, 3]])
<a id="__codelineno-406-13" name="__codelineno-406-13" href="#__codelineno-406-13"></a>&gt;&gt;&gt; torch.combinations(tensor_a, r=3)
<a id="__codelineno-406-14" name="__codelineno-406-14" href="#__codelineno-406-14"></a>tensor([[1, 2, 3]])
<a id="__codelineno-406-15" name="__codelineno-406-15" href="#__codelineno-406-15"></a>&gt;&gt;&gt; torch.combinations(tensor_a, with_replacement=True)
<a id="__codelineno-406-16" name="__codelineno-406-16" href="#__codelineno-406-16"></a>tensor([[1, 1],
<a id="__codelineno-406-17" name="__codelineno-406-17" href="#__codelineno-406-17"></a>        [1, 2],
<a id="__codelineno-406-18" name="__codelineno-406-18" href="#__codelineno-406-18"></a>        [1, 3],
<a id="__codelineno-406-19" name="__codelineno-406-19" href="#__codelineno-406-19"></a>        [2, 2],
<a id="__codelineno-406-20" name="__codelineno-406-20" href="#__codelineno-406-20"></a>        [2, 3],
<a id="__codelineno-406-21" name="__codelineno-406-21" href="#__codelineno-406-21"></a>        [3, 3]])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-407-1" name="__codelineno-407-1" href="#__codelineno-407-1"></a>torch.cross(input, other, dim=-1, out=None) → Tensor¶
</code></pre></div>
<p>返回向量在<code>input</code>和<code>other</code>的维度<code>dim</code>中的叉积。</p>
<p><code>input</code>和<code>other</code>的大小必须相同，并且<code>dim</code>尺寸的大小应为 3。</p>
<p>如果未提供<code>dim</code>，则默认为找到的第一个尺寸为 3 的尺寸。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>other</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the second input tensor</p>
</li>
<li>
<p><strong>暗淡的</strong> (<em>python：int</em> <em>，</em> <em>可选</em>）–取叉积的尺寸。</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-408-1" name="__codelineno-408-1" href="#__codelineno-408-1"></a>&gt;&gt;&gt; a = torch.randn(4, 3)
<a id="__codelineno-408-2" name="__codelineno-408-2" href="#__codelineno-408-2"></a>&gt;&gt;&gt; a
<a id="__codelineno-408-3" name="__codelineno-408-3" href="#__codelineno-408-3"></a>tensor([[-0.3956,  1.1455,  1.6895],
<a id="__codelineno-408-4" name="__codelineno-408-4" href="#__codelineno-408-4"></a>        [-0.5849,  1.3672,  0.3599],
<a id="__codelineno-408-5" name="__codelineno-408-5" href="#__codelineno-408-5"></a>        [-1.1626,  0.7180, -0.0521],
<a id="__codelineno-408-6" name="__codelineno-408-6" href="#__codelineno-408-6"></a>        [-0.1339,  0.9902, -2.0225]])
<a id="__codelineno-408-7" name="__codelineno-408-7" href="#__codelineno-408-7"></a>&gt;&gt;&gt; b = torch.randn(4, 3)
<a id="__codelineno-408-8" name="__codelineno-408-8" href="#__codelineno-408-8"></a>&gt;&gt;&gt; b
<a id="__codelineno-408-9" name="__codelineno-408-9" href="#__codelineno-408-9"></a>tensor([[-0.0257, -1.4725, -1.2251],
<a id="__codelineno-408-10" name="__codelineno-408-10" href="#__codelineno-408-10"></a>        [-1.1479, -0.7005, -1.9757],
<a id="__codelineno-408-11" name="__codelineno-408-11" href="#__codelineno-408-11"></a>        [-1.3904,  0.3726, -1.1836],
<a id="__codelineno-408-12" name="__codelineno-408-12" href="#__codelineno-408-12"></a>        [-0.9688, -0.7153,  0.2159]])
<a id="__codelineno-408-13" name="__codelineno-408-13" href="#__codelineno-408-13"></a>&gt;&gt;&gt; torch.cross(a, b, dim=1)
<a id="__codelineno-408-14" name="__codelineno-408-14" href="#__codelineno-408-14"></a>tensor([[ 1.0844, -0.5281,  0.6120],
<a id="__codelineno-408-15" name="__codelineno-408-15" href="#__codelineno-408-15"></a>        [-2.4490, -1.5687,  1.9792],
<a id="__codelineno-408-16" name="__codelineno-408-16" href="#__codelineno-408-16"></a>        [-0.8304, -1.3037,  0.5650],
<a id="__codelineno-408-17" name="__codelineno-408-17" href="#__codelineno-408-17"></a>        [-1.2329,  1.9883,  1.0551]])
<a id="__codelineno-408-18" name="__codelineno-408-18" href="#__codelineno-408-18"></a>&gt;&gt;&gt; torch.cross(a, b)
<a id="__codelineno-408-19" name="__codelineno-408-19" href="#__codelineno-408-19"></a>tensor([[ 1.0844, -0.5281,  0.6120],
<a id="__codelineno-408-20" name="__codelineno-408-20" href="#__codelineno-408-20"></a>        [-2.4490, -1.5687,  1.9792],
<a id="__codelineno-408-21" name="__codelineno-408-21" href="#__codelineno-408-21"></a>        [-0.8304, -1.3037,  0.5650],
<a id="__codelineno-408-22" name="__codelineno-408-22" href="#__codelineno-408-22"></a>        [-1.2329,  1.9883,  1.0551]])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-409-1" name="__codelineno-409-1" href="#__codelineno-409-1"></a>torch.cumprod(input, dim, out=None, dtype=None) → Tensor¶
</code></pre></div>
<p>返回维度为<code>dim</code>的<code>input</code>元素的累积积。</p>
<p>例如，如果<code>input</code>是大小为 N 的向量，则结果也将是大小为 N 的向量(带有元素）。</p>
<p><img alt="" src="../img/89cdfc25537850f458deb6fa7119b4b4.jpg" /></p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>暗淡的</strong> (<em>python：int</em> )–执行操作的尺寸</p>
</li>
<li>
<p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) – the desired data type of returned tensor. If specified, the input tensor is casted to <code>dtype</code> before the operation is performed. This is useful for preventing data type overflows. Default: None.</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-410-1" name="__codelineno-410-1" href="#__codelineno-410-1"></a>&gt;&gt;&gt; a = torch.randn(10)
<a id="__codelineno-410-2" name="__codelineno-410-2" href="#__codelineno-410-2"></a>&gt;&gt;&gt; a
<a id="__codelineno-410-3" name="__codelineno-410-3" href="#__codelineno-410-3"></a>tensor([ 0.6001,  0.2069, -0.1919,  0.9792,  0.6727,  1.0062,  0.4126,
<a id="__codelineno-410-4" name="__codelineno-410-4" href="#__codelineno-410-4"></a>        -0.2129, -0.4206,  0.1968])
<a id="__codelineno-410-5" name="__codelineno-410-5" href="#__codelineno-410-5"></a>&gt;&gt;&gt; torch.cumprod(a, dim=0)
<a id="__codelineno-410-6" name="__codelineno-410-6" href="#__codelineno-410-6"></a>tensor([ 0.6001,  0.1241, -0.0238, -0.0233, -0.0157, -0.0158, -0.0065,
<a id="__codelineno-410-7" name="__codelineno-410-7" href="#__codelineno-410-7"></a>         0.0014, -0.0006, -0.0001])
<a id="__codelineno-410-8" name="__codelineno-410-8" href="#__codelineno-410-8"></a>
<a id="__codelineno-410-9" name="__codelineno-410-9" href="#__codelineno-410-9"></a>&gt;&gt;&gt; a[5] = 0.0
<a id="__codelineno-410-10" name="__codelineno-410-10" href="#__codelineno-410-10"></a>&gt;&gt;&gt; torch.cumprod(a, dim=0)
<a id="__codelineno-410-11" name="__codelineno-410-11" href="#__codelineno-410-11"></a>tensor([ 0.6001,  0.1241, -0.0238, -0.0233, -0.0157, -0.0000, -0.0000,
<a id="__codelineno-410-12" name="__codelineno-410-12" href="#__codelineno-410-12"></a>         0.0000, -0.0000, -0.0000])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-411-1" name="__codelineno-411-1" href="#__codelineno-411-1"></a>torch.cumsum(input, dim, out=None, dtype=None) → Tensor¶
</code></pre></div>
<p>返回维度为<code>dim</code>的<code>input</code>元素的累积和。</p>
<p>For example, if <code>input</code> is a vector of size N, the result will also be a vector of size N, with elements.</p>
<p><img alt="" src="../img/0a2d62fcc8f81c2e2b3da579ec8cbccb.jpg" /></p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>dim</strong> (<em>python:int</em>) – the dimension to do the operation over</p>
</li>
<li>
<p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) – the desired data type of returned tensor. If specified, the input tensor is casted to <code>dtype</code> before the operation is performed. This is useful for preventing data type overflows. Default: None.</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-412-1" name="__codelineno-412-1" href="#__codelineno-412-1"></a>&gt;&gt;&gt; a = torch.randn(10)
<a id="__codelineno-412-2" name="__codelineno-412-2" href="#__codelineno-412-2"></a>&gt;&gt;&gt; a
<a id="__codelineno-412-3" name="__codelineno-412-3" href="#__codelineno-412-3"></a>tensor([-0.8286, -0.4890,  0.5155,  0.8443,  0.1865, -0.1752, -2.0595,
<a id="__codelineno-412-4" name="__codelineno-412-4" href="#__codelineno-412-4"></a>         0.1850, -1.1571, -0.4243])
<a id="__codelineno-412-5" name="__codelineno-412-5" href="#__codelineno-412-5"></a>&gt;&gt;&gt; torch.cumsum(a, dim=0)
<a id="__codelineno-412-6" name="__codelineno-412-6" href="#__codelineno-412-6"></a>tensor([-0.8286, -1.3175, -0.8020,  0.0423,  0.2289,  0.0537, -2.0058,
<a id="__codelineno-412-7" name="__codelineno-412-7" href="#__codelineno-412-7"></a>        -1.8209, -2.9780, -3.4022])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-413-1" name="__codelineno-413-1" href="#__codelineno-413-1"></a>torch.diag(input, diagonal=0, out=None) → Tensor¶
</code></pre></div>
<ul>
<li>
<p>如果<code>input</code>是矢量(1-D 张量），则返回以<code>input</code>的元素为对角线的 2-D 方形张量。</p>
</li>
<li>
<p>如果<code>input</code>是矩阵(2-D 张量），则返回带有<code>input</code>的对角元素的 1-D 张量。</p>
</li>
</ul>
<p>参数 <a href="#torch.diagonal" title="torch.diagonal"><code>diagonal</code></a> 控制要考虑的对角线：</p>
<ul>
<li>
<p>如果 <a href="#torch.diagonal" title="torch.diagonal"><code>diagonal</code></a> = 0，则它是主对角线。</p>
</li>
<li>
<p>如果 <a href="#torch.diagonal" title="torch.diagonal"><code>diagonal</code></a> &gt; 0，则它在主对角线上方。</p>
</li>
<li>
<p>如果 <a href="#torch.diagonal" title="torch.diagonal"><code>diagonal</code></a> &lt; 0，则它位于主对角线下方。</p>
</li>
</ul>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>对角线</strong> (<em>python：int</em> <em>，</em> <em>可选</em>）–对角线</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>See also</p>
<p><a href="#torch.diagonal" title="torch.diagonal"><code>torch.diagonal()</code></a> 始终返回其输入的对角线。</p>
<p><a href="#torch.diagflat" title="torch.diagflat"><code>torch.diagflat()</code></a> 始终使用输入指定的对角线元素构造张量。</p>
<p>Examples:</p>
<p>获取输入向量为对角线的方阵：</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-414-1" name="__codelineno-414-1" href="#__codelineno-414-1"></a>&gt;&gt;&gt; a = torch.randn(3)
<a id="__codelineno-414-2" name="__codelineno-414-2" href="#__codelineno-414-2"></a>&gt;&gt;&gt; a
<a id="__codelineno-414-3" name="__codelineno-414-3" href="#__codelineno-414-3"></a>tensor([ 0.5950,-0.0872, 2.3298])
<a id="__codelineno-414-4" name="__codelineno-414-4" href="#__codelineno-414-4"></a>&gt;&gt;&gt; torch.diag(a)
<a id="__codelineno-414-5" name="__codelineno-414-5" href="#__codelineno-414-5"></a>tensor([[ 0.5950, 0.0000, 0.0000],
<a id="__codelineno-414-6" name="__codelineno-414-6" href="#__codelineno-414-6"></a>        [ 0.0000,-0.0872, 0.0000],
<a id="__codelineno-414-7" name="__codelineno-414-7" href="#__codelineno-414-7"></a>        [ 0.0000, 0.0000, 2.3298]])
<a id="__codelineno-414-8" name="__codelineno-414-8" href="#__codelineno-414-8"></a>&gt;&gt;&gt; torch.diag(a, 1)
<a id="__codelineno-414-9" name="__codelineno-414-9" href="#__codelineno-414-9"></a>tensor([[ 0.0000, 0.5950, 0.0000, 0.0000],
<a id="__codelineno-414-10" name="__codelineno-414-10" href="#__codelineno-414-10"></a>        [ 0.0000, 0.0000,-0.0872, 0.0000],
<a id="__codelineno-414-11" name="__codelineno-414-11" href="#__codelineno-414-11"></a>        [ 0.0000, 0.0000, 0.0000, 2.3298],
<a id="__codelineno-414-12" name="__codelineno-414-12" href="#__codelineno-414-12"></a>        [ 0.0000, 0.0000, 0.0000, 0.0000]])
</code></pre></div>
<p>获取给定矩阵的第 k 个对角线：</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-415-1" name="__codelineno-415-1" href="#__codelineno-415-1"></a>&gt;&gt;&gt; a = torch.randn(3, 3)
<a id="__codelineno-415-2" name="__codelineno-415-2" href="#__codelineno-415-2"></a>&gt;&gt;&gt; a
<a id="__codelineno-415-3" name="__codelineno-415-3" href="#__codelineno-415-3"></a>tensor([[-0.4264, 0.0255,-0.1064],
<a id="__codelineno-415-4" name="__codelineno-415-4" href="#__codelineno-415-4"></a>        [ 0.8795,-0.2429, 0.1374],
<a id="__codelineno-415-5" name="__codelineno-415-5" href="#__codelineno-415-5"></a>        [ 0.1029,-0.6482,-1.6300]])
<a id="__codelineno-415-6" name="__codelineno-415-6" href="#__codelineno-415-6"></a>&gt;&gt;&gt; torch.diag(a, 0)
<a id="__codelineno-415-7" name="__codelineno-415-7" href="#__codelineno-415-7"></a>tensor([-0.4264,-0.2429,-1.6300])
<a id="__codelineno-415-8" name="__codelineno-415-8" href="#__codelineno-415-8"></a>&gt;&gt;&gt; torch.diag(a, 1)
<a id="__codelineno-415-9" name="__codelineno-415-9" href="#__codelineno-415-9"></a>tensor([ 0.0255, 0.1374])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-416-1" name="__codelineno-416-1" href="#__codelineno-416-1"></a>torch.diag_embed(input, offset=0, dim1=-2, dim2=-1) → Tensor¶
</code></pre></div>
<p>创建一个张量，其某些 2D 平面(由<code>dim1</code>和<code>dim2</code>指定）的对角线由<code>input</code>填充。 为了便于创建批处理对角矩阵，默认情况下选择由返回张量的最后两个维构成的 2D 平面。</p>
<p>参数<code>offset</code>控制要考虑的对角线：</p>
<ul>
<li>
<p>如果<code>offset</code> = 0，则它是主对角线。</p>
</li>
<li>
<p>如果<code>offset</code> &gt;为 0，则它​​在主对角线上方。</p>
</li>
<li>
<p>如果<code>offset</code> &lt;为 0，则它​​在主对角线下方。</p>
</li>
</ul>
<p>将计算新矩阵的大小，以使指定的对角线成为最后一个输入尺寸的大小。 注意，对于<img alt="" src="../img/e647c6371faf8b6dd9327515ae95cbdb.jpg" />以外的<code>offset</code>，<code>dim1</code>和<code>dim2</code>的顺序很重要。 交换它们等效于更改<code>offset</code>的符号。</p>
<p>将 <a href="#torch.diagonal" title="torch.diagonal"><code>torch.diagonal()</code></a> 应用于具有相同参数的该函数的输出，将产生与输入相同的矩阵。 但是， <a href="#torch.diagonal" title="torch.diagonal"><code>torch.diagonal()</code></a> 具有不同的默认尺寸，因此需要明确指定这些尺寸。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>输入</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–输入张量。 必须至少为一维。</p>
</li>
<li>
<p><strong>偏移量</strong> (<em>python：int</em> <em>，</em> <em>可选</em>）–要考虑的对角线。 默认值：0(主对角线）。</p>
</li>
<li>
<p><strong>dim1</strong>  (<em>python：int</em> <em>，</em> <em>可选</em>）–取对角线的第一维。 默认值：-2。</p>
</li>
<li>
<p><strong>dim2</strong>  (<em>python：int</em> <em>，</em> <em>可选</em>）–取对角线的第二维。 默认值：-1。</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-417-1" name="__codelineno-417-1" href="#__codelineno-417-1"></a>&gt;&gt;&gt; a = torch.randn(2, 3)
<a id="__codelineno-417-2" name="__codelineno-417-2" href="#__codelineno-417-2"></a>&gt;&gt;&gt; torch.diag_embed(a)
<a id="__codelineno-417-3" name="__codelineno-417-3" href="#__codelineno-417-3"></a>tensor([[[ 1.5410,  0.0000,  0.0000],
<a id="__codelineno-417-4" name="__codelineno-417-4" href="#__codelineno-417-4"></a>         [ 0.0000, -0.2934,  0.0000],
<a id="__codelineno-417-5" name="__codelineno-417-5" href="#__codelineno-417-5"></a>         [ 0.0000,  0.0000, -2.1788]],
<a id="__codelineno-417-6" name="__codelineno-417-6" href="#__codelineno-417-6"></a>
<a id="__codelineno-417-7" name="__codelineno-417-7" href="#__codelineno-417-7"></a>        [[ 0.5684,  0.0000,  0.0000],
<a id="__codelineno-417-8" name="__codelineno-417-8" href="#__codelineno-417-8"></a>         [ 0.0000, -1.0845,  0.0000],
<a id="__codelineno-417-9" name="__codelineno-417-9" href="#__codelineno-417-9"></a>         [ 0.0000,  0.0000, -1.3986]]])
<a id="__codelineno-417-10" name="__codelineno-417-10" href="#__codelineno-417-10"></a>
<a id="__codelineno-417-11" name="__codelineno-417-11" href="#__codelineno-417-11"></a>&gt;&gt;&gt; torch.diag_embed(a, offset=1, dim1=0, dim2=2)
<a id="__codelineno-417-12" name="__codelineno-417-12" href="#__codelineno-417-12"></a>tensor([[[ 0.0000,  1.5410,  0.0000,  0.0000],
<a id="__codelineno-417-13" name="__codelineno-417-13" href="#__codelineno-417-13"></a>         [ 0.0000,  0.5684,  0.0000,  0.0000]],
<a id="__codelineno-417-14" name="__codelineno-417-14" href="#__codelineno-417-14"></a>
<a id="__codelineno-417-15" name="__codelineno-417-15" href="#__codelineno-417-15"></a>        [[ 0.0000,  0.0000, -0.2934,  0.0000],
<a id="__codelineno-417-16" name="__codelineno-417-16" href="#__codelineno-417-16"></a>         [ 0.0000,  0.0000, -1.0845,  0.0000]],
<a id="__codelineno-417-17" name="__codelineno-417-17" href="#__codelineno-417-17"></a>
<a id="__codelineno-417-18" name="__codelineno-417-18" href="#__codelineno-417-18"></a>        [[ 0.0000,  0.0000,  0.0000, -2.1788],
<a id="__codelineno-417-19" name="__codelineno-417-19" href="#__codelineno-417-19"></a>         [ 0.0000,  0.0000,  0.0000, -1.3986]],
<a id="__codelineno-417-20" name="__codelineno-417-20" href="#__codelineno-417-20"></a>
<a id="__codelineno-417-21" name="__codelineno-417-21" href="#__codelineno-417-21"></a>        [[ 0.0000,  0.0000,  0.0000,  0.0000],
<a id="__codelineno-417-22" name="__codelineno-417-22" href="#__codelineno-417-22"></a>         [ 0.0000,  0.0000,  0.0000,  0.0000]]])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-418-1" name="__codelineno-418-1" href="#__codelineno-418-1"></a>torch.diagflat(input, offset=0) → Tensor¶
</code></pre></div>
<ul>
<li>
<p>If <code>input</code> is a vector (1-D tensor), then returns a 2-D square tensor with the elements of <code>input</code> as the diagonal.</p>
</li>
<li>
<p>如果<code>input</code>是一维以上的张量，则返回一个二维张量，其对角线元素等于展平的<code>input</code>。</p>
</li>
</ul>
<p>The argument <code>offset</code> controls which diagonal to consider:</p>
<ul>
<li>
<p>If <code>offset</code> = 0, it is the main diagonal.</p>
</li>
<li>
<p>If <code>offset</code> &gt; 0, it is above the main diagonal.</p>
</li>
<li>
<p>If <code>offset</code> &lt; 0, it is below the main diagonal.</p>
</li>
</ul>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>偏移量</strong> (<em>python：int</em> <em>，</em> <em>可选</em>）–要考虑的对角线。 默认值：0(主对角线）。</p>
</li>
</ul>
<p>Examples:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-419-1" name="__codelineno-419-1" href="#__codelineno-419-1"></a>&gt;&gt;&gt; a = torch.randn(3)
<a id="__codelineno-419-2" name="__codelineno-419-2" href="#__codelineno-419-2"></a>&gt;&gt;&gt; a
<a id="__codelineno-419-3" name="__codelineno-419-3" href="#__codelineno-419-3"></a>tensor([-0.2956, -0.9068,  0.1695])
<a id="__codelineno-419-4" name="__codelineno-419-4" href="#__codelineno-419-4"></a>&gt;&gt;&gt; torch.diagflat(a)
<a id="__codelineno-419-5" name="__codelineno-419-5" href="#__codelineno-419-5"></a>tensor([[-0.2956,  0.0000,  0.0000],
<a id="__codelineno-419-6" name="__codelineno-419-6" href="#__codelineno-419-6"></a>        [ 0.0000, -0.9068,  0.0000],
<a id="__codelineno-419-7" name="__codelineno-419-7" href="#__codelineno-419-7"></a>        [ 0.0000,  0.0000,  0.1695]])
<a id="__codelineno-419-8" name="__codelineno-419-8" href="#__codelineno-419-8"></a>&gt;&gt;&gt; torch.diagflat(a, 1)
<a id="__codelineno-419-9" name="__codelineno-419-9" href="#__codelineno-419-9"></a>tensor([[ 0.0000, -0.2956,  0.0000,  0.0000],
<a id="__codelineno-419-10" name="__codelineno-419-10" href="#__codelineno-419-10"></a>        [ 0.0000,  0.0000, -0.9068,  0.0000],
<a id="__codelineno-419-11" name="__codelineno-419-11" href="#__codelineno-419-11"></a>        [ 0.0000,  0.0000,  0.0000,  0.1695],
<a id="__codelineno-419-12" name="__codelineno-419-12" href="#__codelineno-419-12"></a>        [ 0.0000,  0.0000,  0.0000,  0.0000]])
<a id="__codelineno-419-13" name="__codelineno-419-13" href="#__codelineno-419-13"></a>
<a id="__codelineno-419-14" name="__codelineno-419-14" href="#__codelineno-419-14"></a>&gt;&gt;&gt; a = torch.randn(2, 2)
<a id="__codelineno-419-15" name="__codelineno-419-15" href="#__codelineno-419-15"></a>&gt;&gt;&gt; a
<a id="__codelineno-419-16" name="__codelineno-419-16" href="#__codelineno-419-16"></a>tensor([[ 0.2094, -0.3018],
<a id="__codelineno-419-17" name="__codelineno-419-17" href="#__codelineno-419-17"></a>        [-0.1516,  1.9342]])
<a id="__codelineno-419-18" name="__codelineno-419-18" href="#__codelineno-419-18"></a>&gt;&gt;&gt; torch.diagflat(a)
<a id="__codelineno-419-19" name="__codelineno-419-19" href="#__codelineno-419-19"></a>tensor([[ 0.2094,  0.0000,  0.0000,  0.0000],
<a id="__codelineno-419-20" name="__codelineno-419-20" href="#__codelineno-419-20"></a>        [ 0.0000, -0.3018,  0.0000,  0.0000],
<a id="__codelineno-419-21" name="__codelineno-419-21" href="#__codelineno-419-21"></a>        [ 0.0000,  0.0000, -0.1516,  0.0000],
<a id="__codelineno-419-22" name="__codelineno-419-22" href="#__codelineno-419-22"></a>        [ 0.0000,  0.0000,  0.0000,  1.9342]])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-420-1" name="__codelineno-420-1" href="#__codelineno-420-1"></a>torch.diagonal(input, offset=0, dim1=0, dim2=1) → Tensor¶
</code></pre></div>
<p>返回<code>input</code>的局部视图，其对角线元素相对于<code>dim1</code>和<code>dim2</code>附加为尺寸的末端形状。</p>
<p>The argument <code>offset</code> controls which diagonal to consider:</p>
<ul>
<li>
<p>If <code>offset</code> = 0, it is the main diagonal.</p>
</li>
<li>
<p>If <code>offset</code> &gt; 0, it is above the main diagonal.</p>
</li>
<li>
<p>If <code>offset</code> &lt; 0, it is below the main diagonal.</p>
</li>
</ul>
<p>将 <a href="#torch.diag_embed" title="torch.diag_embed"><code>torch.diag_embed()</code></a> 应用于具有相同参数的此函数的输出，将产生一个带有输入对角线项的对角矩阵。 但是， <a href="#torch.diag_embed" title="torch.diag_embed"><code>torch.diag_embed()</code></a> 具有不同的默认尺寸，因此需要明确指定这些尺寸。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>输入</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–输入张量。 必须至少为二维。</p>
</li>
<li>
<p><strong>offset</strong> (<em>python:int__,</em> <em>optional</em>) – which diagonal to consider. Default: 0 (main diagonal).</p>
</li>
<li>
<p><strong>dim1</strong>  (<em>python：int</em> <em>，</em> <em>可选</em>）–取对角线的第一维。 默认值：0</p>
</li>
<li>
<p><strong>dim2</strong>  (<em>python：int</em> <em>，</em> <em>可选</em>）–取对角线的第二维。 默认值：1。</p>
</li>
</ul>
<p>Note</p>
<p>要取一批对角线，请传入 dim1 = -2，dim2 = -1。</p>
<p>Examples:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-421-1" name="__codelineno-421-1" href="#__codelineno-421-1"></a>&gt;&gt;&gt; a = torch.randn(3, 3)
<a id="__codelineno-421-2" name="__codelineno-421-2" href="#__codelineno-421-2"></a>&gt;&gt;&gt; a
<a id="__codelineno-421-3" name="__codelineno-421-3" href="#__codelineno-421-3"></a>tensor([[-1.0854,  1.1431, -0.1752],
<a id="__codelineno-421-4" name="__codelineno-421-4" href="#__codelineno-421-4"></a>        [ 0.8536, -0.0905,  0.0360],
<a id="__codelineno-421-5" name="__codelineno-421-5" href="#__codelineno-421-5"></a>        [ 0.6927, -0.3735, -0.4945]])
<a id="__codelineno-421-6" name="__codelineno-421-6" href="#__codelineno-421-6"></a>
<a id="__codelineno-421-7" name="__codelineno-421-7" href="#__codelineno-421-7"></a>&gt;&gt;&gt; torch.diagonal(a, 0)
<a id="__codelineno-421-8" name="__codelineno-421-8" href="#__codelineno-421-8"></a>tensor([-1.0854, -0.0905, -0.4945])
<a id="__codelineno-421-9" name="__codelineno-421-9" href="#__codelineno-421-9"></a>
<a id="__codelineno-421-10" name="__codelineno-421-10" href="#__codelineno-421-10"></a>&gt;&gt;&gt; torch.diagonal(a, 1)
<a id="__codelineno-421-11" name="__codelineno-421-11" href="#__codelineno-421-11"></a>tensor([ 1.1431,  0.0360])
<a id="__codelineno-421-12" name="__codelineno-421-12" href="#__codelineno-421-12"></a>
<a id="__codelineno-421-13" name="__codelineno-421-13" href="#__codelineno-421-13"></a>&gt;&gt;&gt; x = torch.randn(2, 5, 4, 2)
<a id="__codelineno-421-14" name="__codelineno-421-14" href="#__codelineno-421-14"></a>&gt;&gt;&gt; torch.diagonal(x, offset=-1, dim1=1, dim2=2)
<a id="__codelineno-421-15" name="__codelineno-421-15" href="#__codelineno-421-15"></a>tensor([[[-1.2631,  0.3755, -1.5977, -1.8172],
<a id="__codelineno-421-16" name="__codelineno-421-16" href="#__codelineno-421-16"></a>         [-1.1065,  1.0401, -0.2235, -0.7938]],
<a id="__codelineno-421-17" name="__codelineno-421-17" href="#__codelineno-421-17"></a>
<a id="__codelineno-421-18" name="__codelineno-421-18" href="#__codelineno-421-18"></a>        [[-1.7325, -0.3081,  0.6166,  0.2335],
<a id="__codelineno-421-19" name="__codelineno-421-19" href="#__codelineno-421-19"></a>         [ 1.0500,  0.7336, -0.3836, -1.1015]]])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-422-1" name="__codelineno-422-1" href="#__codelineno-422-1"></a>torch.einsum(equation, *operands) → Tensor¶
</code></pre></div>
<p>此函数提供了一种使用爱因斯坦求和约定来计算多线性表达式(即乘积和）的方法。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>公式</strong>(<em>字符串</em>）–该公式以与操作数和结果的每个维相关联的小写字母(索引）形式给出。 左侧列出了操作数维，以逗号分隔。 每个张量维应该有一个索引字母。 右侧紧随&lt;cite&gt;-&gt;&lt;/cite&gt; 之后，并给出输出的索引。 如果省略&lt;cite&gt;-&gt;&lt;/cite&gt; 和右侧，则将其隐式定义为所有索引的按字母顺序排序的列表，这些列表在左侧仅出现一次。 在将操作数条目相乘后，将输出中不等于的索引相加。 如果同一操作数的索引出现多次，则采用对角线。 椭圆&lt;cite&gt;…&lt;/cite&gt;代表固定数量的尺寸。 如果推断出右侧，则省略号尺寸位于输出的开头。</p>
</li>
<li>
<p><strong>操作数</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–计算爱因斯坦总和的操作数。</p>
</li>
</ul>
<p>Examples:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-423-1" name="__codelineno-423-1" href="#__codelineno-423-1"></a>&gt;&gt;&gt; x = torch.randn(5)
<a id="__codelineno-423-2" name="__codelineno-423-2" href="#__codelineno-423-2"></a>&gt;&gt;&gt; y = torch.randn(4)
<a id="__codelineno-423-3" name="__codelineno-423-3" href="#__codelineno-423-3"></a>&gt;&gt;&gt; torch.einsum(&#39;i,j-&gt;ij&#39;, x, y)  # outer product
<a id="__codelineno-423-4" name="__codelineno-423-4" href="#__codelineno-423-4"></a>tensor([[-0.0570, -0.0286, -0.0231,  0.0197],
<a id="__codelineno-423-5" name="__codelineno-423-5" href="#__codelineno-423-5"></a>        [ 1.2616,  0.6335,  0.5113, -0.4351],
<a id="__codelineno-423-6" name="__codelineno-423-6" href="#__codelineno-423-6"></a>        [ 1.4452,  0.7257,  0.5857, -0.4984],
<a id="__codelineno-423-7" name="__codelineno-423-7" href="#__codelineno-423-7"></a>        [-0.4647, -0.2333, -0.1883,  0.1603],
<a id="__codelineno-423-8" name="__codelineno-423-8" href="#__codelineno-423-8"></a>        [-1.1130, -0.5588, -0.4510,  0.3838]])
<a id="__codelineno-423-9" name="__codelineno-423-9" href="#__codelineno-423-9"></a>
<a id="__codelineno-423-10" name="__codelineno-423-10" href="#__codelineno-423-10"></a>&gt;&gt;&gt; A = torch.randn(3,5,4)
<a id="__codelineno-423-11" name="__codelineno-423-11" href="#__codelineno-423-11"></a>&gt;&gt;&gt; l = torch.randn(2,5)
<a id="__codelineno-423-12" name="__codelineno-423-12" href="#__codelineno-423-12"></a>&gt;&gt;&gt; r = torch.randn(2,4)
<a id="__codelineno-423-13" name="__codelineno-423-13" href="#__codelineno-423-13"></a>&gt;&gt;&gt; torch.einsum(&#39;bn,anm,bm-&gt;ba&#39;, l, A, r) # compare torch.nn.functional.bilinear
<a id="__codelineno-423-14" name="__codelineno-423-14" href="#__codelineno-423-14"></a>tensor([[-0.3430, -5.2405,  0.4494],
<a id="__codelineno-423-15" name="__codelineno-423-15" href="#__codelineno-423-15"></a>        [ 0.3311,  5.5201, -3.0356]])
<a id="__codelineno-423-16" name="__codelineno-423-16" href="#__codelineno-423-16"></a>
<a id="__codelineno-423-17" name="__codelineno-423-17" href="#__codelineno-423-17"></a>&gt;&gt;&gt; As = torch.randn(3,2,5)
<a id="__codelineno-423-18" name="__codelineno-423-18" href="#__codelineno-423-18"></a>&gt;&gt;&gt; Bs = torch.randn(3,5,4)
<a id="__codelineno-423-19" name="__codelineno-423-19" href="#__codelineno-423-19"></a>&gt;&gt;&gt; torch.einsum(&#39;bij,bjk-&gt;bik&#39;, As, Bs) # batch matrix multiplication
<a id="__codelineno-423-20" name="__codelineno-423-20" href="#__codelineno-423-20"></a>tensor([[[-1.0564, -1.5904,  3.2023,  3.1271],
<a id="__codelineno-423-21" name="__codelineno-423-21" href="#__codelineno-423-21"></a>         [-1.6706, -0.8097, -0.8025, -2.1183]],
<a id="__codelineno-423-22" name="__codelineno-423-22" href="#__codelineno-423-22"></a>
<a id="__codelineno-423-23" name="__codelineno-423-23" href="#__codelineno-423-23"></a>        [[ 4.2239,  0.3107, -0.5756, -0.2354],
<a id="__codelineno-423-24" name="__codelineno-423-24" href="#__codelineno-423-24"></a>         [-1.4558, -0.3460,  1.5087, -0.8530]],
<a id="__codelineno-423-25" name="__codelineno-423-25" href="#__codelineno-423-25"></a>
<a id="__codelineno-423-26" name="__codelineno-423-26" href="#__codelineno-423-26"></a>        [[ 2.8153,  1.8787, -4.3839, -1.2112],
<a id="__codelineno-423-27" name="__codelineno-423-27" href="#__codelineno-423-27"></a>         [ 0.3728, -2.1131,  0.0921,  0.8305]]])
<a id="__codelineno-423-28" name="__codelineno-423-28" href="#__codelineno-423-28"></a>
<a id="__codelineno-423-29" name="__codelineno-423-29" href="#__codelineno-423-29"></a>&gt;&gt;&gt; A = torch.randn(3, 3)
<a id="__codelineno-423-30" name="__codelineno-423-30" href="#__codelineno-423-30"></a>&gt;&gt;&gt; torch.einsum(&#39;ii-&gt;i&#39;, A) # diagonal
<a id="__codelineno-423-31" name="__codelineno-423-31" href="#__codelineno-423-31"></a>tensor([-0.7825,  0.8291, -0.1936])
<a id="__codelineno-423-32" name="__codelineno-423-32" href="#__codelineno-423-32"></a>
<a id="__codelineno-423-33" name="__codelineno-423-33" href="#__codelineno-423-33"></a>&gt;&gt;&gt; A = torch.randn(4, 3, 3)
<a id="__codelineno-423-34" name="__codelineno-423-34" href="#__codelineno-423-34"></a>&gt;&gt;&gt; torch.einsum(&#39;...ii-&gt;...i&#39;, A) # batch diagonal
<a id="__codelineno-423-35" name="__codelineno-423-35" href="#__codelineno-423-35"></a>tensor([[-1.0864,  0.7292,  0.0569],
<a id="__codelineno-423-36" name="__codelineno-423-36" href="#__codelineno-423-36"></a>        [-0.9725, -1.0270,  0.6493],
<a id="__codelineno-423-37" name="__codelineno-423-37" href="#__codelineno-423-37"></a>        [ 0.5832, -1.1716, -1.5084],
<a id="__codelineno-423-38" name="__codelineno-423-38" href="#__codelineno-423-38"></a>        [ 0.4041, -1.1690,  0.8570]])
<a id="__codelineno-423-39" name="__codelineno-423-39" href="#__codelineno-423-39"></a>
<a id="__codelineno-423-40" name="__codelineno-423-40" href="#__codelineno-423-40"></a>&gt;&gt;&gt; A = torch.randn(2, 3, 4, 5)
<a id="__codelineno-423-41" name="__codelineno-423-41" href="#__codelineno-423-41"></a>&gt;&gt;&gt; torch.einsum(&#39;...ij-&gt;...ji&#39;, A).shape # batch permute
<a id="__codelineno-423-42" name="__codelineno-423-42" href="#__codelineno-423-42"></a>torch.Size([2, 3, 5, 4])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-424-1" name="__codelineno-424-1" href="#__codelineno-424-1"></a>torch.flatten(input, start_dim=0, end_dim=-1) → Tensor¶
</code></pre></div>
<p>展平张量中连续的暗淡范围。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>start_dim</strong>  (<em>python：int</em> )–第一个变暗的像素</p>
</li>
<li>
<p><strong>end_dim</strong>  (<em>python：int</em> )–最后一个变暗的像素</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-425-1" name="__codelineno-425-1" href="#__codelineno-425-1"></a>&gt;&gt;&gt; t = torch.tensor([[[1, 2],
<a id="__codelineno-425-2" name="__codelineno-425-2" href="#__codelineno-425-2"></a>                       [3, 4]],
<a id="__codelineno-425-3" name="__codelineno-425-3" href="#__codelineno-425-3"></a>                      [[5, 6],
<a id="__codelineno-425-4" name="__codelineno-425-4" href="#__codelineno-425-4"></a>                       [7, 8]]])
<a id="__codelineno-425-5" name="__codelineno-425-5" href="#__codelineno-425-5"></a>&gt;&gt;&gt; torch.flatten(t)
<a id="__codelineno-425-6" name="__codelineno-425-6" href="#__codelineno-425-6"></a>tensor([1, 2, 3, 4, 5, 6, 7, 8])
<a id="__codelineno-425-7" name="__codelineno-425-7" href="#__codelineno-425-7"></a>&gt;&gt;&gt; torch.flatten(t, start_dim=1)
<a id="__codelineno-425-8" name="__codelineno-425-8" href="#__codelineno-425-8"></a>tensor([[1, 2, 3, 4],
<a id="__codelineno-425-9" name="__codelineno-425-9" href="#__codelineno-425-9"></a>        [5, 6, 7, 8]])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-426-1" name="__codelineno-426-1" href="#__codelineno-426-1"></a>torch.flip(input, dims) → Tensor¶
</code></pre></div>
<p>沿给定轴反转 n-D 张量的顺序，以暗淡表示。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>使</strong>变暗(<em>列表</em> <em>或</em> <em>元组</em>）–翻转轴</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-427-1" name="__codelineno-427-1" href="#__codelineno-427-1"></a>&gt;&gt;&gt; x = torch.arange(8).view(2, 2, 2)
<a id="__codelineno-427-2" name="__codelineno-427-2" href="#__codelineno-427-2"></a>&gt;&gt;&gt; x
<a id="__codelineno-427-3" name="__codelineno-427-3" href="#__codelineno-427-3"></a>tensor([[[ 0,  1],
<a id="__codelineno-427-4" name="__codelineno-427-4" href="#__codelineno-427-4"></a>         [ 2,  3]],
<a id="__codelineno-427-5" name="__codelineno-427-5" href="#__codelineno-427-5"></a>
<a id="__codelineno-427-6" name="__codelineno-427-6" href="#__codelineno-427-6"></a>        [[ 4,  5],
<a id="__codelineno-427-7" name="__codelineno-427-7" href="#__codelineno-427-7"></a>         [ 6,  7]]])
<a id="__codelineno-427-8" name="__codelineno-427-8" href="#__codelineno-427-8"></a>&gt;&gt;&gt; torch.flip(x, [0, 1])
<a id="__codelineno-427-9" name="__codelineno-427-9" href="#__codelineno-427-9"></a>tensor([[[ 6,  7],
<a id="__codelineno-427-10" name="__codelineno-427-10" href="#__codelineno-427-10"></a>         [ 4,  5]],
<a id="__codelineno-427-11" name="__codelineno-427-11" href="#__codelineno-427-11"></a>
<a id="__codelineno-427-12" name="__codelineno-427-12" href="#__codelineno-427-12"></a>        [[ 2,  3],
<a id="__codelineno-427-13" name="__codelineno-427-13" href="#__codelineno-427-13"></a>         [ 0,  1]]])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-428-1" name="__codelineno-428-1" href="#__codelineno-428-1"></a>torch.rot90(input, k, dims) → Tensor¶
</code></pre></div>
<p>在调光轴指定的平面中将 n-D 张量旋转 90 度。 如果 k &gt; 0，则旋转方向是从第一个轴到第二个轴，对于 k &lt; 0，旋转方向是从第二个轴到第一个轴。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>k</strong>  (<em>python：int</em> )–旋转次数</p>
</li>
<li>
<p><strong>使</strong>变暗(<em>列表</em> <em>或</em> <em>元组</em>）–旋转轴</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-429-1" name="__codelineno-429-1" href="#__codelineno-429-1"></a>&gt;&gt;&gt; x = torch.arange(4).view(2, 2)
<a id="__codelineno-429-2" name="__codelineno-429-2" href="#__codelineno-429-2"></a>&gt;&gt;&gt; x
<a id="__codelineno-429-3" name="__codelineno-429-3" href="#__codelineno-429-3"></a>tensor([[0, 1],
<a id="__codelineno-429-4" name="__codelineno-429-4" href="#__codelineno-429-4"></a>        [2, 3]])
<a id="__codelineno-429-5" name="__codelineno-429-5" href="#__codelineno-429-5"></a>&gt;&gt;&gt; torch.rot90(x, 1, [0, 1])
<a id="__codelineno-429-6" name="__codelineno-429-6" href="#__codelineno-429-6"></a>tensor([[1, 3],
<a id="__codelineno-429-7" name="__codelineno-429-7" href="#__codelineno-429-7"></a>        [0, 2]])
<a id="__codelineno-429-8" name="__codelineno-429-8" href="#__codelineno-429-8"></a>
<a id="__codelineno-429-9" name="__codelineno-429-9" href="#__codelineno-429-9"></a>&gt;&gt;&gt; x = torch.arange(8).view(2, 2, 2)
<a id="__codelineno-429-10" name="__codelineno-429-10" href="#__codelineno-429-10"></a>&gt;&gt;&gt; x
<a id="__codelineno-429-11" name="__codelineno-429-11" href="#__codelineno-429-11"></a>tensor([[[0, 1],
<a id="__codelineno-429-12" name="__codelineno-429-12" href="#__codelineno-429-12"></a>         [2, 3]],
<a id="__codelineno-429-13" name="__codelineno-429-13" href="#__codelineno-429-13"></a>
<a id="__codelineno-429-14" name="__codelineno-429-14" href="#__codelineno-429-14"></a>        [[4, 5],
<a id="__codelineno-429-15" name="__codelineno-429-15" href="#__codelineno-429-15"></a>         [6, 7]]])
<a id="__codelineno-429-16" name="__codelineno-429-16" href="#__codelineno-429-16"></a>&gt;&gt;&gt; torch.rot90(x, 1, [1, 2])
<a id="__codelineno-429-17" name="__codelineno-429-17" href="#__codelineno-429-17"></a>tensor([[[1, 3],
<a id="__codelineno-429-18" name="__codelineno-429-18" href="#__codelineno-429-18"></a>         [0, 2]],
<a id="__codelineno-429-19" name="__codelineno-429-19" href="#__codelineno-429-19"></a>
<a id="__codelineno-429-20" name="__codelineno-429-20" href="#__codelineno-429-20"></a>        [[5, 7],
<a id="__codelineno-429-21" name="__codelineno-429-21" href="#__codelineno-429-21"></a>         [4, 6]]])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-430-1" name="__codelineno-430-1" href="#__codelineno-430-1"></a>torch.histc(input, bins=100, min=0, max=0, out=None) → Tensor¶
</code></pre></div>
<p>计算张量的直方图。</p>
<p>元素被分类为 <a href="#torch.min" title="torch.min"><code>min</code></a> 和 <a href="#torch.max" title="torch.max"><code>max</code></a> 之间的等宽单元。 如果 <a href="#torch.min" title="torch.min"><code>min</code></a> 和 <a href="#torch.max" title="torch.max"><code>max</code></a> 均为零，则使用数据的最小值和最大值。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>箱</strong> (<em>python：int</em> )–直方图箱数</p>
</li>
<li>
<p><strong>min</strong>  (<em>python：int</em> )–范围的下限(包括）</p>
</li>
<li>
<p><strong>最大</strong> (<em>python：int</em> )–范围的上限(包括）</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Returns</p>
<p>直方图表示为张量</p>
<p>Return type</p>
<p><a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-431-1" name="__codelineno-431-1" href="#__codelineno-431-1"></a>&gt;&gt;&gt; torch.histc(torch.tensor([1., 2, 1]), bins=4, min=0, max=3)
<a id="__codelineno-431-2" name="__codelineno-431-2" href="#__codelineno-431-2"></a>tensor([ 0.,  2.,  1.,  0.])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-432-1" name="__codelineno-432-1" href="#__codelineno-432-1"></a>torch.meshgrid(*tensors, **kwargs)¶
</code></pre></div>
<p>取<img alt="" src="../img/7ea0d6aa290d37c8bc57957206bafe80.jpg" />张量(每个张量可以是标量或一维向量），并创建<img alt="" src="../img/7ea0d6aa290d37c8bc57957206bafe80.jpg" /> N 维网格，其中通过扩展<img alt="" src="../img/db26d1a59be5965889bd4d5533b7be61.jpg" />定义<img alt="" src="../img/db26d1a59be5965889bd4d5533b7be61.jpg" /> &lt;sup&gt;第&lt;/sup&gt;网格。 &lt;sup&gt;和&lt;/sup&gt;输入超出其他输入定义的尺寸。</p>
<blockquote>
<div class="highlight"><pre><span></span><code><a id="__codelineno-433-1" name="__codelineno-433-1" href="#__codelineno-433-1"></a>Args:
</code></pre></div>
<p>张量(张量列表）：标量或一维张量的列表。 标量将被自动视为大小为<img alt="" src="../img/fc55934714b3777971b760dd3cf42978.jpg" />的张量</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-434-1" name="__codelineno-434-1" href="#__codelineno-434-1"></a>Returns:
</code></pre></div>
<p>seq(张量序列）：如果输入的<img alt="" src="../img/678502bee29f9b13e7115b18864a5822.jpg" />张量为<img alt="" src="../img/dc2a28f0eb5d798e9f246215dbb10795.jpg" />，则输出也将具有<img alt="" src="../img/678502bee29f9b13e7115b18864a5822.jpg" />张量，其中所有张量均为<img alt="" src="../img/3a092c0d2d4874edbfe18b4364b8a48d.jpg" />。</p>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-435-1" name="__codelineno-435-1" href="#__codelineno-435-1"></a>&amp;gt;&amp;gt;&amp;gt; x = torch.tensor([1, 2, 3])
<a id="__codelineno-435-2" name="__codelineno-435-2" href="#__codelineno-435-2"></a>&amp;gt;&amp;gt;&amp;gt; y = torch.tensor([4, 5, 6])
<a id="__codelineno-435-3" name="__codelineno-435-3" href="#__codelineno-435-3"></a>&amp;gt;&amp;gt;&amp;gt; grid_x, grid_y = torch.meshgrid(x, y)
<a id="__codelineno-435-4" name="__codelineno-435-4" href="#__codelineno-435-4"></a>&amp;gt;&amp;gt;&amp;gt; grid_x
<a id="__codelineno-435-5" name="__codelineno-435-5" href="#__codelineno-435-5"></a>tensor([[1, 1, 1],
<a id="__codelineno-435-6" name="__codelineno-435-6" href="#__codelineno-435-6"></a>        [2, 2, 2],
<a id="__codelineno-435-7" name="__codelineno-435-7" href="#__codelineno-435-7"></a>        [3, 3, 3]])
<a id="__codelineno-435-8" name="__codelineno-435-8" href="#__codelineno-435-8"></a>&amp;gt;&amp;gt;&amp;gt; grid_y
<a id="__codelineno-435-9" name="__codelineno-435-9" href="#__codelineno-435-9"></a>tensor([[4, 5, 6],
<a id="__codelineno-435-10" name="__codelineno-435-10" href="#__codelineno-435-10"></a>        [4, 5, 6],
<a id="__codelineno-435-11" name="__codelineno-435-11" href="#__codelineno-435-11"></a>        [4, 5, 6]])
</code></pre></div>
</blockquote>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-436-1" name="__codelineno-436-1" href="#__codelineno-436-1"></a>torch.renorm(input, p, dim, maxnorm, out=None) → Tensor¶
</code></pre></div>
<p>返回一个张量，其中<code>input</code>沿维度<code>dim</code>的每个子张量均被规范化，以使子张量的 &lt;cite&gt;p&lt;/cite&gt; -norm 小于值<code>maxnorm</code></p>
<p>Note</p>
<p>如果某行的范数低于 &lt;cite&gt;maxnorm&lt;/cite&gt; ，则该行不变</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>p</strong>  (<em>python：float</em> )–范数计算的能力</p>
</li>
<li>
<p><strong>暗淡的</strong> (<em>python：int</em> )–切片以获得子张量的维度</p>
</li>
<li>
<p><strong>maxnorm</strong>  (<em>python：float</em> )–保持每个子张量低于的最大规范</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-437-1" name="__codelineno-437-1" href="#__codelineno-437-1"></a>&gt;&gt;&gt; x = torch.ones(3, 3)
<a id="__codelineno-437-2" name="__codelineno-437-2" href="#__codelineno-437-2"></a>&gt;&gt;&gt; x[1].fill_(2)
<a id="__codelineno-437-3" name="__codelineno-437-3" href="#__codelineno-437-3"></a>tensor([ 2.,  2.,  2.])
<a id="__codelineno-437-4" name="__codelineno-437-4" href="#__codelineno-437-4"></a>&gt;&gt;&gt; x[2].fill_(3)
<a id="__codelineno-437-5" name="__codelineno-437-5" href="#__codelineno-437-5"></a>tensor([ 3.,  3.,  3.])
<a id="__codelineno-437-6" name="__codelineno-437-6" href="#__codelineno-437-6"></a>&gt;&gt;&gt; x
<a id="__codelineno-437-7" name="__codelineno-437-7" href="#__codelineno-437-7"></a>tensor([[ 1.,  1.,  1.],
<a id="__codelineno-437-8" name="__codelineno-437-8" href="#__codelineno-437-8"></a>        [ 2.,  2.,  2.],
<a id="__codelineno-437-9" name="__codelineno-437-9" href="#__codelineno-437-9"></a>        [ 3.,  3.,  3.]])
<a id="__codelineno-437-10" name="__codelineno-437-10" href="#__codelineno-437-10"></a>&gt;&gt;&gt; torch.renorm(x, 1, 0, 5)
<a id="__codelineno-437-11" name="__codelineno-437-11" href="#__codelineno-437-11"></a>tensor([[ 1.0000,  1.0000,  1.0000],
<a id="__codelineno-437-12" name="__codelineno-437-12" href="#__codelineno-437-12"></a>        [ 1.6667,  1.6667,  1.6667],
<a id="__codelineno-437-13" name="__codelineno-437-13" href="#__codelineno-437-13"></a>        [ 1.6667,  1.6667,  1.6667]])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-438-1" name="__codelineno-438-1" href="#__codelineno-438-1"></a>torch.repeat_interleave()¶
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-439-1" name="__codelineno-439-1" href="#__codelineno-439-1"></a>torch.repeat_interleave(input, repeats, dim=None) → Tensor
</code></pre></div>
<p>重复张量的元素。</p>
<p>Warning</p>
<p>这与<code>torch.repeat()</code>不同，但与 &lt;cite&gt;numpy.repeat&lt;/cite&gt; 相似。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>重复</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a> <em>或</em> <em>python：int</em> )–每个元素的重复次数。 重复播放以适合给定轴的形状。</p>
</li>
<li>
<p><strong>暗淡的</strong> (<em>python：int</em> <em>，</em> <em>可选</em>）–沿其重复值的尺寸。 默认情况下，使用展平的输入数组，并返回展平的输出数组。</p>
</li>
</ul>
<p>Returns</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-440-1" name="__codelineno-440-1" href="#__codelineno-440-1"></a>Repeated tensor which has the same shape as input, except along the
</code></pre></div>
<p>给定的轴。</p>
<p>Return type</p>
<p><a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-441-1" name="__codelineno-441-1" href="#__codelineno-441-1"></a>&gt;&gt;&gt; x = torch.tensor([1, 2, 3])
<a id="__codelineno-441-2" name="__codelineno-441-2" href="#__codelineno-441-2"></a>&gt;&gt;&gt; x.repeat_interleave(2)
<a id="__codelineno-441-3" name="__codelineno-441-3" href="#__codelineno-441-3"></a>tensor([1, 1, 2, 2, 3, 3])
<a id="__codelineno-441-4" name="__codelineno-441-4" href="#__codelineno-441-4"></a>&gt;&gt;&gt; y = torch.tensor([[1, 2], [3, 4]])
<a id="__codelineno-441-5" name="__codelineno-441-5" href="#__codelineno-441-5"></a>&gt;&gt;&gt; torch.repeat_interleave(y, 2)
<a id="__codelineno-441-6" name="__codelineno-441-6" href="#__codelineno-441-6"></a>tensor([1, 1, 2, 2, 3, 3, 4, 4])
<a id="__codelineno-441-7" name="__codelineno-441-7" href="#__codelineno-441-7"></a>&gt;&gt;&gt; torch.repeat_interleave(y, 3, dim=1)
<a id="__codelineno-441-8" name="__codelineno-441-8" href="#__codelineno-441-8"></a>tensor([[1, 1, 1, 2, 2, 2],
<a id="__codelineno-441-9" name="__codelineno-441-9" href="#__codelineno-441-9"></a>        [3, 3, 3, 4, 4, 4]])
<a id="__codelineno-441-10" name="__codelineno-441-10" href="#__codelineno-441-10"></a>&gt;&gt;&gt; torch.repeat_interleave(y, torch.tensor([1, 2]), dim=0)
<a id="__codelineno-441-11" name="__codelineno-441-11" href="#__codelineno-441-11"></a>tensor([[1, 2],
<a id="__codelineno-441-12" name="__codelineno-441-12" href="#__codelineno-441-12"></a>        [3, 4],
<a id="__codelineno-441-13" name="__codelineno-441-13" href="#__codelineno-441-13"></a>        [3, 4]])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-442-1" name="__codelineno-442-1" href="#__codelineno-442-1"></a>torch.repeat_interleave(repeats) → Tensor
</code></pre></div>
<p>如果&lt;cite&gt;重复&lt;/cite&gt;为&lt;cite&gt;张量([n1，n2，n3，…]）&lt;/cite&gt;，则输出将为&lt;cite&gt;张量([0，0，…，1，1， …，2，2，…，…]）&lt;/cite&gt;，其中 &lt;cite&gt;0&lt;/cite&gt; 出现 &lt;cite&gt;n1&lt;/cite&gt; 次， &lt;cite&gt;1&lt;/cite&gt; 出现 &lt;cite&gt;n2&lt;/cite&gt; 次，[ &lt;cite&gt;2&lt;/cite&gt; 出现 &lt;cite&gt;n3&lt;/cite&gt; 次，等等。</p>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-443-1" name="__codelineno-443-1" href="#__codelineno-443-1"></a>torch.roll(input, shifts, dims=None) → Tensor¶
</code></pre></div>
<p>沿给定尺寸滚动张量。 移出最后位置的元素将重新引入第一个位置。 如果未指定尺寸，则张量将在滚动之前变平，然后恢复为原始形状。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>移位</strong> (<em>python：int</em> <em>或</em> <em>python：ints</em> 的元组）–张量元素移位的位数 。 如果 shifts 是一个元组，则 dims 必须是相同大小的元组，并且每个维度将滚动相应的值</p>
</li>
<li>
<p><strong>变暗</strong> (<em>python：int</em> <em>或</em> <em>tuple of python：ints</em> )–滚动轴</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-444-1" name="__codelineno-444-1" href="#__codelineno-444-1"></a>&gt;&gt;&gt; x = torch.tensor([1, 2, 3, 4, 5, 6, 7, 8]).view(4, 2)
<a id="__codelineno-444-2" name="__codelineno-444-2" href="#__codelineno-444-2"></a>&gt;&gt;&gt; x
<a id="__codelineno-444-3" name="__codelineno-444-3" href="#__codelineno-444-3"></a>tensor([[1, 2],
<a id="__codelineno-444-4" name="__codelineno-444-4" href="#__codelineno-444-4"></a>        [3, 4],
<a id="__codelineno-444-5" name="__codelineno-444-5" href="#__codelineno-444-5"></a>        [5, 6],
<a id="__codelineno-444-6" name="__codelineno-444-6" href="#__codelineno-444-6"></a>        [7, 8]])
<a id="__codelineno-444-7" name="__codelineno-444-7" href="#__codelineno-444-7"></a>&gt;&gt;&gt; torch.roll(x, 1, 0)
<a id="__codelineno-444-8" name="__codelineno-444-8" href="#__codelineno-444-8"></a>tensor([[7, 8],
<a id="__codelineno-444-9" name="__codelineno-444-9" href="#__codelineno-444-9"></a>        [1, 2],
<a id="__codelineno-444-10" name="__codelineno-444-10" href="#__codelineno-444-10"></a>        [3, 4],
<a id="__codelineno-444-11" name="__codelineno-444-11" href="#__codelineno-444-11"></a>        [5, 6]])
<a id="__codelineno-444-12" name="__codelineno-444-12" href="#__codelineno-444-12"></a>&gt;&gt;&gt; torch.roll(x, -1, 0)
<a id="__codelineno-444-13" name="__codelineno-444-13" href="#__codelineno-444-13"></a>tensor([[3, 4],
<a id="__codelineno-444-14" name="__codelineno-444-14" href="#__codelineno-444-14"></a>        [5, 6],
<a id="__codelineno-444-15" name="__codelineno-444-15" href="#__codelineno-444-15"></a>        [7, 8],
<a id="__codelineno-444-16" name="__codelineno-444-16" href="#__codelineno-444-16"></a>        [1, 2]])
<a id="__codelineno-444-17" name="__codelineno-444-17" href="#__codelineno-444-17"></a>&gt;&gt;&gt; torch.roll(x, shifts=(2, 1), dims=(0, 1))
<a id="__codelineno-444-18" name="__codelineno-444-18" href="#__codelineno-444-18"></a>tensor([[6, 5],
<a id="__codelineno-444-19" name="__codelineno-444-19" href="#__codelineno-444-19"></a>        [8, 7],
<a id="__codelineno-444-20" name="__codelineno-444-20" href="#__codelineno-444-20"></a>        [2, 1],
<a id="__codelineno-444-21" name="__codelineno-444-21" href="#__codelineno-444-21"></a>        [4, 3]])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-445-1" name="__codelineno-445-1" href="#__codelineno-445-1"></a>torch.tensordot(a, b, dims=2)¶
</code></pre></div>
<p>返回 a 和 b 在多个维度上的收缩。</p>
<p><a href="#torch.tensordot" title="torch.tensordot"><code>tensordot</code></a> 实现了广义矩阵乘积。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>a</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–左张量收缩</p>
</li>
<li>
<p><strong>b</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–右张量收缩</p>
</li>
<li>
<p><strong>变暗</strong> (<em>python：int</em> <em>或</em> <em>python：integers</em> 的两个列表的元组）–要收缩的尺寸数或尺寸的显式列表 分别用于<code>a</code>和<code>b</code></p>
</li>
</ul>
<p>当使用整数参数<code>dims</code> = <img alt="" src="../img/95d07e221a9b945feb93422a44ac3d42.jpg" />调用并且<code>a</code>和<code>b</code>的维数分别为<img alt="" src="../img/03327e7b697db30918e96b2209927929.jpg" />和<img alt="" src="../img/5f0051b0454fa75ca446b59b47eff6f6.jpg" />时，它将计算</p>
<p><img alt="" src="../img/1a4ce18109f30700be1f1dea87661f3e.jpg" /></p>
<p>当使用列表形式的<code>dims</code>调用时，给定的尺寸将代替<code>a</code>的最后一个<img alt="" src="../img/95d07e221a9b945feb93422a44ac3d42.jpg" />和<img alt="" src="../img/23a2cc697e4e1a8adeb60eb42ab51a6e.jpg" />的第一个<img alt="" src="../img/95d07e221a9b945feb93422a44ac3d42.jpg" />收缩。 这些尺寸的尺寸必须匹配，但是 <a href="#torch.tensordot" title="torch.tensordot"><code>tensordot</code></a> 将处理广播的尺寸。</p>
<p>Examples:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-446-1" name="__codelineno-446-1" href="#__codelineno-446-1"></a>&gt;&gt;&gt; a = torch.arange(60.).reshape(3, 4, 5)
<a id="__codelineno-446-2" name="__codelineno-446-2" href="#__codelineno-446-2"></a>&gt;&gt;&gt; b = torch.arange(24.).reshape(4, 3, 2)
<a id="__codelineno-446-3" name="__codelineno-446-3" href="#__codelineno-446-3"></a>&gt;&gt;&gt; torch.tensordot(a, b, dims=([1, 0], [0, 1]))
<a id="__codelineno-446-4" name="__codelineno-446-4" href="#__codelineno-446-4"></a>tensor([[4400., 4730.],
<a id="__codelineno-446-5" name="__codelineno-446-5" href="#__codelineno-446-5"></a>        [4532., 4874.],
<a id="__codelineno-446-6" name="__codelineno-446-6" href="#__codelineno-446-6"></a>        [4664., 5018.],
<a id="__codelineno-446-7" name="__codelineno-446-7" href="#__codelineno-446-7"></a>        [4796., 5162.],
<a id="__codelineno-446-8" name="__codelineno-446-8" href="#__codelineno-446-8"></a>        [4928., 5306.]])
<a id="__codelineno-446-9" name="__codelineno-446-9" href="#__codelineno-446-9"></a>
<a id="__codelineno-446-10" name="__codelineno-446-10" href="#__codelineno-446-10"></a>&gt;&gt;&gt; a = torch.randn(3, 4, 5, device=&#39;cuda&#39;)
<a id="__codelineno-446-11" name="__codelineno-446-11" href="#__codelineno-446-11"></a>&gt;&gt;&gt; b = torch.randn(4, 5, 6, device=&#39;cuda&#39;)
<a id="__codelineno-446-12" name="__codelineno-446-12" href="#__codelineno-446-12"></a>&gt;&gt;&gt; c = torch.tensordot(a, b, dims=2).cpu()
<a id="__codelineno-446-13" name="__codelineno-446-13" href="#__codelineno-446-13"></a>tensor([[ 8.3504, -2.5436,  6.2922,  2.7556, -1.0732,  3.2741],
<a id="__codelineno-446-14" name="__codelineno-446-14" href="#__codelineno-446-14"></a>        [ 3.3161,  0.0704,  5.0187, -0.4079, -4.3126,  4.8744],
<a id="__codelineno-446-15" name="__codelineno-446-15" href="#__codelineno-446-15"></a>        [ 0.8223,  3.9445,  3.2168, -0.2400,  3.4117,  1.7780]])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-447-1" name="__codelineno-447-1" href="#__codelineno-447-1"></a>torch.trace(input) → Tensor¶
</code></pre></div>
<p>返回输入二维矩阵对角线元素的总和。</p>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-448-1" name="__codelineno-448-1" href="#__codelineno-448-1"></a>&gt;&gt;&gt; x = torch.arange(1., 10.).view(3, 3)
<a id="__codelineno-448-2" name="__codelineno-448-2" href="#__codelineno-448-2"></a>&gt;&gt;&gt; x
<a id="__codelineno-448-3" name="__codelineno-448-3" href="#__codelineno-448-3"></a>tensor([[ 1.,  2.,  3.],
<a id="__codelineno-448-4" name="__codelineno-448-4" href="#__codelineno-448-4"></a>        [ 4.,  5.,  6.],
<a id="__codelineno-448-5" name="__codelineno-448-5" href="#__codelineno-448-5"></a>        [ 7.,  8.,  9.]])
<a id="__codelineno-448-6" name="__codelineno-448-6" href="#__codelineno-448-6"></a>&gt;&gt;&gt; torch.trace(x)
<a id="__codelineno-448-7" name="__codelineno-448-7" href="#__codelineno-448-7"></a>tensor(15.)
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-449-1" name="__codelineno-449-1" href="#__codelineno-449-1"></a>torch.tril(input, diagonal=0, out=None) → Tensor¶
</code></pre></div>
<p>返回矩阵(2-D 张量）或矩阵批次<code>input</code>的下三角部分，结果张量<code>out</code>的其他元素设置为 0。</p>
<p>矩阵的下三角部分定义为对角线之上和之下的元素。</p>
<p>参数 <a href="#torch.diagonal" title="torch.diagonal"><code>diagonal</code></a> 控制要考虑的对角线。 如果 <a href="#torch.diagonal" title="torch.diagonal"><code>diagonal</code></a> = 0，则保留主对角线上和下方的所有元素。 正值包括在主对角线上方的对角线，同样，负值排除在主对角线下方的对角线。 主对角线是<img alt="" src="../img/f1dac6cf1b08b41a3c94272830a3eaba.jpg" />的索引集<img alt="" src="../img/efdb6cba2dff4626df6ecbd8c9117b62.jpg" />，其中<img alt="" src="../img/a760ba0f4e82b4f6875dd8b8db1bd3c4.jpg" />是矩阵的维数。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>diagonal</strong> (<em>python:int__,</em> <em>optional</em>) – the diagonal to consider</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-450-1" name="__codelineno-450-1" href="#__codelineno-450-1"></a>&gt;&gt;&gt; a = torch.randn(3, 3)
<a id="__codelineno-450-2" name="__codelineno-450-2" href="#__codelineno-450-2"></a>&gt;&gt;&gt; a
<a id="__codelineno-450-3" name="__codelineno-450-3" href="#__codelineno-450-3"></a>tensor([[-1.0813, -0.8619,  0.7105],
<a id="__codelineno-450-4" name="__codelineno-450-4" href="#__codelineno-450-4"></a>        [ 0.0935,  0.1380,  2.2112],
<a id="__codelineno-450-5" name="__codelineno-450-5" href="#__codelineno-450-5"></a>        [-0.3409, -0.9828,  0.0289]])
<a id="__codelineno-450-6" name="__codelineno-450-6" href="#__codelineno-450-6"></a>&gt;&gt;&gt; torch.tril(a)
<a id="__codelineno-450-7" name="__codelineno-450-7" href="#__codelineno-450-7"></a>tensor([[-1.0813,  0.0000,  0.0000],
<a id="__codelineno-450-8" name="__codelineno-450-8" href="#__codelineno-450-8"></a>        [ 0.0935,  0.1380,  0.0000],
<a id="__codelineno-450-9" name="__codelineno-450-9" href="#__codelineno-450-9"></a>        [-0.3409, -0.9828,  0.0289]])
<a id="__codelineno-450-10" name="__codelineno-450-10" href="#__codelineno-450-10"></a>
<a id="__codelineno-450-11" name="__codelineno-450-11" href="#__codelineno-450-11"></a>&gt;&gt;&gt; b = torch.randn(4, 6)
<a id="__codelineno-450-12" name="__codelineno-450-12" href="#__codelineno-450-12"></a>&gt;&gt;&gt; b
<a id="__codelineno-450-13" name="__codelineno-450-13" href="#__codelineno-450-13"></a>tensor([[ 1.2219,  0.5653, -0.2521, -0.2345,  1.2544,  0.3461],
<a id="__codelineno-450-14" name="__codelineno-450-14" href="#__codelineno-450-14"></a>        [ 0.4785, -0.4477,  0.6049,  0.6368,  0.8775,  0.7145],
<a id="__codelineno-450-15" name="__codelineno-450-15" href="#__codelineno-450-15"></a>        [ 1.1502,  3.2716, -1.1243, -0.5413,  0.3615,  0.6864],
<a id="__codelineno-450-16" name="__codelineno-450-16" href="#__codelineno-450-16"></a>        [-0.0614, -0.7344, -1.3164, -0.7648, -1.4024,  0.0978]])
<a id="__codelineno-450-17" name="__codelineno-450-17" href="#__codelineno-450-17"></a>&gt;&gt;&gt; torch.tril(b, diagonal=1)
<a id="__codelineno-450-18" name="__codelineno-450-18" href="#__codelineno-450-18"></a>tensor([[ 1.2219,  0.5653,  0.0000,  0.0000,  0.0000,  0.0000],
<a id="__codelineno-450-19" name="__codelineno-450-19" href="#__codelineno-450-19"></a>        [ 0.4785, -0.4477,  0.6049,  0.0000,  0.0000,  0.0000],
<a id="__codelineno-450-20" name="__codelineno-450-20" href="#__codelineno-450-20"></a>        [ 1.1502,  3.2716, -1.1243, -0.5413,  0.0000,  0.0000],
<a id="__codelineno-450-21" name="__codelineno-450-21" href="#__codelineno-450-21"></a>        [-0.0614, -0.7344, -1.3164, -0.7648, -1.4024,  0.0000]])
<a id="__codelineno-450-22" name="__codelineno-450-22" href="#__codelineno-450-22"></a>&gt;&gt;&gt; torch.tril(b, diagonal=-1)
<a id="__codelineno-450-23" name="__codelineno-450-23" href="#__codelineno-450-23"></a>tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],
<a id="__codelineno-450-24" name="__codelineno-450-24" href="#__codelineno-450-24"></a>        [ 0.4785,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],
<a id="__codelineno-450-25" name="__codelineno-450-25" href="#__codelineno-450-25"></a>        [ 1.1502,  3.2716,  0.0000,  0.0000,  0.0000,  0.0000],
<a id="__codelineno-450-26" name="__codelineno-450-26" href="#__codelineno-450-26"></a>        [-0.0614, -0.7344, -1.3164,  0.0000,  0.0000,  0.0000]])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-451-1" name="__codelineno-451-1" href="#__codelineno-451-1"></a>torch.tril_indices(row, col, offset=0, dtype=torch.long, device=&#39;cpu&#39;, layout=torch.strided) → Tensor¶
</code></pre></div>
<p>返回 2*N 张量中<code>row</code>-<code>col</code>矩阵的下三角部分的索引，其中第一行包含所有索引的行坐标，第二行包含列坐标。 索引是根据行然后按列排序的。</p>
<p>The lower triangular part of the matrix is defined as the elements on and below the diagonal.</p>
<p>参数<code>offset</code>控制要考虑的对角线。 如果<code>offset</code> = 0，则保留主对角线上和下方的所有元素。 正值包括在主对角线上方的对角线，同样，负值排除在主对角线下方的对角线。 主要对角线是<img alt="" src="../img/f1dac6cf1b08b41a3c94272830a3eaba.jpg" />的索引集<img alt="" src="../img/efdb6cba2dff4626df6ecbd8c9117b62.jpg" />，其中<img alt="" src="../img/a760ba0f4e82b4f6875dd8b8db1bd3c4.jpg" />是矩阵的尺寸。</p>
<p>注意：在“ cuda”上运行时，行* col 必须小于<img alt="" src="../img/62e45bcf8fe738ca1f051a4f7e9a7845.jpg" />，以防止计算期间溢出。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>行</strong>(<code>int</code>）–二维矩阵中的行数。</p>
</li>
<li>
<p><strong>col</strong> (<code>int</code>）–二维矩阵中的列数。</p>
</li>
<li>
<p><strong>偏移量</strong>(<code>int</code>）–与主对角线的对角线偏移。 默认值：如果未提供，则为 0。</p>
</li>
<li>
<p><strong>dtype</strong>  (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a> ，可选）–返回张量的所需数据类型。 默认值：如果<code>None</code>，<code>torch.long</code>。</p>
</li>
<li>
<p><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) – the desired device of returned tensor. Default: if <code>None</code>, uses the current device for the default tensor type (see <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>). <code>device</code> will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.</p>
</li>
<li>
<p><strong>布局</strong> (<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code>torch.layout</code></a> ，可选）–当前仅支持<code>torch.strided</code>。</p>
</li>
</ul>
<div class="highlight"><pre><span></span><code><a id="__codelineno-452-1" name="__codelineno-452-1" href="#__codelineno-452-1"></a>Example::
</code></pre></div>
<div class="highlight"><pre><span></span><code><a id="__codelineno-453-1" name="__codelineno-453-1" href="#__codelineno-453-1"></a>&gt;&gt;&gt; a = torch.tril_indices(3, 3)
<a id="__codelineno-453-2" name="__codelineno-453-2" href="#__codelineno-453-2"></a>&gt;&gt;&gt; a
<a id="__codelineno-453-3" name="__codelineno-453-3" href="#__codelineno-453-3"></a>tensor([[0, 1, 1, 2, 2, 2],
<a id="__codelineno-453-4" name="__codelineno-453-4" href="#__codelineno-453-4"></a>        [0, 0, 1, 0, 1, 2]])
</code></pre></div>
<div class="highlight"><pre><span></span><code><a id="__codelineno-454-1" name="__codelineno-454-1" href="#__codelineno-454-1"></a>&gt;&gt;&gt; a = torch.tril_indices(4, 3, -1)
<a id="__codelineno-454-2" name="__codelineno-454-2" href="#__codelineno-454-2"></a>&gt;&gt;&gt; a
<a id="__codelineno-454-3" name="__codelineno-454-3" href="#__codelineno-454-3"></a>tensor([[1, 2, 2, 3, 3, 3],
<a id="__codelineno-454-4" name="__codelineno-454-4" href="#__codelineno-454-4"></a>        [0, 0, 1, 0, 1, 2]])
</code></pre></div>
<div class="highlight"><pre><span></span><code><a id="__codelineno-455-1" name="__codelineno-455-1" href="#__codelineno-455-1"></a>&gt;&gt;&gt; a = torch.tril_indices(4, 3, 1)
<a id="__codelineno-455-2" name="__codelineno-455-2" href="#__codelineno-455-2"></a>&gt;&gt;&gt; a
<a id="__codelineno-455-3" name="__codelineno-455-3" href="#__codelineno-455-3"></a>tensor([[0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3],
<a id="__codelineno-455-4" name="__codelineno-455-4" href="#__codelineno-455-4"></a>        [0, 1, 0, 1, 2, 0, 1, 2, 0, 1, 2]])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-456-1" name="__codelineno-456-1" href="#__codelineno-456-1"></a>torch.triu(input, diagonal=0, out=None) → Tensor¶
</code></pre></div>
<p>返回矩阵(2-D 张量）或矩阵批次<code>input</code>的上三角部分，结果张量<code>out</code>的其他元素设置为 0。</p>
<p>矩阵的上三角部分定义为对角线上方和上方的元素。</p>
<p>参数 <a href="#torch.diagonal" title="torch.diagonal"><code>diagonal</code></a> 控制要考虑的对角线。 如果 <a href="#torch.diagonal" title="torch.diagonal"><code>diagonal</code></a> = 0，则保留主对角线上和上方的所有元素。 正值排除主要对角线上方的对角线，同样，负值包括主要对角线下方的对角线。 主对角线是<img alt="" src="../img/f1dac6cf1b08b41a3c94272830a3eaba.jpg" />的索引集<img alt="" src="../img/efdb6cba2dff4626df6ecbd8c9117b62.jpg" />，其中<img alt="" src="../img/a760ba0f4e82b4f6875dd8b8db1bd3c4.jpg" />是矩阵的维数。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>diagonal</strong> (<em>python:int__,</em> <em>optional</em>) – the diagonal to consider</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-457-1" name="__codelineno-457-1" href="#__codelineno-457-1"></a>&gt;&gt;&gt; a = torch.randn(3, 3)
<a id="__codelineno-457-2" name="__codelineno-457-2" href="#__codelineno-457-2"></a>&gt;&gt;&gt; a
<a id="__codelineno-457-3" name="__codelineno-457-3" href="#__codelineno-457-3"></a>tensor([[ 0.2309,  0.5207,  2.0049],
<a id="__codelineno-457-4" name="__codelineno-457-4" href="#__codelineno-457-4"></a>        [ 0.2072, -1.0680,  0.6602],
<a id="__codelineno-457-5" name="__codelineno-457-5" href="#__codelineno-457-5"></a>        [ 0.3480, -0.5211, -0.4573]])
<a id="__codelineno-457-6" name="__codelineno-457-6" href="#__codelineno-457-6"></a>&gt;&gt;&gt; torch.triu(a)
<a id="__codelineno-457-7" name="__codelineno-457-7" href="#__codelineno-457-7"></a>tensor([[ 0.2309,  0.5207,  2.0049],
<a id="__codelineno-457-8" name="__codelineno-457-8" href="#__codelineno-457-8"></a>        [ 0.0000, -1.0680,  0.6602],
<a id="__codelineno-457-9" name="__codelineno-457-9" href="#__codelineno-457-9"></a>        [ 0.0000,  0.0000, -0.4573]])
<a id="__codelineno-457-10" name="__codelineno-457-10" href="#__codelineno-457-10"></a>&gt;&gt;&gt; torch.triu(a, diagonal=1)
<a id="__codelineno-457-11" name="__codelineno-457-11" href="#__codelineno-457-11"></a>tensor([[ 0.0000,  0.5207,  2.0049],
<a id="__codelineno-457-12" name="__codelineno-457-12" href="#__codelineno-457-12"></a>        [ 0.0000,  0.0000,  0.6602],
<a id="__codelineno-457-13" name="__codelineno-457-13" href="#__codelineno-457-13"></a>        [ 0.0000,  0.0000,  0.0000]])
<a id="__codelineno-457-14" name="__codelineno-457-14" href="#__codelineno-457-14"></a>&gt;&gt;&gt; torch.triu(a, diagonal=-1)
<a id="__codelineno-457-15" name="__codelineno-457-15" href="#__codelineno-457-15"></a>tensor([[ 0.2309,  0.5207,  2.0049],
<a id="__codelineno-457-16" name="__codelineno-457-16" href="#__codelineno-457-16"></a>        [ 0.2072, -1.0680,  0.6602],
<a id="__codelineno-457-17" name="__codelineno-457-17" href="#__codelineno-457-17"></a>        [ 0.0000, -0.5211, -0.4573]])
<a id="__codelineno-457-18" name="__codelineno-457-18" href="#__codelineno-457-18"></a>
<a id="__codelineno-457-19" name="__codelineno-457-19" href="#__codelineno-457-19"></a>&gt;&gt;&gt; b = torch.randn(4, 6)
<a id="__codelineno-457-20" name="__codelineno-457-20" href="#__codelineno-457-20"></a>&gt;&gt;&gt; b
<a id="__codelineno-457-21" name="__codelineno-457-21" href="#__codelineno-457-21"></a>tensor([[ 0.5876, -0.0794, -1.8373,  0.6654,  0.2604,  1.5235],
<a id="__codelineno-457-22" name="__codelineno-457-22" href="#__codelineno-457-22"></a>        [-0.2447,  0.9556, -1.2919,  1.3378, -0.1768, -1.0857],
<a id="__codelineno-457-23" name="__codelineno-457-23" href="#__codelineno-457-23"></a>        [ 0.4333,  0.3146,  0.6576, -1.0432,  0.9348, -0.4410],
<a id="__codelineno-457-24" name="__codelineno-457-24" href="#__codelineno-457-24"></a>        [-0.9888,  1.0679, -1.3337, -1.6556,  0.4798,  0.2830]])
<a id="__codelineno-457-25" name="__codelineno-457-25" href="#__codelineno-457-25"></a>&gt;&gt;&gt; torch.triu(b, diagonal=1)
<a id="__codelineno-457-26" name="__codelineno-457-26" href="#__codelineno-457-26"></a>tensor([[ 0.0000, -0.0794, -1.8373,  0.6654,  0.2604,  1.5235],
<a id="__codelineno-457-27" name="__codelineno-457-27" href="#__codelineno-457-27"></a>        [ 0.0000,  0.0000, -1.2919,  1.3378, -0.1768, -1.0857],
<a id="__codelineno-457-28" name="__codelineno-457-28" href="#__codelineno-457-28"></a>        [ 0.0000,  0.0000,  0.0000, -1.0432,  0.9348, -0.4410],
<a id="__codelineno-457-29" name="__codelineno-457-29" href="#__codelineno-457-29"></a>        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.4798,  0.2830]])
<a id="__codelineno-457-30" name="__codelineno-457-30" href="#__codelineno-457-30"></a>&gt;&gt;&gt; torch.triu(b, diagonal=-1)
<a id="__codelineno-457-31" name="__codelineno-457-31" href="#__codelineno-457-31"></a>tensor([[ 0.5876, -0.0794, -1.8373,  0.6654,  0.2604,  1.5235],
<a id="__codelineno-457-32" name="__codelineno-457-32" href="#__codelineno-457-32"></a>        [-0.2447,  0.9556, -1.2919,  1.3378, -0.1768, -1.0857],
<a id="__codelineno-457-33" name="__codelineno-457-33" href="#__codelineno-457-33"></a>        [ 0.0000,  0.3146,  0.6576, -1.0432,  0.9348, -0.4410],
<a id="__codelineno-457-34" name="__codelineno-457-34" href="#__codelineno-457-34"></a>        [ 0.0000,  0.0000, -1.3337, -1.6556,  0.4798,  0.2830]])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-458-1" name="__codelineno-458-1" href="#__codelineno-458-1"></a>torch.triu_indices(row, col, offset=0, dtype=torch.long, device=&#39;cpu&#39;, layout=torch.strided) → Tensor¶
</code></pre></div>
<p>返回 2*N 张量中<code>row</code> x <code>col</code>矩阵的上三角部分的索引，其中第一行包含所有索引的行坐标，第二行包含列坐标。 索引是根据行然后按列排序的。</p>
<p>The upper triangular part of the matrix is defined as the elements on and above the diagonal.</p>
<p>参数<code>offset</code>控制要考虑的对角线。 如果<code>offset</code> = 0，则保留主对角线上和上方的所有元素。 正值排除主要对角线上方的对角线，同样，负值包括主要对角线下方的对角线。 主要对角线是<img alt="" src="../img/f1dac6cf1b08b41a3c94272830a3eaba.jpg" />的索引集<img alt="" src="../img/efdb6cba2dff4626df6ecbd8c9117b62.jpg" />，其中<img alt="" src="../img/a760ba0f4e82b4f6875dd8b8db1bd3c4.jpg" />是矩阵的尺寸。</p>
<p>NOTE: when running on 'cuda', row * col must be less than <img alt="" src="../img/62e45bcf8fe738ca1f051a4f7e9a7845.jpg" /> to prevent overflow during calculation.</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>row</strong> (<code>int</code>) – number of rows in the 2-D matrix.</p>
</li>
<li>
<p><strong>col</strong> (<code>int</code>) – number of columns in the 2-D matrix.</p>
</li>
<li>
<p><strong>offset</strong> (<code>int</code>) – diagonal offset from the main diagonal. Default: if not provided, 0.</p>
</li>
<li>
<p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) – the desired data type of returned tensor. Default: if <code>None</code>, <code>torch.long</code>.</p>
</li>
<li>
<p><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) – the desired device of returned tensor. Default: if <code>None</code>, uses the current device for the default tensor type (see <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>). <code>device</code> will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.</p>
</li>
<li>
<p><strong>layout</strong> (<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code>torch.layout</code></a>, optional) – currently only support <code>torch.strided</code>.</p>
</li>
</ul>
<div class="highlight"><pre><span></span><code><a id="__codelineno-459-1" name="__codelineno-459-1" href="#__codelineno-459-1"></a>Example::
</code></pre></div>
<div class="highlight"><pre><span></span><code><a id="__codelineno-460-1" name="__codelineno-460-1" href="#__codelineno-460-1"></a>&gt;&gt;&gt; a = torch.triu_indices(3, 3)
<a id="__codelineno-460-2" name="__codelineno-460-2" href="#__codelineno-460-2"></a>&gt;&gt;&gt; a
<a id="__codelineno-460-3" name="__codelineno-460-3" href="#__codelineno-460-3"></a>tensor([[0, 0, 0, 1, 1, 2],
<a id="__codelineno-460-4" name="__codelineno-460-4" href="#__codelineno-460-4"></a>        [0, 1, 2, 1, 2, 2]])
</code></pre></div>
<div class="highlight"><pre><span></span><code><a id="__codelineno-461-1" name="__codelineno-461-1" href="#__codelineno-461-1"></a>&gt;&gt;&gt; a = torch.triu_indices(4, 3, -1)
<a id="__codelineno-461-2" name="__codelineno-461-2" href="#__codelineno-461-2"></a>&gt;&gt;&gt; a
<a id="__codelineno-461-3" name="__codelineno-461-3" href="#__codelineno-461-3"></a>tensor([[0, 0, 0, 1, 1, 1, 2, 2, 3],
<a id="__codelineno-461-4" name="__codelineno-461-4" href="#__codelineno-461-4"></a>        [0, 1, 2, 0, 1, 2, 1, 2, 2]])
</code></pre></div>
<div class="highlight"><pre><span></span><code><a id="__codelineno-462-1" name="__codelineno-462-1" href="#__codelineno-462-1"></a>&gt;&gt;&gt; a = torch.triu_indices(4, 3, 1)
<a id="__codelineno-462-2" name="__codelineno-462-2" href="#__codelineno-462-2"></a>&gt;&gt;&gt; a
<a id="__codelineno-462-3" name="__codelineno-462-3" href="#__codelineno-462-3"></a>tensor([[0, 0, 1],
<a id="__codelineno-462-4" name="__codelineno-462-4" href="#__codelineno-462-4"></a>        [1, 2, 2]])
</code></pre></div>
<h3 id="blas-lapack">BLAS 和 LAPACK 操作</h3>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-463-1" name="__codelineno-463-1" href="#__codelineno-463-1"></a>torch.addbmm(beta=1, input, alpha=1, batch1, batch2, out=None) → Tensor¶
</code></pre></div>
<p>执行存储在<code>batch1</code>和<code>batch2</code>中的矩阵的批矩阵矩阵乘积，并减少加法步骤(所有矩阵乘法沿第一维累积）。 <code>input</code>被添加到最终结果中。</p>
<p><code>batch1</code>和<code>batch2</code>必须是 3D 张量，每个张量包含相同数量的矩阵。</p>
<p>如果<code>batch1</code>是<img alt="" src="../img/6bcc61944ebb068b264d492e9d3a9e9c.jpg" />张量，<code>batch2</code>是<img alt="" src="../img/30e4202e8cde7f489d43fdfbf59e0c46.jpg" />张量，则<code>input</code>必须是<a href="notes/broadcasting.html#broadcasting-semantics">可广播的</a>，带有<img alt="" src="../img/80b2c8de6d028b93a22dfe571079ee9c.jpg" />张量，而<code>out</code>将是<img alt="" src="../img/80b2c8de6d028b93a22dfe571079ee9c.jpg" />张量。 。</p>
<p><img alt="" src="../img/2c2652d014606e2ab1a85bdd3c257494.jpg" /></p>
<p>对于类型为 &lt;cite&gt;FloatTensor&lt;/cite&gt; 或 &lt;cite&gt;DoubleTensor&lt;/cite&gt; 的输入，参数<code>beta</code>和<code>alpha</code>必须为实数，否则应为整数。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>beta</strong> (<em>数字</em> <em>，</em> <em>可选</em>）– <code>input</code>(<img alt="" src="../img/53a496ec7d546e2af9595a7055dd6a7e.jpg" />）的乘数</p>
</li>
<li>
<p><strong>输入</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–要添加的矩阵</p>
</li>
<li>
<p><strong>alpha</strong> (<em>编号</em> <em>，</em> <em>可选</em>）– &lt;cite&gt;batch1 @ batch2&lt;/cite&gt; (<img alt="" src="../img/5b9866fb35b01c553ed3e738e3972ae9.jpg" />）的乘数</p>
</li>
<li>
<p><strong>batch1</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–第一批要相乘的矩阵</p>
</li>
<li>
<p><strong>batch2</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–要相乘的第二批矩阵</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-464-1" name="__codelineno-464-1" href="#__codelineno-464-1"></a>&gt;&gt;&gt; M = torch.randn(3, 5)
<a id="__codelineno-464-2" name="__codelineno-464-2" href="#__codelineno-464-2"></a>&gt;&gt;&gt; batch1 = torch.randn(10, 3, 4)
<a id="__codelineno-464-3" name="__codelineno-464-3" href="#__codelineno-464-3"></a>&gt;&gt;&gt; batch2 = torch.randn(10, 4, 5)
<a id="__codelineno-464-4" name="__codelineno-464-4" href="#__codelineno-464-4"></a>&gt;&gt;&gt; torch.addbmm(M, batch1, batch2)
<a id="__codelineno-464-5" name="__codelineno-464-5" href="#__codelineno-464-5"></a>tensor([[  6.6311,   0.0503,   6.9768, -12.0362,  -2.1653],
<a id="__codelineno-464-6" name="__codelineno-464-6" href="#__codelineno-464-6"></a>        [ -4.8185,  -1.4255,  -6.6760,   8.9453,   2.5743],
<a id="__codelineno-464-7" name="__codelineno-464-7" href="#__codelineno-464-7"></a>        [ -3.8202,   4.3691,   1.0943,  -1.1109,   5.4730]])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-465-1" name="__codelineno-465-1" href="#__codelineno-465-1"></a>torch.addmm(beta=1, input, alpha=1, mat1, mat2, out=None) → Tensor¶
</code></pre></div>
<p>对矩阵<code>mat1</code>和<code>mat2</code>进行矩阵乘法。 矩阵<code>input</code>被添加到最终结果中。</p>
<p>如果<code>mat1</code>是<img alt="" src="../img/95760d62046dcfa418c3b7ffea4caefc.jpg" />张量，<code>mat2</code>是<img alt="" src="../img/d145985cd9c2b23f68a55b0d5429c2ac.jpg" />张量，那么<code>input</code>必须是<a href="notes/broadcasting.html#broadcasting-semantics">可广播的</a>，带有<img alt="" src="../img/80b2c8de6d028b93a22dfe571079ee9c.jpg" />张量，而<code>out</code>将是<img alt="" src="../img/80b2c8de6d028b93a22dfe571079ee9c.jpg" /> 张量。</p>
<p><code>alpha</code>和<code>beta</code>分别是<code>mat1</code>和<code>mat2</code>与添加的矩阵<code>input</code>之间的矩阵向量乘积的比例因子。</p>
<p><img alt="" src="../img/9acb7c0c73f7a2456fcd58168e607d32.jpg" /></p>
<p>对于类型为 &lt;cite&gt;FloatTensor&lt;/cite&gt; 或 &lt;cite&gt;DoubleTensor&lt;/cite&gt; 的输入，参数<code>beta</code>和<code>alpha</code>必须为实数，否则应为整数。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>beta</strong> (<em>Number__,</em> <em>optional</em>) – multiplier for <code>input</code> (<img alt="" src="../img/53a496ec7d546e2af9595a7055dd6a7e.jpg" />)</p>
</li>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – matrix to be added</p>
</li>
<li>
<p><strong>alpha</strong> (<em>编号</em> <em>，</em> <em>可选</em>）– <img alt="" src="../img/6a8a2e3415f1a12e8299e3d32d94728d.jpg" />(<img alt="" src="../img/5b9866fb35b01c553ed3e738e3972ae9.jpg" />）的乘数</p>
</li>
<li>
<p><strong>mat1</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–要相乘的第一个矩阵</p>
</li>
<li>
<p><strong>mat2</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–要相乘的第二个矩阵</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-466-1" name="__codelineno-466-1" href="#__codelineno-466-1"></a>&gt;&gt;&gt; M = torch.randn(2, 3)
<a id="__codelineno-466-2" name="__codelineno-466-2" href="#__codelineno-466-2"></a>&gt;&gt;&gt; mat1 = torch.randn(2, 3)
<a id="__codelineno-466-3" name="__codelineno-466-3" href="#__codelineno-466-3"></a>&gt;&gt;&gt; mat2 = torch.randn(3, 3)
<a id="__codelineno-466-4" name="__codelineno-466-4" href="#__codelineno-466-4"></a>&gt;&gt;&gt; torch.addmm(M, mat1, mat2)
<a id="__codelineno-466-5" name="__codelineno-466-5" href="#__codelineno-466-5"></a>tensor([[-4.8716,  1.4671, -1.3746],
<a id="__codelineno-466-6" name="__codelineno-466-6" href="#__codelineno-466-6"></a>        [ 0.7573, -3.9555, -2.8681]])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-467-1" name="__codelineno-467-1" href="#__codelineno-467-1"></a>torch.addmv(beta=1, input, alpha=1, mat, vec, out=None) → Tensor¶
</code></pre></div>
<p>执行矩阵<code>mat</code>与向量<code>vec</code>的矩阵向量积。 向量<code>input</code>被添加到最终结果中。</p>
<p>如果<code>mat</code>是<img alt="" src="../img/95760d62046dcfa418c3b7ffea4caefc.jpg" />张量，<code>vec</code>是大小 &lt;cite&gt;m&lt;/cite&gt; 的一维张量，则<code>input</code>必须是<a href="notes/broadcasting.html#broadcasting-semantics">可广播</a>，且一维张量为 &lt;cite&gt;n&lt;/cite&gt; 和<code>out</code>大小将是 &lt;cite&gt;n&lt;/cite&gt; 大小的一维张量。</p>
<p><code>alpha</code>和<code>beta</code>分别是<code>mat</code>和<code>vec</code>与添加的张量<code>input</code>之间的矩阵向量乘积的比例因子。</p>
<p><img alt="" src="../img/c267729def81e95d252835fd4b3c0885.jpg" /></p>
<p>对于类型为 &lt;cite&gt;FloatTensor&lt;/cite&gt; 或 &lt;cite&gt;DoubleTensor&lt;/cite&gt; 的输入，参数<code>beta</code>和<code>alpha</code>必须为实数，否则应为整数</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>beta</strong> (<em>Number__,</em> <em>optional</em>) – multiplier for <code>input</code> (<img alt="" src="../img/53a496ec7d546e2af9595a7055dd6a7e.jpg" />)</p>
</li>
<li>
<p><strong>输入</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–要添加的向量</p>
</li>
<li>
<p><strong>alpha</strong> (<em>编号</em> <em>，</em> <em>可选</em>）– <img alt="" src="../img/7906de3cf6f1147a41fa843b63151dce.jpg" />(<img alt="" src="../img/5b9866fb35b01c553ed3e738e3972ae9.jpg" />）的乘数</p>
</li>
<li>
<p><strong>垫</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–要相乘的矩阵</p>
</li>
<li>
<p><strong>vec</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–要相乘的向量</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-468-1" name="__codelineno-468-1" href="#__codelineno-468-1"></a>&gt;&gt;&gt; M = torch.randn(2)
<a id="__codelineno-468-2" name="__codelineno-468-2" href="#__codelineno-468-2"></a>&gt;&gt;&gt; mat = torch.randn(2, 3)
<a id="__codelineno-468-3" name="__codelineno-468-3" href="#__codelineno-468-3"></a>&gt;&gt;&gt; vec = torch.randn(3)
<a id="__codelineno-468-4" name="__codelineno-468-4" href="#__codelineno-468-4"></a>&gt;&gt;&gt; torch.addmv(M, mat, vec)
<a id="__codelineno-468-5" name="__codelineno-468-5" href="#__codelineno-468-5"></a>tensor([-0.3768, -5.5565])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-469-1" name="__codelineno-469-1" href="#__codelineno-469-1"></a>torch.addr(beta=1, input, alpha=1, vec1, vec2, out=None) → Tensor¶
</code></pre></div>
<p>执行向量<code>vec1</code>和<code>vec2</code>的外积并将其添加到矩阵<code>input</code>中。</p>
<p>可选值<code>beta</code>和<code>alpha</code>分别是<code>vec1</code>和<code>vec2</code>与添加矩阵<code>input</code>之间的外部乘积的比例因子。</p>
<p><img alt="" src="../img/d41c12ed61e56a7f779894121105461d.jpg" /></p>
<p>如果<code>vec1</code>是 &lt;cite&gt;n&lt;/cite&gt; 大小的向量，而<code>vec2</code>是 &lt;cite&gt;m&lt;/cite&gt; 大小的向量，则<code>input</code>必须是<a href="notes/broadcasting.html#broadcasting-semantics">可广播</a>且矩阵为 <img alt="" src="../img/95760d62046dcfa418c3b7ffea4caefc.jpg" />和<code>out</code>大小将是<img alt="" src="../img/95760d62046dcfa418c3b7ffea4caefc.jpg" />大小的矩阵。</p>
<p>For inputs of type &lt;cite&gt;FloatTensor&lt;/cite&gt; or &lt;cite&gt;DoubleTensor&lt;/cite&gt;, arguments <code>beta</code> and <code>alpha</code> must be real numbers, otherwise they should be integers</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>beta</strong> (<em>Number__,</em> <em>optional</em>) – multiplier for <code>input</code> (<img alt="" src="../img/53a496ec7d546e2af9595a7055dd6a7e.jpg" />)</p>
</li>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – matrix to be added</p>
</li>
<li>
<p><strong>alpha</strong> (<em>编号</em> <em>，</em> <em>可选</em>）– <img alt="" src="../img/8b4088eb8d72b6ef9fe78d97a3149b5b.jpg" />(<img alt="" src="../img/5b9866fb35b01c553ed3e738e3972ae9.jpg" />）的乘数</p>
</li>
<li>
<p><strong>vec1</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–外积的第一个向量</p>
</li>
<li>
<p><strong>vec2</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–外积的第二向量</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-470-1" name="__codelineno-470-1" href="#__codelineno-470-1"></a>&gt;&gt;&gt; vec1 = torch.arange(1., 4.)
<a id="__codelineno-470-2" name="__codelineno-470-2" href="#__codelineno-470-2"></a>&gt;&gt;&gt; vec2 = torch.arange(1., 3.)
<a id="__codelineno-470-3" name="__codelineno-470-3" href="#__codelineno-470-3"></a>&gt;&gt;&gt; M = torch.zeros(3, 2)
<a id="__codelineno-470-4" name="__codelineno-470-4" href="#__codelineno-470-4"></a>&gt;&gt;&gt; torch.addr(M, vec1, vec2)
<a id="__codelineno-470-5" name="__codelineno-470-5" href="#__codelineno-470-5"></a>tensor([[ 1.,  2.],
<a id="__codelineno-470-6" name="__codelineno-470-6" href="#__codelineno-470-6"></a>        [ 2.,  4.],
<a id="__codelineno-470-7" name="__codelineno-470-7" href="#__codelineno-470-7"></a>        [ 3.,  6.]])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-471-1" name="__codelineno-471-1" href="#__codelineno-471-1"></a>torch.baddbmm(beta=1, input, alpha=1, batch1, batch2, out=None) → Tensor¶
</code></pre></div>
<p>在<code>batch1</code>和<code>batch2</code>中执行矩阵的批处理矩阵矩阵乘积。 <code>input</code>被添加到最终结果中。</p>
<p><code>batch1</code>和<code>batch2</code>必须是 3D 张量，每个张量包含相同数量的矩阵。</p>
<p>如果<code>batch1</code>是<img alt="" src="../img/6bcc61944ebb068b264d492e9d3a9e9c.jpg" />张量，<code>batch2</code>是<img alt="" src="../img/30e4202e8cde7f489d43fdfbf59e0c46.jpg" />张量，那么<code>input</code>必须是<a href="notes/broadcasting.html#broadcasting-semantics">可广播的</a>，带有<img alt="" src="../img/f827b00456c04e393fa94ba4df1c7e08.jpg" />张量，而<code>out</code>将是<img alt="" src="../img/f827b00456c04e393fa94ba4df1c7e08.jpg" /> 张量。 <code>alpha</code>和<code>beta</code>的含义均与 <a href="#torch.addbmm" title="torch.addbmm"><code>torch.addbmm()</code></a> 中使用的缩放因子相同。</p>
<p><img alt="" src="../img/63aed732a8b14d4e5804e77ade6d9340.jpg" /></p>
<p>For inputs of type &lt;cite&gt;FloatTensor&lt;/cite&gt; or &lt;cite&gt;DoubleTensor&lt;/cite&gt;, arguments <code>beta</code> and <code>alpha</code> must be real numbers, otherwise they should be integers.</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>beta</strong> (<em>Number__,</em> <em>optional</em>) – multiplier for <code>input</code> (<img alt="" src="../img/53a496ec7d546e2af9595a7055dd6a7e.jpg" />)</p>
</li>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the tensor to be added</p>
</li>
<li>
<p><strong>alpha</strong> (<em>编号</em> <em>，</em> <em>可选</em>）– <img alt="" src="../img/7b5e33d60c908d236e66d3aba3044f4f.jpg" />(<img alt="" src="../img/5b9866fb35b01c553ed3e738e3972ae9.jpg" />）的乘数</p>
</li>
<li>
<p><strong>batch1</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the first batch of matrices to be multiplied</p>
</li>
<li>
<p><strong>batch2</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the second batch of matrices to be multiplied</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-472-1" name="__codelineno-472-1" href="#__codelineno-472-1"></a>&gt;&gt;&gt; M = torch.randn(10, 3, 5)
<a id="__codelineno-472-2" name="__codelineno-472-2" href="#__codelineno-472-2"></a>&gt;&gt;&gt; batch1 = torch.randn(10, 3, 4)
<a id="__codelineno-472-3" name="__codelineno-472-3" href="#__codelineno-472-3"></a>&gt;&gt;&gt; batch2 = torch.randn(10, 4, 5)
<a id="__codelineno-472-4" name="__codelineno-472-4" href="#__codelineno-472-4"></a>&gt;&gt;&gt; torch.baddbmm(M, batch1, batch2).size()
<a id="__codelineno-472-5" name="__codelineno-472-5" href="#__codelineno-472-5"></a>torch.Size([10, 3, 5])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-473-1" name="__codelineno-473-1" href="#__codelineno-473-1"></a>torch.bmm(input, mat2, out=None) → Tensor¶
</code></pre></div>
<p>对<code>input</code>和<code>mat2</code>中存储的矩阵执行批处理矩阵矩阵乘积。</p>
<p><code>input</code>和<code>mat2</code>必须是 3D 张量，每个张量包含相同数量的矩阵。</p>
<p>如果<code>input</code>是<img alt="" src="../img/6bcc61944ebb068b264d492e9d3a9e9c.jpg" />张量，<code>mat2</code>是<img alt="" src="../img/30e4202e8cde7f489d43fdfbf59e0c46.jpg" />张量，<code>out</code>将是<img alt="" src="../img/f827b00456c04e393fa94ba4df1c7e08.jpg" />张量。</p>
<p><img alt="" src="../img/582f347178ea996d728f2eb71d21ae1d.jpg" /></p>
<p>Note</p>
<p>该功能不<a href="notes/broadcasting.html#broadcasting-semantics">广播</a>。 有关广播矩阵产品，请参见 <a href="#torch.matmul" title="torch.matmul"><code>torch.matmul()</code></a> 。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>输入</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–要相乘的第一批矩阵</p>
</li>
<li>
<p><strong>mat2</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–第二批矩阵相乘</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-474-1" name="__codelineno-474-1" href="#__codelineno-474-1"></a>&gt;&gt;&gt; input = torch.randn(10, 3, 4)
<a id="__codelineno-474-2" name="__codelineno-474-2" href="#__codelineno-474-2"></a>&gt;&gt;&gt; mat2 = torch.randn(10, 4, 5)
<a id="__codelineno-474-3" name="__codelineno-474-3" href="#__codelineno-474-3"></a>&gt;&gt;&gt; res = torch.bmm(input, mat2)
<a id="__codelineno-474-4" name="__codelineno-474-4" href="#__codelineno-474-4"></a>&gt;&gt;&gt; res.size()
<a id="__codelineno-474-5" name="__codelineno-474-5" href="#__codelineno-474-5"></a>torch.Size([10, 3, 5])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-475-1" name="__codelineno-475-1" href="#__codelineno-475-1"></a>torch.chain_matmul(*matrices)¶
</code></pre></div>
<p>返回<img alt="" src="../img/7ea0d6aa290d37c8bc57957206bafe80.jpg" /> 2-D 张量的矩阵乘积。 使用矩阵链顺序算法可以有效地计算该乘积，该算法选择以算术运算 (<a href="https://mitpress.mit.edu/books/introduction-algorithms-third-edition">[CLRS]</a>)产生最低成本的顺序。 注意，由于这是一个计算乘积的函数，因此<img alt="" src="../img/7ea0d6aa290d37c8bc57957206bafe80.jpg" />必须大于或等于 2；因此，<img alt="" src="../img/7ea0d6aa290d37c8bc57957206bafe80.jpg" />必须大于或等于 2。 如果等于 2，则返回平凡的矩阵矩阵乘积。 如果<img alt="" src="../img/7ea0d6aa290d37c8bc57957206bafe80.jpg" />为 1，则为空操作-原始矩阵按原样返回。</p>
<p>Parameters</p>
<p><strong>矩阵</strong>(<em>张量...</em> )–由 2 个或多个 2D 张量确定其乘积的序列。</p>
<p>Returns</p>
<p>如果<img alt="" src="../img/10b8c0924577b4e47a4e8b6537a6ac9a.jpg" />张量的尺寸为<img alt="" src="../img/9342f0f16c0cf8e65f9e7baf672bc083.jpg" />，则乘积将为尺寸<img alt="" src="../img/169badb20d1f38fab1c101fc7c5c674f.jpg" />。</p>
<p>Return type</p>
<p><a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-476-1" name="__codelineno-476-1" href="#__codelineno-476-1"></a>&gt;&gt;&gt; a = torch.randn(3, 4)
<a id="__codelineno-476-2" name="__codelineno-476-2" href="#__codelineno-476-2"></a>&gt;&gt;&gt; b = torch.randn(4, 5)
<a id="__codelineno-476-3" name="__codelineno-476-3" href="#__codelineno-476-3"></a>&gt;&gt;&gt; c = torch.randn(5, 6)
<a id="__codelineno-476-4" name="__codelineno-476-4" href="#__codelineno-476-4"></a>&gt;&gt;&gt; d = torch.randn(6, 7)
<a id="__codelineno-476-5" name="__codelineno-476-5" href="#__codelineno-476-5"></a>&gt;&gt;&gt; torch.chain_matmul(a, b, c, d)
<a id="__codelineno-476-6" name="__codelineno-476-6" href="#__codelineno-476-6"></a>tensor([[ -2.3375,  -3.9790,  -4.1119,  -6.6577,   9.5609, -11.5095,  -3.2614],
<a id="__codelineno-476-7" name="__codelineno-476-7" href="#__codelineno-476-7"></a>        [ 21.4038,   3.3378,  -8.4982,  -5.2457, -10.2561,  -2.4684,   2.7163],
<a id="__codelineno-476-8" name="__codelineno-476-8" href="#__codelineno-476-8"></a>        [ -0.9647,  -5.8917,  -2.3213,  -5.2284,  12.8615, -12.2816,  -2.5095]])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-477-1" name="__codelineno-477-1" href="#__codelineno-477-1"></a>torch.cholesky(input, upper=False, out=None) → Tensor¶
</code></pre></div>
<p>计算对称正定矩阵<img alt="" src="../img/8cab287af6acaf0838d1d67381a3716d.jpg" />或一批对称正定矩阵的 Cholesky 分解。</p>
<p>如果<code>upper</code>为<code>True</code>，则返回的矩阵<code>U</code>为上三角，分解形式为：</p>
<p><img alt="" src="../img/917cbfcd41dc365c78ee1bb4651d5a3a.jpg" /></p>
<p>如果<code>upper</code>为<code>False</code>，则返回的矩阵<code>L</code>为下三角，分解形式为：</p>
<p><img alt="" src="../img/3f3a6cdf9c276b261c15da651aff430e.jpg" /></p>
<p>如果<code>upper</code>为<code>True</code>，并且<img alt="" src="../img/8cab287af6acaf0838d1d67381a3716d.jpg" />为一批对称的正定矩阵，则返回的张量将由各个矩阵的上三角 Cholesky 因子组成。 同样，当<code>upper</code>为<code>False</code>时，返回的张量将由每个单独矩阵的下三角 Cholesky 因子组成。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>输入</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–大小为<img alt="" src="../img/aa6a866e7977a9ee67a53687003d3821.jpg" />的输入张量<img alt="" src="../img/8cab287af6acaf0838d1d67381a3716d.jpg" />，其中 &lt;cite&gt;*&lt;/cite&gt; 为零或更多个批处理尺寸，包括 对称正定矩阵。</p>
</li>
<li>
<p><strong>上</strong> (<em>bool</em> <em>，</em> <em>可选</em>）–指示是否返回上三角矩阵或下三角矩阵的标志。 默认值：<code>False</code></p>
</li>
<li>
<p><strong>out</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a> <em>，</em> <em>可选</em>）–输出矩阵</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-478-1" name="__codelineno-478-1" href="#__codelineno-478-1"></a>&gt;&gt;&gt; a = torch.randn(3, 3)
<a id="__codelineno-478-2" name="__codelineno-478-2" href="#__codelineno-478-2"></a>&gt;&gt;&gt; a = torch.mm(a, a.t()) # make symmetric positive-definite
<a id="__codelineno-478-3" name="__codelineno-478-3" href="#__codelineno-478-3"></a>&gt;&gt;&gt; l = torch.cholesky(a)
<a id="__codelineno-478-4" name="__codelineno-478-4" href="#__codelineno-478-4"></a>&gt;&gt;&gt; a
<a id="__codelineno-478-5" name="__codelineno-478-5" href="#__codelineno-478-5"></a>tensor([[ 2.4112, -0.7486,  1.4551],
<a id="__codelineno-478-6" name="__codelineno-478-6" href="#__codelineno-478-6"></a>        [-0.7486,  1.3544,  0.1294],
<a id="__codelineno-478-7" name="__codelineno-478-7" href="#__codelineno-478-7"></a>        [ 1.4551,  0.1294,  1.6724]])
<a id="__codelineno-478-8" name="__codelineno-478-8" href="#__codelineno-478-8"></a>&gt;&gt;&gt; l
<a id="__codelineno-478-9" name="__codelineno-478-9" href="#__codelineno-478-9"></a>tensor([[ 1.5528,  0.0000,  0.0000],
<a id="__codelineno-478-10" name="__codelineno-478-10" href="#__codelineno-478-10"></a>        [-0.4821,  1.0592,  0.0000],
<a id="__codelineno-478-11" name="__codelineno-478-11" href="#__codelineno-478-11"></a>        [ 0.9371,  0.5487,  0.7023]])
<a id="__codelineno-478-12" name="__codelineno-478-12" href="#__codelineno-478-12"></a>&gt;&gt;&gt; torch.mm(l, l.t())
<a id="__codelineno-478-13" name="__codelineno-478-13" href="#__codelineno-478-13"></a>tensor([[ 2.4112, -0.7486,  1.4551],
<a id="__codelineno-478-14" name="__codelineno-478-14" href="#__codelineno-478-14"></a>        [-0.7486,  1.3544,  0.1294],
<a id="__codelineno-478-15" name="__codelineno-478-15" href="#__codelineno-478-15"></a>        [ 1.4551,  0.1294,  1.6724]])
<a id="__codelineno-478-16" name="__codelineno-478-16" href="#__codelineno-478-16"></a>&gt;&gt;&gt; a = torch.randn(3, 2, 2)
<a id="__codelineno-478-17" name="__codelineno-478-17" href="#__codelineno-478-17"></a>&gt;&gt;&gt; a = torch.matmul(a, a.transpose(-1, -2)) + 1e-03 # make symmetric positive-definite
<a id="__codelineno-478-18" name="__codelineno-478-18" href="#__codelineno-478-18"></a>&gt;&gt;&gt; l = torch.cholesky(a)
<a id="__codelineno-478-19" name="__codelineno-478-19" href="#__codelineno-478-19"></a>&gt;&gt;&gt; z = torch.matmul(l, l.transpose(-1, -2))
<a id="__codelineno-478-20" name="__codelineno-478-20" href="#__codelineno-478-20"></a>&gt;&gt;&gt; torch.max(torch.abs(z - a)) # Max non-zero
<a id="__codelineno-478-21" name="__codelineno-478-21" href="#__codelineno-478-21"></a>tensor(2.3842e-07)
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-479-1" name="__codelineno-479-1" href="#__codelineno-479-1"></a>torch.cholesky_inverse(input, upper=False, out=None) → Tensor¶
</code></pre></div>
<p>使用其 Cholesky 因子<img alt="" src="../img/83ebacfa515af65e0d7e683595edbde1.jpg" />计算对称正定矩阵<img alt="" src="../img/8cab287af6acaf0838d1d67381a3716d.jpg" />的逆：返回矩阵<code>inv</code>。 使用 LAPACK 例程<code>dpotri</code>和<code>spotri</code>(以及相应的 MAGMA 例程）计算逆。</p>
<p>如果<code>upper</code>为<code>False</code>，则<img alt="" src="../img/83ebacfa515af65e0d7e683595edbde1.jpg" />为下三角，这样返回的张量为</p>
<p><img alt="" src="../img/68e4a9138b1537d38f0e11634c0f068b.jpg" /></p>
<p>如果<code>upper</code>为<code>True</code>，或未提供，则<img alt="" src="../img/83ebacfa515af65e0d7e683595edbde1.jpg" />为上三角，使得返回的张量为</p>
<p><img alt="" src="../img/c4e7afdf52ea3475cb80b4d812b87a16.jpg" /></p>
<p>Parameters</p>
<ul>
<li>
<p><strong>输入</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–输入二维张量<img alt="" src="../img/83ebacfa515af65e0d7e683595edbde1.jpg" />，上或下三角 Cholesky 因子</p>
</li>
<li>
<p><strong>上部</strong> (<em>bool</em> <em>，</em> <em>可选</em>）–是否返回下部(默认）或上部三角矩阵</p>
</li>
<li>
<p><strong>out</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a> <em>，</em> <em>可选</em>）– &lt;cite&gt;inv&lt;/cite&gt; 的输出张量</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-480-1" name="__codelineno-480-1" href="#__codelineno-480-1"></a>&gt;&gt;&gt; a = torch.randn(3, 3)
<a id="__codelineno-480-2" name="__codelineno-480-2" href="#__codelineno-480-2"></a>&gt;&gt;&gt; a = torch.mm(a, a.t()) + 1e-05 * torch.eye(3) # make symmetric positive definite
<a id="__codelineno-480-3" name="__codelineno-480-3" href="#__codelineno-480-3"></a>&gt;&gt;&gt; u = torch.cholesky(a)
<a id="__codelineno-480-4" name="__codelineno-480-4" href="#__codelineno-480-4"></a>&gt;&gt;&gt; a
<a id="__codelineno-480-5" name="__codelineno-480-5" href="#__codelineno-480-5"></a>tensor([[  0.9935,  -0.6353,   1.5806],
<a id="__codelineno-480-6" name="__codelineno-480-6" href="#__codelineno-480-6"></a>        [ -0.6353,   0.8769,  -1.7183],
<a id="__codelineno-480-7" name="__codelineno-480-7" href="#__codelineno-480-7"></a>        [  1.5806,  -1.7183,  10.6618]])
<a id="__codelineno-480-8" name="__codelineno-480-8" href="#__codelineno-480-8"></a>&gt;&gt;&gt; torch.cholesky_inverse(u)
<a id="__codelineno-480-9" name="__codelineno-480-9" href="#__codelineno-480-9"></a>tensor([[ 1.9314,  1.2251, -0.0889],
<a id="__codelineno-480-10" name="__codelineno-480-10" href="#__codelineno-480-10"></a>        [ 1.2251,  2.4439,  0.2122],
<a id="__codelineno-480-11" name="__codelineno-480-11" href="#__codelineno-480-11"></a>        [-0.0889,  0.2122,  0.1412]])
<a id="__codelineno-480-12" name="__codelineno-480-12" href="#__codelineno-480-12"></a>&gt;&gt;&gt; a.inverse()
<a id="__codelineno-480-13" name="__codelineno-480-13" href="#__codelineno-480-13"></a>tensor([[ 1.9314,  1.2251, -0.0889],
<a id="__codelineno-480-14" name="__codelineno-480-14" href="#__codelineno-480-14"></a>        [ 1.2251,  2.4439,  0.2122],
<a id="__codelineno-480-15" name="__codelineno-480-15" href="#__codelineno-480-15"></a>        [-0.0889,  0.2122,  0.1412]])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-481-1" name="__codelineno-481-1" href="#__codelineno-481-1"></a>torch.cholesky_solve(input, input2, upper=False, out=None) → Tensor¶
</code></pre></div>
<p>给定其 Cholesky 因子矩阵<img alt="" src="../img/83ebacfa515af65e0d7e683595edbde1.jpg" />，以正半定矩阵解线性方程组。</p>
<p>如果<code>upper</code>为<code>False</code>，则<img alt="" src="../img/83ebacfa515af65e0d7e683595edbde1.jpg" />为且下部三角形，并且返回 &lt;cite&gt;c&lt;/cite&gt; 使得：</p>
<p><img alt="" src="../img/fbdb7ce9ef686ee96dd7242ca863ee3c.jpg" /></p>
<p>如果<code>upper</code>为<code>True</code>，则不提供<img alt="" src="../img/83ebacfa515af65e0d7e683595edbde1.jpg" />为上三角形，并且返回 &lt;cite&gt;c&lt;/cite&gt; ，使得：</p>
<p><img alt="" src="../img/b63d89883b7ac400b9b862e2f05738f7.jpg" /></p>
<p>&lt;cite&gt;torch.cholesky_solve(b，u）&lt;/cite&gt;可以接受 2D 输入 &lt;cite&gt;b，u&lt;/cite&gt; 或一批 2D 矩阵的输入。 如果输入为批次，则返回成批输出 &lt;cite&gt;c&lt;/cite&gt;</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>输入</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–大小为<img alt="" src="../img/8ebb9d35b764041761ddb4e3436e265d.jpg" />的输入矩阵<img alt="" src="../img/23a2cc697e4e1a8adeb60eb42ab51a6e.jpg" />，其中<img alt="" src="../img/dcef98688866c0d5a21137cf53bf228d.jpg" />为零或更大批处理尺寸</p>
</li>
<li>
<p><strong>input2</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–大小为<img alt="" src="../img/10ea424983a008dbf72dc86628a2813d.jpg" />的输入矩阵<img alt="" src="../img/83ebacfa515af65e0d7e683595edbde1.jpg" />，其中<img alt="" src="../img/dcef98688866c0d5a21137cf53bf228d.jpg" />为零个或多个由上或下三角组成的批处理尺寸 胆固醇系数</p>
</li>
<li>
<p><strong>上</strong> (<em>bool</em> <em>，</em> <em>可选</em>）–是否考虑将 Cholesky 因子视为下三角矩阵还是上三角矩阵。 默认值：<code>False</code>。</p>
</li>
<li>
<p><strong>输出</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a> <em>，</em> <em>可选</em>）– &lt;cite&gt;c&lt;/cite&gt; 的输出张量</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-482-1" name="__codelineno-482-1" href="#__codelineno-482-1"></a>&gt;&gt;&gt; a = torch.randn(3, 3)
<a id="__codelineno-482-2" name="__codelineno-482-2" href="#__codelineno-482-2"></a>&gt;&gt;&gt; a = torch.mm(a, a.t()) # make symmetric positive definite
<a id="__codelineno-482-3" name="__codelineno-482-3" href="#__codelineno-482-3"></a>&gt;&gt;&gt; u = torch.cholesky(a)
<a id="__codelineno-482-4" name="__codelineno-482-4" href="#__codelineno-482-4"></a>&gt;&gt;&gt; a
<a id="__codelineno-482-5" name="__codelineno-482-5" href="#__codelineno-482-5"></a>tensor([[ 0.7747, -1.9549,  1.3086],
<a id="__codelineno-482-6" name="__codelineno-482-6" href="#__codelineno-482-6"></a>        [-1.9549,  6.7546, -5.4114],
<a id="__codelineno-482-7" name="__codelineno-482-7" href="#__codelineno-482-7"></a>        [ 1.3086, -5.4114,  4.8733]])
<a id="__codelineno-482-8" name="__codelineno-482-8" href="#__codelineno-482-8"></a>&gt;&gt;&gt; b = torch.randn(3, 2)
<a id="__codelineno-482-9" name="__codelineno-482-9" href="#__codelineno-482-9"></a>&gt;&gt;&gt; b
<a id="__codelineno-482-10" name="__codelineno-482-10" href="#__codelineno-482-10"></a>tensor([[-0.6355,  0.9891],
<a id="__codelineno-482-11" name="__codelineno-482-11" href="#__codelineno-482-11"></a>        [ 0.1974,  1.4706],
<a id="__codelineno-482-12" name="__codelineno-482-12" href="#__codelineno-482-12"></a>        [-0.4115, -0.6225]])
<a id="__codelineno-482-13" name="__codelineno-482-13" href="#__codelineno-482-13"></a>&gt;&gt;&gt; torch.cholesky_solve(b, u)
<a id="__codelineno-482-14" name="__codelineno-482-14" href="#__codelineno-482-14"></a>tensor([[ -8.1625,  19.6097],
<a id="__codelineno-482-15" name="__codelineno-482-15" href="#__codelineno-482-15"></a>        [ -5.8398,  14.2387],
<a id="__codelineno-482-16" name="__codelineno-482-16" href="#__codelineno-482-16"></a>        [ -4.3771,  10.4173]])
<a id="__codelineno-482-17" name="__codelineno-482-17" href="#__codelineno-482-17"></a>&gt;&gt;&gt; torch.mm(a.inverse(), b)
<a id="__codelineno-482-18" name="__codelineno-482-18" href="#__codelineno-482-18"></a>tensor([[ -8.1626,  19.6097],
<a id="__codelineno-482-19" name="__codelineno-482-19" href="#__codelineno-482-19"></a>        [ -5.8398,  14.2387],
<a id="__codelineno-482-20" name="__codelineno-482-20" href="#__codelineno-482-20"></a>        [ -4.3771,  10.4173]])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-483-1" name="__codelineno-483-1" href="#__codelineno-483-1"></a>torch.dot(input, tensor) → Tensor¶
</code></pre></div>
<p>计算两个张量的点积(内积）。</p>
<p>Note</p>
<p>该功能不<a href="notes/broadcasting.html#broadcasting-semantics">广播</a>。</p>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-484-1" name="__codelineno-484-1" href="#__codelineno-484-1"></a>&gt;&gt;&gt; torch.dot(torch.tensor([2, 3]), torch.tensor([2, 1]))
<a id="__codelineno-484-2" name="__codelineno-484-2" href="#__codelineno-484-2"></a>tensor(7)
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-485-1" name="__codelineno-485-1" href="#__codelineno-485-1"></a>torch.eig(input, eigenvectors=False, out=None) -&gt; (Tensor, Tensor)¶
</code></pre></div>
<p>计算实方矩阵的特征值和特征向量。</p>
<p>Note</p>
<p>由于特征值和特征向量可能很复杂，因此仅 <a href="#torch.symeig" title="torch.symeig"><code>torch.symeig()</code></a> 支持反向传递</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>输入</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–形状为<img alt="" src="../img/c64d7c7677227e8b59da7d7cfb2466d8.jpg" />的方阵，将为其计算特征值和特征向量</p>
</li>
<li>
<p><strong>特征向量</strong> (<em>bool</em> )– <code>True</code>以计算特征值和特征向量； 否则，将仅计算特征值</p>
</li>
<li>
<p><strong>out</strong> (<em>元组</em> <em>，</em> <em>可选</em>）–输出张量</p>
</li>
</ul>
<p>Returns</p>
<p>包含的 namedtuple(特征值，特征向量）</p>
<blockquote>
<ul>
<li>
<p><strong>特征值</strong>(<em>tensor</em>）：形状<img alt="" src="../img/f274e719751329e2ef63ca92533b23da.jpg" />。 每行是<code>input</code>的特征值，其中第一个元素是实部，第二个元素是虚部。 特征值不一定是有序的。</p>
</li>
<li>
<p><strong>特征向量</strong>(<em>tensor</em>）：如果<code>eigenvectors=False</code>为空，则为张量。 否则，可以使用形状<img alt="" src="../img/c64d7c7677227e8b59da7d7cfb2466d8.jpg" />的张量来计算对应特征值的归一化(单位长度）特征向量，如下所示。 如果对应的&lt;cite&gt;特征值[j]&lt;/cite&gt; 是实数，则&lt;cite&gt;特征向量[：，j]&lt;/cite&gt; 列是与&lt;cite&gt;特征值[j]&lt;/cite&gt; 相对应的特征向量。 如果相应的&lt;cite&gt;特征值[j]&lt;/cite&gt; 和&lt;cite&gt;特征值[j + 1]&lt;/cite&gt; 形成复共轭对，则真实特征向量可以计算为<img alt="" src="../img/cf872428156c87dba5ec860dd64ff692.jpg" />，<img alt="" src="../img/3a205492a67d1307528fb311bfba7d2a.jpg" />。</p>
</li>
</ul>
</blockquote>
<p>Return type</p>
<p>(<a href="tensors.html#torch.Tensor" title="torch.Tensor">张量</a>，<a href="tensors.html#torch.Tensor" title="torch.Tensor">张量</a>）</p>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-486-1" name="__codelineno-486-1" href="#__codelineno-486-1"></a>torch.geqrf(input, out=None) -&gt; (Tensor, Tensor)¶
</code></pre></div>
<p>这是直接调用 LAPACK 的底层函数。 该函数返回[eqg0f] 的 <a href="https://software.intel.com/en-us/node/521004">LAPACK 文档中定义的 namedtuple(a，tau）。</a></p>
<p>通常，您通常要使用 <a href="#torch.qr" title="torch.qr"><code>torch.qr()</code></a> 。</p>
<p>计算<code>input</code>的 QR 分解，但不将<img alt="" src="../img/7f927fea3856ca4796aab74326229f61.jpg" />和<img alt="" src="../img/fd6855baddb0a56aeca293dd58a9758d.jpg" />构造为明确的单独矩阵。</p>
<p>而是直接调用基础的 LAPACK 函数&lt;cite&gt;？geqrf&lt;/cite&gt; ，该函数产生一系列“基本反射器”。</p>
<p>有关更多详细信息，请参见 geqrf 的 <a href="https://software.intel.com/en-us/node/521004">LAPACK 文档。</a></p>
<p>Parameters</p>
<ul>
<li>
<p><strong>输入</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–输入矩阵</p>
</li>
<li>
<p><strong>out</strong> (<em>元组</em> <em>，</em> <em>可选</em>）–(张量，张量）的输出元组</p>
</li>
</ul>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-487-1" name="__codelineno-487-1" href="#__codelineno-487-1"></a>torch.ger(input, vec2, out=None) → Tensor¶
</code></pre></div>
<p><code>input</code>和<code>vec2</code>的外部乘积。 如果<code>input</code>是大小为<img alt="" src="../img/5f0051b0454fa75ca446b59b47eff6f6.jpg" />的向量，而<code>vec2</code>是大小为<img alt="" src="../img/03327e7b697db30918e96b2209927929.jpg" />的向量，则<code>out</code>必须是大小为<img alt="" src="../img/95760d62046dcfa418c3b7ffea4caefc.jpg" />的矩阵。</p>
<p>Note</p>
<p>This function does not <a href="notes/broadcasting.html#broadcasting-semantics">broadcast</a>.</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>输入</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–一维输入向量</p>
</li>
<li>
<p><strong>vec2</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–一维输入向量</p>
</li>
<li>
<p><strong>输出</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a> <em>，</em> <em>可选</em>）–可选输出矩阵</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-488-1" name="__codelineno-488-1" href="#__codelineno-488-1"></a>&gt;&gt;&gt; v1 = torch.arange(1., 5.)
<a id="__codelineno-488-2" name="__codelineno-488-2" href="#__codelineno-488-2"></a>&gt;&gt;&gt; v2 = torch.arange(1., 4.)
<a id="__codelineno-488-3" name="__codelineno-488-3" href="#__codelineno-488-3"></a>&gt;&gt;&gt; torch.ger(v1, v2)
<a id="__codelineno-488-4" name="__codelineno-488-4" href="#__codelineno-488-4"></a>tensor([[  1.,   2.,   3.],
<a id="__codelineno-488-5" name="__codelineno-488-5" href="#__codelineno-488-5"></a>        [  2.,   4.,   6.],
<a id="__codelineno-488-6" name="__codelineno-488-6" href="#__codelineno-488-6"></a>        [  3.,   6.,   9.],
<a id="__codelineno-488-7" name="__codelineno-488-7" href="#__codelineno-488-7"></a>        [  4.,   8.,  12.]])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-489-1" name="__codelineno-489-1" href="#__codelineno-489-1"></a>torch.inverse(input, out=None) → Tensor¶
</code></pre></div>
<p>取方阵<code>input</code>的逆。 <code>input</code>可以是 2D 方形张量的批处理，在这种情况下，此函数将返回由各个逆组成的张量。</p>
<p>Note</p>
<p>无论原始步幅如何，返回的张量都将被转置，即使用 &lt;cite&gt;input.contiguous(）。transpose(-2，-1）.stride(）&lt;/cite&gt;之类的步幅</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>输入</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–大小为<img alt="" src="../img/aa6a866e7977a9ee67a53687003d3821.jpg" />的输入张量，其中 &lt;cite&gt;*&lt;/cite&gt; 为零或更大批处理尺寸</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-490-1" name="__codelineno-490-1" href="#__codelineno-490-1"></a>&gt;&gt;&gt; x = torch.rand(4, 4)
<a id="__codelineno-490-2" name="__codelineno-490-2" href="#__codelineno-490-2"></a>&gt;&gt;&gt; y = torch.inverse(x)
<a id="__codelineno-490-3" name="__codelineno-490-3" href="#__codelineno-490-3"></a>&gt;&gt;&gt; z = torch.mm(x, y)
<a id="__codelineno-490-4" name="__codelineno-490-4" href="#__codelineno-490-4"></a>&gt;&gt;&gt; z
<a id="__codelineno-490-5" name="__codelineno-490-5" href="#__codelineno-490-5"></a>tensor([[ 1.0000, -0.0000, -0.0000,  0.0000],
<a id="__codelineno-490-6" name="__codelineno-490-6" href="#__codelineno-490-6"></a>        [ 0.0000,  1.0000,  0.0000,  0.0000],
<a id="__codelineno-490-7" name="__codelineno-490-7" href="#__codelineno-490-7"></a>        [ 0.0000,  0.0000,  1.0000,  0.0000],
<a id="__codelineno-490-8" name="__codelineno-490-8" href="#__codelineno-490-8"></a>        [ 0.0000, -0.0000, -0.0000,  1.0000]])
<a id="__codelineno-490-9" name="__codelineno-490-9" href="#__codelineno-490-9"></a>&gt;&gt;&gt; torch.max(torch.abs(z - torch.eye(4))) # Max non-zero
<a id="__codelineno-490-10" name="__codelineno-490-10" href="#__codelineno-490-10"></a>tensor(1.1921e-07)
<a id="__codelineno-490-11" name="__codelineno-490-11" href="#__codelineno-490-11"></a>&gt;&gt;&gt; # Batched inverse example
<a id="__codelineno-490-12" name="__codelineno-490-12" href="#__codelineno-490-12"></a>&gt;&gt;&gt; x = torch.randn(2, 3, 4, 4)
<a id="__codelineno-490-13" name="__codelineno-490-13" href="#__codelineno-490-13"></a>&gt;&gt;&gt; y = torch.inverse(x)
<a id="__codelineno-490-14" name="__codelineno-490-14" href="#__codelineno-490-14"></a>&gt;&gt;&gt; z = torch.matmul(x, y)
<a id="__codelineno-490-15" name="__codelineno-490-15" href="#__codelineno-490-15"></a>&gt;&gt;&gt; torch.max(torch.abs(z - torch.eye(4).expand_as(x))) # Max non-zero
<a id="__codelineno-490-16" name="__codelineno-490-16" href="#__codelineno-490-16"></a>tensor(1.9073e-06)
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-491-1" name="__codelineno-491-1" href="#__codelineno-491-1"></a>torch.det(input) → Tensor¶
</code></pre></div>
<p>计算平方矩阵或批次平方矩阵的行列式。</p>
<p>Note</p>
<p>当<code>input</code>不可逆时，向后通过 <a href="#torch.det" title="torch.det"><code>det()</code></a> 内部使用 SVD 结果。 在这种情况下，如果<code>input</code>没有不同的奇异值，则通过 <a href="#torch.det" title="torch.det"><code>det()</code></a> 向后翻倍将不稳定。 有关详细信息，请参见 <a href="#torch.svd" title="torch.svd"><code>svd()</code></a> 。</p>
<p>Parameters</p>
<p><strong>输入</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–大小为<code>(*, n, n)</code>的输入张量，其中<code>*</code>为零或更大的批量尺寸。</p>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-492-1" name="__codelineno-492-1" href="#__codelineno-492-1"></a>&gt;&gt;&gt; A = torch.randn(3, 3)
<a id="__codelineno-492-2" name="__codelineno-492-2" href="#__codelineno-492-2"></a>&gt;&gt;&gt; torch.det(A)
<a id="__codelineno-492-3" name="__codelineno-492-3" href="#__codelineno-492-3"></a>tensor(3.7641)
<a id="__codelineno-492-4" name="__codelineno-492-4" href="#__codelineno-492-4"></a>
<a id="__codelineno-492-5" name="__codelineno-492-5" href="#__codelineno-492-5"></a>&gt;&gt;&gt; A = torch.randn(3, 2, 2)
<a id="__codelineno-492-6" name="__codelineno-492-6" href="#__codelineno-492-6"></a>&gt;&gt;&gt; A
<a id="__codelineno-492-7" name="__codelineno-492-7" href="#__codelineno-492-7"></a>tensor([[[ 0.9254, -0.6213],
<a id="__codelineno-492-8" name="__codelineno-492-8" href="#__codelineno-492-8"></a>         [-0.5787,  1.6843]],
<a id="__codelineno-492-9" name="__codelineno-492-9" href="#__codelineno-492-9"></a>
<a id="__codelineno-492-10" name="__codelineno-492-10" href="#__codelineno-492-10"></a>        [[ 0.3242, -0.9665],
<a id="__codelineno-492-11" name="__codelineno-492-11" href="#__codelineno-492-11"></a>         [ 0.4539, -0.0887]],
<a id="__codelineno-492-12" name="__codelineno-492-12" href="#__codelineno-492-12"></a>
<a id="__codelineno-492-13" name="__codelineno-492-13" href="#__codelineno-492-13"></a>        [[ 1.1336, -0.4025],
<a id="__codelineno-492-14" name="__codelineno-492-14" href="#__codelineno-492-14"></a>         [-0.7089,  0.9032]]])
<a id="__codelineno-492-15" name="__codelineno-492-15" href="#__codelineno-492-15"></a>&gt;&gt;&gt; A.det()
<a id="__codelineno-492-16" name="__codelineno-492-16" href="#__codelineno-492-16"></a>tensor([1.1990, 0.4099, 0.7386])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-493-1" name="__codelineno-493-1" href="#__codelineno-493-1"></a>torch.logdet(input) → Tensor¶
</code></pre></div>
<p>计算平方矩阵或批次平方矩阵的对数行列式。</p>
<p>Note</p>
<p>如果<code>input</code>的对数行列式为 0，则结果为<code>-inf</code>；如果<code>input</code>的行列式为负数，则结果为<code>nan</code>。</p>
<p>Note</p>
<p>当<code>input</code>不可逆时，向后通过 <a href="#torch.logdet" title="torch.logdet"><code>logdet()</code></a> 内部使用 SVD 结果。 在这种情况下，如果<code>input</code>没有不同的奇异值，则通过 <a href="#torch.logdet" title="torch.logdet"><code>logdet()</code></a> 向后翻倍将不稳定。 有关详细信息，请参见 <a href="#torch.svd" title="torch.svd"><code>svd()</code></a> 。</p>
<p>Parameters</p>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor of size <code>(*, n, n)</code> where <code>*</code> is zero or more batch dimensions.</p>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-494-1" name="__codelineno-494-1" href="#__codelineno-494-1"></a>&gt;&gt;&gt; A = torch.randn(3, 3)
<a id="__codelineno-494-2" name="__codelineno-494-2" href="#__codelineno-494-2"></a>&gt;&gt;&gt; torch.det(A)
<a id="__codelineno-494-3" name="__codelineno-494-3" href="#__codelineno-494-3"></a>tensor(0.2611)
<a id="__codelineno-494-4" name="__codelineno-494-4" href="#__codelineno-494-4"></a>&gt;&gt;&gt; torch.logdet(A)
<a id="__codelineno-494-5" name="__codelineno-494-5" href="#__codelineno-494-5"></a>tensor(-1.3430)
<a id="__codelineno-494-6" name="__codelineno-494-6" href="#__codelineno-494-6"></a>&gt;&gt;&gt; A
<a id="__codelineno-494-7" name="__codelineno-494-7" href="#__codelineno-494-7"></a>tensor([[[ 0.9254, -0.6213],
<a id="__codelineno-494-8" name="__codelineno-494-8" href="#__codelineno-494-8"></a>         [-0.5787,  1.6843]],
<a id="__codelineno-494-9" name="__codelineno-494-9" href="#__codelineno-494-9"></a>
<a id="__codelineno-494-10" name="__codelineno-494-10" href="#__codelineno-494-10"></a>        [[ 0.3242, -0.9665],
<a id="__codelineno-494-11" name="__codelineno-494-11" href="#__codelineno-494-11"></a>         [ 0.4539, -0.0887]],
<a id="__codelineno-494-12" name="__codelineno-494-12" href="#__codelineno-494-12"></a>
<a id="__codelineno-494-13" name="__codelineno-494-13" href="#__codelineno-494-13"></a>        [[ 1.1336, -0.4025],
<a id="__codelineno-494-14" name="__codelineno-494-14" href="#__codelineno-494-14"></a>         [-0.7089,  0.9032]]])
<a id="__codelineno-494-15" name="__codelineno-494-15" href="#__codelineno-494-15"></a>&gt;&gt;&gt; A.det()
<a id="__codelineno-494-16" name="__codelineno-494-16" href="#__codelineno-494-16"></a>tensor([1.1990, 0.4099, 0.7386])
<a id="__codelineno-494-17" name="__codelineno-494-17" href="#__codelineno-494-17"></a>&gt;&gt;&gt; A.det().log()
<a id="__codelineno-494-18" name="__codelineno-494-18" href="#__codelineno-494-18"></a>tensor([ 0.1815, -0.8917, -0.3031])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-495-1" name="__codelineno-495-1" href="#__codelineno-495-1"></a>torch.slogdet(input) -&gt; (Tensor, Tensor)¶
</code></pre></div>
<p>计算平方矩阵或一批平方矩阵的行列式的正负号和对数绝对值。</p>
<p>Note</p>
<p>如果<code>input</code>的行列式为零，则返回<code>(0, -inf)</code>。</p>
<p>Note</p>
<p>当<code>input</code>不可逆时，向后通过 <a href="#torch.slogdet" title="torch.slogdet"><code>slogdet()</code></a> 内部使用 SVD 结果。 在这种情况下，如果<code>input</code>没有不同的奇异值，则通过 <a href="#torch.slogdet" title="torch.slogdet"><code>slogdet()</code></a> 向后翻倍将不稳定。 有关详细信息，请参见 <a href="#torch.svd" title="torch.svd"><code>svd()</code></a> 。</p>
<p>Parameters</p>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor of size <code>(*, n, n)</code> where <code>*</code> is zero or more batch dimensions.</p>
<p>Returns</p>
<p>包含行列式的符号和绝对行列式的对数值的 namedtuple(符号，logabsdet）。</p>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-496-1" name="__codelineno-496-1" href="#__codelineno-496-1"></a>&gt;&gt;&gt; A = torch.randn(3, 3)
<a id="__codelineno-496-2" name="__codelineno-496-2" href="#__codelineno-496-2"></a>&gt;&gt;&gt; A
<a id="__codelineno-496-3" name="__codelineno-496-3" href="#__codelineno-496-3"></a>tensor([[ 0.0032, -0.2239, -1.1219],
<a id="__codelineno-496-4" name="__codelineno-496-4" href="#__codelineno-496-4"></a>        [-0.6690,  0.1161,  0.4053],
<a id="__codelineno-496-5" name="__codelineno-496-5" href="#__codelineno-496-5"></a>        [-1.6218, -0.9273, -0.0082]])
<a id="__codelineno-496-6" name="__codelineno-496-6" href="#__codelineno-496-6"></a>&gt;&gt;&gt; torch.det(A)
<a id="__codelineno-496-7" name="__codelineno-496-7" href="#__codelineno-496-7"></a>tensor(-0.7576)
<a id="__codelineno-496-8" name="__codelineno-496-8" href="#__codelineno-496-8"></a>&gt;&gt;&gt; torch.logdet(A)
<a id="__codelineno-496-9" name="__codelineno-496-9" href="#__codelineno-496-9"></a>tensor(nan)
<a id="__codelineno-496-10" name="__codelineno-496-10" href="#__codelineno-496-10"></a>&gt;&gt;&gt; torch.slogdet(A)
<a id="__codelineno-496-11" name="__codelineno-496-11" href="#__codelineno-496-11"></a>torch.return_types.slogdet(sign=tensor(-1.), logabsdet=tensor(-0.2776))
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-497-1" name="__codelineno-497-1" href="#__codelineno-497-1"></a>torch.lstsq(input, A, out=None) → Tensor¶
</code></pre></div>
<p>计算大小为<img alt="" src="../img/88bf1d4bce0ef2568c0d7c879f26ce08.jpg" />的满秩矩阵<img alt="" src="../img/8cab287af6acaf0838d1d67381a3716d.jpg" />和大小为<img alt="" src="../img/811a1fadb1118d551ad2e0f7515b3ab2.jpg" />的矩阵<img alt="" src="../img/041cf842180f19a622690f37ed9f70d2.jpg" />的最小二乘和最小范数问题的解。</p>
<p>如果<img alt="" src="../img/7cbd71a05144eeca3bb1f22a22c30c3a.jpg" />， <a href="#torch.lstsq" title="torch.lstsq"><code>lstsq()</code></a> 解决了最小二乘问题：</p>
<p><img alt="" src="../img/9d1018603f258178ba5a9db4739d49ff.jpg" /></p>
<p>如果<img alt="" src="../img/30ee994fb1857c1f8d6540a60056fe45.jpg" />， <a href="#torch.lstsq" title="torch.lstsq"><code>lstsq()</code></a> 解决了最小范数问题：</p>
<p><img alt="" src="../img/cb7af0f918cae78e16fd68223243e29c.jpg" /></p>
<p>返回的张量<img alt="" src="../img/1dc567019b272fda0c5051c472dac2b7.jpg" />具有形状<img alt="" src="../img/2e7f8c37553bcb25b33aa412a4d78219.jpg" />。 <img alt="" src="../img/1dc567019b272fda0c5051c472dac2b7.jpg" />的前<img alt="" src="../img/5f0051b0454fa75ca446b59b47eff6f6.jpg" />行包含解决方案。 如果为<img alt="" src="../img/7cbd71a05144eeca3bb1f22a22c30c3a.jpg" />，则每列中解决方案的剩余平方和由该列其余<img alt="" src="../img/6d15ee60528ade686ac8933bcac404a4.jpg" />行中元素的平方和得出。</p>
<p>Note</p>
<p>GPU 不支持<img alt="" src="../img/30ee994fb1857c1f8d6540a60056fe45.jpg" />的情况。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>输入</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–矩阵<img alt="" src="../img/041cf842180f19a622690f37ed9f70d2.jpg" /></p>
</li>
<li>
<p><strong>A</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–由<img alt="" src="../img/5f0051b0454fa75ca446b59b47eff6f6.jpg" />矩阵<img alt="" src="../img/8cab287af6acaf0838d1d67381a3716d.jpg" />构成的<img alt="" src="../img/03327e7b697db30918e96b2209927929.jpg" /></p>
</li>
<li>
<p><strong>out</strong> (<em>元组</em> <em>，</em> <em>可选</em>）–可选目标张量</p>
</li>
</ul>
<p>Returns</p>
<p>一个命名元组(解决方案，QR），其中包含：</p>
<blockquote>
<ul>
<li>
<p><strong>解</strong>(<em>tensor</em>）：最小二乘解</p>
</li>
<li>
<p><strong>QR</strong>  (<em>Tensor</em> )：QR 因式分解的详细信息</p>
</li>
</ul>
</blockquote>
<p>Return type</p>
<p>(<a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a>, <a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a>)</p>
<p>Note</p>
<p>无论输入矩阵的跨度如何，返回的矩阵将始终进行转置。 即，他们将具有&lt;cite&gt;(1，m）&lt;/cite&gt;而不是&lt;cite&gt;(m，1）&lt;/cite&gt;的步幅。</p>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-498-1" name="__codelineno-498-1" href="#__codelineno-498-1"></a>&gt;&gt;&gt; A = torch.tensor([[1., 1, 1],
<a id="__codelineno-498-2" name="__codelineno-498-2" href="#__codelineno-498-2"></a>                      [2, 3, 4],
<a id="__codelineno-498-3" name="__codelineno-498-3" href="#__codelineno-498-3"></a>                      [3, 5, 2],
<a id="__codelineno-498-4" name="__codelineno-498-4" href="#__codelineno-498-4"></a>                      [4, 2, 5],
<a id="__codelineno-498-5" name="__codelineno-498-5" href="#__codelineno-498-5"></a>                      [5, 4, 3]])
<a id="__codelineno-498-6" name="__codelineno-498-6" href="#__codelineno-498-6"></a>&gt;&gt;&gt; B = torch.tensor([[-10., -3],
<a id="__codelineno-498-7" name="__codelineno-498-7" href="#__codelineno-498-7"></a>                      [ 12, 14],
<a id="__codelineno-498-8" name="__codelineno-498-8" href="#__codelineno-498-8"></a>                      [ 14, 12],
<a id="__codelineno-498-9" name="__codelineno-498-9" href="#__codelineno-498-9"></a>                      [ 16, 16],
<a id="__codelineno-498-10" name="__codelineno-498-10" href="#__codelineno-498-10"></a>                      [ 18, 16]])
<a id="__codelineno-498-11" name="__codelineno-498-11" href="#__codelineno-498-11"></a>&gt;&gt;&gt; X, _ = torch.lstsq(B, A)
<a id="__codelineno-498-12" name="__codelineno-498-12" href="#__codelineno-498-12"></a>&gt;&gt;&gt; X
<a id="__codelineno-498-13" name="__codelineno-498-13" href="#__codelineno-498-13"></a>tensor([[  2.0000,   1.0000],
<a id="__codelineno-498-14" name="__codelineno-498-14" href="#__codelineno-498-14"></a>        [  1.0000,   1.0000],
<a id="__codelineno-498-15" name="__codelineno-498-15" href="#__codelineno-498-15"></a>        [  1.0000,   2.0000],
<a id="__codelineno-498-16" name="__codelineno-498-16" href="#__codelineno-498-16"></a>        [ 10.9635,   4.8501],
<a id="__codelineno-498-17" name="__codelineno-498-17" href="#__codelineno-498-17"></a>        [  8.9332,   5.2418]])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-499-1" name="__codelineno-499-1" href="#__codelineno-499-1"></a>torch.lu(A, pivot=True, get_infos=False, out=None)¶
</code></pre></div>
<p>计算矩阵或矩阵批次的 LU 分解<code>A</code>。 返回一个包含 LU 分解和<code>A</code>的枢轴的元组。 如果<code>pivot</code>设置为<code>True</code>，则完成旋转。</p>
<p>Note</p>
<p>该函数返回的枢轴为 1 索引。 如果<code>pivot</code>为<code>False</code>，则返回的枢轴是一个张量，该张量填充有适当大小的零。</p>
<p>Note</p>
<p><code>pivot</code> = <code>False</code>的 LU 分解不适用于 CPU，尝试这样做会引发错误。 但是，CUDA 可使用<code>pivot</code> = <code>False</code>的 LU 分解。</p>
<p>Note</p>
<p>该函数不会检查分解是否成功，因为<code>get_infos</code>为<code>True</code>，因为返回元组的第三个元素中存在分解的状态。</p>
<p>Note</p>
<p>在 CUDA 设备上批量处理大小小于或等于 32 的平方矩阵的情况下，由于 MAGMA 库中的错误，对奇异矩阵重复进行 LU 因式分解(请参见岩浆问题 13）。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>A</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–大小的张量<img alt="" src="../img/6217ff594f4af540b27d0cc551ab742c.jpg" /></p>
</li>
<li>
<p><strong>枢轴</strong> (<em>bool</em> <em>，</em> <em>可选</em>）–控制是否完成枢轴。 默认值：<code>True</code></p>
</li>
<li>
<p><strong>get_infos</strong>  (<em>bool</em> <em>，</em> <em>可选</em>）–如果设置为<code>True</code>，则返回信息 IntTensor。 默认值：<code>False</code></p>
</li>
<li>
<p><strong>输出</strong>(<em>元组</em> <em>，</em> <em>可选</em>）–可选输出元组。 如果<code>get_infos</code>为<code>True</code>，则元组中的元素为 Tensor，IntTensor 和 IntTensor。 如果<code>get_infos</code>为<code>False</code>，则元组中的元素为 Tensor，IntTensor。 默认值：<code>None</code></p>
</li>
</ul>
<p>Returns</p>
<p>张量的元组包含</p>
<blockquote>
<ul>
<li>
<p><strong>分解</strong>(<em>tensor</em>）：大小<img alt="" src="../img/6217ff594f4af540b27d0cc551ab742c.jpg" />的分解</p>
</li>
<li>
<p><strong>枢轴</strong> (<em>IntTensor</em> )：大小为<img alt="" src="../img/74dd7138867888df79f198ead03a8f07.jpg" />的枢轴</p>
</li>
<li>
<p><strong>信息</strong> (<em>IntTensor</em> ，<em>可选</em>）：如果<code>get_infos</code>为<code>True</code>，则此张量为<img alt="" src="../img/39f05b69c0ea6b9b6b228f948944c4f6.jpg" />，其中非零值表示是否 矩阵分解或每个小批量成功或失败</p>
</li>
</ul>
</blockquote>
<p>Return type</p>
<p>(<a href="tensors.html#torch.Tensor" title="torch.Tensor">张量</a>，IntTensor，IntTensor(可选））</p>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-500-1" name="__codelineno-500-1" href="#__codelineno-500-1"></a>&gt;&gt;&gt; A = torch.randn(2, 3, 3)
<a id="__codelineno-500-2" name="__codelineno-500-2" href="#__codelineno-500-2"></a>&gt;&gt;&gt; A_LU, pivots = torch.lu(A)
<a id="__codelineno-500-3" name="__codelineno-500-3" href="#__codelineno-500-3"></a>&gt;&gt;&gt; A_LU
<a id="__codelineno-500-4" name="__codelineno-500-4" href="#__codelineno-500-4"></a>tensor([[[ 1.3506,  2.5558, -0.0816],
<a id="__codelineno-500-5" name="__codelineno-500-5" href="#__codelineno-500-5"></a>         [ 0.1684,  1.1551,  0.1940],
<a id="__codelineno-500-6" name="__codelineno-500-6" href="#__codelineno-500-6"></a>         [ 0.1193,  0.6189, -0.5497]],
<a id="__codelineno-500-7" name="__codelineno-500-7" href="#__codelineno-500-7"></a>
<a id="__codelineno-500-8" name="__codelineno-500-8" href="#__codelineno-500-8"></a>        [[ 0.4526,  1.2526, -0.3285],
<a id="__codelineno-500-9" name="__codelineno-500-9" href="#__codelineno-500-9"></a>         [-0.7988,  0.7175, -0.9701],
<a id="__codelineno-500-10" name="__codelineno-500-10" href="#__codelineno-500-10"></a>         [ 0.2634, -0.9255, -0.3459]]])
<a id="__codelineno-500-11" name="__codelineno-500-11" href="#__codelineno-500-11"></a>&gt;&gt;&gt; pivots
<a id="__codelineno-500-12" name="__codelineno-500-12" href="#__codelineno-500-12"></a>tensor([[ 3,  3,  3],
<a id="__codelineno-500-13" name="__codelineno-500-13" href="#__codelineno-500-13"></a>        [ 3,  3,  3]], dtype=torch.int32)
<a id="__codelineno-500-14" name="__codelineno-500-14" href="#__codelineno-500-14"></a>&gt;&gt;&gt; A_LU, pivots, info = torch.lu(A, get_infos=True)
<a id="__codelineno-500-15" name="__codelineno-500-15" href="#__codelineno-500-15"></a>&gt;&gt;&gt; if info.nonzero().size(0) == 0:
<a id="__codelineno-500-16" name="__codelineno-500-16" href="#__codelineno-500-16"></a>...   print(&#39;LU factorization succeeded for all samples!&#39;)
<a id="__codelineno-500-17" name="__codelineno-500-17" href="#__codelineno-500-17"></a>LU factorization succeeded for all samples!
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-501-1" name="__codelineno-501-1" href="#__codelineno-501-1"></a>torch.lu_solve(input, LU_data, LU_pivots, out=None) → Tensor¶
</code></pre></div>
<p>使用 <a href="#torch.lu" title="torch.lu"><code>torch.lu()</code></a> 中 A 的部分枢轴 LU 分解，返回线性系统<img alt="" src="../img/328fdc7aa24f647110fc0900733a006f.jpg" />的 LU 解。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>b</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–尺寸为<img alt="" src="../img/8ebb9d35b764041761ddb4e3436e265d.jpg" />的 RHS 张量，其中<img alt="" src="../img/dcef98688866c0d5a21137cf53bf228d.jpg" />为零或更大的批量尺寸。</p>
</li>
<li>
<p><strong>LU_data</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)– A 从 <a href="#torch.lu" title="torch.lu"><code>torch.lu()</code></a> 大小为<img alt="" src="../img/10ea424983a008dbf72dc86628a2813d.jpg" />的 A 的透视 LU 分解，其中<img alt="" src="../img/dcef98688866c0d5a21137cf53bf228d.jpg" />为 零个或多个批次尺寸。</p>
</li>
<li>
<p><strong>LU_pivots</strong>  (<em>IntTensor</em> )– LU 分解的枢轴来自 <a href="#torch.lu" title="torch.lu"><code>torch.lu()</code></a> ，大小为<img alt="" src="../img/74dd7138867888df79f198ead03a8f07.jpg" />，其中<img alt="" src="../img/dcef98688866c0d5a21137cf53bf228d.jpg" />为零或更大的批生产尺寸。 <code>LU_pivots</code>的批次尺寸必须等于<code>LU_data</code>的批次尺寸。</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-502-1" name="__codelineno-502-1" href="#__codelineno-502-1"></a>&gt;&gt;&gt; A = torch.randn(2, 3, 3)
<a id="__codelineno-502-2" name="__codelineno-502-2" href="#__codelineno-502-2"></a>&gt;&gt;&gt; b = torch.randn(2, 3, 1)
<a id="__codelineno-502-3" name="__codelineno-502-3" href="#__codelineno-502-3"></a>&gt;&gt;&gt; A_LU = torch.lu(A)
<a id="__codelineno-502-4" name="__codelineno-502-4" href="#__codelineno-502-4"></a>&gt;&gt;&gt; x = torch.lu_solve(b, *A_LU)
<a id="__codelineno-502-5" name="__codelineno-502-5" href="#__codelineno-502-5"></a>&gt;&gt;&gt; torch.norm(torch.bmm(A, x) - b)
<a id="__codelineno-502-6" name="__codelineno-502-6" href="#__codelineno-502-6"></a>tensor(1.00000e-07 *
<a id="__codelineno-502-7" name="__codelineno-502-7" href="#__codelineno-502-7"></a>       2.8312)
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-503-1" name="__codelineno-503-1" href="#__codelineno-503-1"></a>torch.lu_unpack(LU_data, LU_pivots, unpack_data=True, unpack_pivots=True)¶
</code></pre></div>
<p>解压缩数据并从张量的 LU 分解中枢转。</p>
<p>返回张量的元组为<code>(the pivots, the L tensor, the U tensor)</code>。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>LU_data</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–打包 LU 分解数据</p>
</li>
<li>
<p><strong>LU_pivots</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–压缩 LU 分解枢轴</p>
</li>
<li>
<p><strong>unpack_data</strong>  (<em>bool</em> )–指示是否应拆包数据的标志</p>
</li>
<li>
<p><strong>unpack_pivots</strong>  (<em>bool</em> )–指示是否应拆开枢轴的标志</p>
</li>
</ul>
<p>Examples:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-504-1" name="__codelineno-504-1" href="#__codelineno-504-1"></a>&gt;&gt;&gt; A = torch.randn(2, 3, 3)
<a id="__codelineno-504-2" name="__codelineno-504-2" href="#__codelineno-504-2"></a>&gt;&gt;&gt; A_LU, pivots = A.lu()
<a id="__codelineno-504-3" name="__codelineno-504-3" href="#__codelineno-504-3"></a>&gt;&gt;&gt; P, A_L, A_U = torch.lu_unpack(A_LU, pivots)
<a id="__codelineno-504-4" name="__codelineno-504-4" href="#__codelineno-504-4"></a>&gt;&gt;&gt;
<a id="__codelineno-504-5" name="__codelineno-504-5" href="#__codelineno-504-5"></a>&gt;&gt;&gt; # can recover A from factorization
<a id="__codelineno-504-6" name="__codelineno-504-6" href="#__codelineno-504-6"></a>&gt;&gt;&gt; A_ = torch.bmm(P, torch.bmm(A_L, A_U))
<a id="__codelineno-504-7" name="__codelineno-504-7" href="#__codelineno-504-7"></a>
<a id="__codelineno-504-8" name="__codelineno-504-8" href="#__codelineno-504-8"></a>&gt;&gt;&gt; # LU factorization of a rectangular matrix:
<a id="__codelineno-504-9" name="__codelineno-504-9" href="#__codelineno-504-9"></a>&gt;&gt;&gt; A = torch.randn(2, 3, 2)
<a id="__codelineno-504-10" name="__codelineno-504-10" href="#__codelineno-504-10"></a>&gt;&gt;&gt; A_LU, pivots = A.lu()
<a id="__codelineno-504-11" name="__codelineno-504-11" href="#__codelineno-504-11"></a>&gt;&gt;&gt; P, A_L, A_U = torch.lu_unpack(A_LU, pivots)
<a id="__codelineno-504-12" name="__codelineno-504-12" href="#__codelineno-504-12"></a>&gt;&gt;&gt; P
<a id="__codelineno-504-13" name="__codelineno-504-13" href="#__codelineno-504-13"></a>tensor([[[1., 0., 0.],
<a id="__codelineno-504-14" name="__codelineno-504-14" href="#__codelineno-504-14"></a>         [0., 1., 0.],
<a id="__codelineno-504-15" name="__codelineno-504-15" href="#__codelineno-504-15"></a>         [0., 0., 1.]],
<a id="__codelineno-504-16" name="__codelineno-504-16" href="#__codelineno-504-16"></a>
<a id="__codelineno-504-17" name="__codelineno-504-17" href="#__codelineno-504-17"></a>        [[0., 0., 1.],
<a id="__codelineno-504-18" name="__codelineno-504-18" href="#__codelineno-504-18"></a>         [0., 1., 0.],
<a id="__codelineno-504-19" name="__codelineno-504-19" href="#__codelineno-504-19"></a>         [1., 0., 0.]]])
<a id="__codelineno-504-20" name="__codelineno-504-20" href="#__codelineno-504-20"></a>&gt;&gt;&gt; A_L
<a id="__codelineno-504-21" name="__codelineno-504-21" href="#__codelineno-504-21"></a>tensor([[[ 1.0000,  0.0000],
<a id="__codelineno-504-22" name="__codelineno-504-22" href="#__codelineno-504-22"></a>         [ 0.4763,  1.0000],
<a id="__codelineno-504-23" name="__codelineno-504-23" href="#__codelineno-504-23"></a>         [ 0.3683,  0.1135]],
<a id="__codelineno-504-24" name="__codelineno-504-24" href="#__codelineno-504-24"></a>
<a id="__codelineno-504-25" name="__codelineno-504-25" href="#__codelineno-504-25"></a>        [[ 1.0000,  0.0000],
<a id="__codelineno-504-26" name="__codelineno-504-26" href="#__codelineno-504-26"></a>         [ 0.2957,  1.0000],
<a id="__codelineno-504-27" name="__codelineno-504-27" href="#__codelineno-504-27"></a>         [-0.9668, -0.3335]]])
<a id="__codelineno-504-28" name="__codelineno-504-28" href="#__codelineno-504-28"></a>&gt;&gt;&gt; A_U
<a id="__codelineno-504-29" name="__codelineno-504-29" href="#__codelineno-504-29"></a>tensor([[[ 2.1962,  1.0881],
<a id="__codelineno-504-30" name="__codelineno-504-30" href="#__codelineno-504-30"></a>         [ 0.0000, -0.8681]],
<a id="__codelineno-504-31" name="__codelineno-504-31" href="#__codelineno-504-31"></a>
<a id="__codelineno-504-32" name="__codelineno-504-32" href="#__codelineno-504-32"></a>        [[-1.0947,  0.3736],
<a id="__codelineno-504-33" name="__codelineno-504-33" href="#__codelineno-504-33"></a>         [ 0.0000,  0.5718]]])
<a id="__codelineno-504-34" name="__codelineno-504-34" href="#__codelineno-504-34"></a>&gt;&gt;&gt; A_ = torch.bmm(P, torch.bmm(A_L, A_U))
<a id="__codelineno-504-35" name="__codelineno-504-35" href="#__codelineno-504-35"></a>&gt;&gt;&gt; torch.norm(A_ - A)
<a id="__codelineno-504-36" name="__codelineno-504-36" href="#__codelineno-504-36"></a>tensor(2.9802e-08)
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-505-1" name="__codelineno-505-1" href="#__codelineno-505-1"></a>torch.matmul(input, other, out=None) → Tensor¶
</code></pre></div>
<p>两个张量的矩阵乘积。</p>
<p>行为取决于张量的维数，如下所示：</p>
<ul>
<li>
<p>如果两个张量都是一维的，则返回点积(标量）。</p>
</li>
<li>
<p>如果两个参数都是二维的，则返回矩阵矩阵乘积。</p>
</li>
<li>
<p>如果第一个自变量是一维的，第二个自变量是二维的，则为了矩阵乘法，会将 1 附加到其维上。 矩阵相乘后，将删除前置尺寸。</p>
</li>
<li>
<p>如果第一个参数为 2 维，第二个参数为 1 维，则返回矩阵向量乘积。</p>
</li>
<li>
<p>如果两个自变量至少为一维且至少一个自变量为 N 维(其中 N &gt; 2），则返回批处理矩阵乘法。 如果第一个自变量是一维的，则将 1 附加到其维的前面，以实现批量矩阵乘法并在之后将其删除。 如果第二个参数是一维的，则将 1 附加到其维上，以实现成批矩阵倍数的目的，然后将其删除。 非矩阵(即批处理）尺寸是<a href="notes/broadcasting.html#broadcasting-semantics">广播的</a>(因此必须是可广播的）。 例如，如果<code>input</code>是<img alt="" src="../img/48e81addd1b15307d101249324d93372.jpg" />张量，<code>other</code>是<img alt="" src="../img/561dfed20b3d8014c77c118b8904ec66.jpg" />张量，则<code>out</code>将是<img alt="" src="../img/cb41125b3953740f3a7f50204cd20d7f.jpg" />张量。</p>
</li>
</ul>
<p>Note</p>
<p>此功能的一维点积产品版本不支持<code>out</code>参数。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>输入</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–要相乘的第一个张量</p>
</li>
<li>
<p><strong>其他</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–要相乘的第二张量</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-506-1" name="__codelineno-506-1" href="#__codelineno-506-1"></a>&gt;&gt;&gt; # vector x vector
<a id="__codelineno-506-2" name="__codelineno-506-2" href="#__codelineno-506-2"></a>&gt;&gt;&gt; tensor1 = torch.randn(3)
<a id="__codelineno-506-3" name="__codelineno-506-3" href="#__codelineno-506-3"></a>&gt;&gt;&gt; tensor2 = torch.randn(3)
<a id="__codelineno-506-4" name="__codelineno-506-4" href="#__codelineno-506-4"></a>&gt;&gt;&gt; torch.matmul(tensor1, tensor2).size()
<a id="__codelineno-506-5" name="__codelineno-506-5" href="#__codelineno-506-5"></a>torch.Size([])
<a id="__codelineno-506-6" name="__codelineno-506-6" href="#__codelineno-506-6"></a>&gt;&gt;&gt; # matrix x vector
<a id="__codelineno-506-7" name="__codelineno-506-7" href="#__codelineno-506-7"></a>&gt;&gt;&gt; tensor1 = torch.randn(3, 4)
<a id="__codelineno-506-8" name="__codelineno-506-8" href="#__codelineno-506-8"></a>&gt;&gt;&gt; tensor2 = torch.randn(4)
<a id="__codelineno-506-9" name="__codelineno-506-9" href="#__codelineno-506-9"></a>&gt;&gt;&gt; torch.matmul(tensor1, tensor2).size()
<a id="__codelineno-506-10" name="__codelineno-506-10" href="#__codelineno-506-10"></a>torch.Size([3])
<a id="__codelineno-506-11" name="__codelineno-506-11" href="#__codelineno-506-11"></a>&gt;&gt;&gt; # batched matrix x broadcasted vector
<a id="__codelineno-506-12" name="__codelineno-506-12" href="#__codelineno-506-12"></a>&gt;&gt;&gt; tensor1 = torch.randn(10, 3, 4)
<a id="__codelineno-506-13" name="__codelineno-506-13" href="#__codelineno-506-13"></a>&gt;&gt;&gt; tensor2 = torch.randn(4)
<a id="__codelineno-506-14" name="__codelineno-506-14" href="#__codelineno-506-14"></a>&gt;&gt;&gt; torch.matmul(tensor1, tensor2).size()
<a id="__codelineno-506-15" name="__codelineno-506-15" href="#__codelineno-506-15"></a>torch.Size([10, 3])
<a id="__codelineno-506-16" name="__codelineno-506-16" href="#__codelineno-506-16"></a>&gt;&gt;&gt; # batched matrix x batched matrix
<a id="__codelineno-506-17" name="__codelineno-506-17" href="#__codelineno-506-17"></a>&gt;&gt;&gt; tensor1 = torch.randn(10, 3, 4)
<a id="__codelineno-506-18" name="__codelineno-506-18" href="#__codelineno-506-18"></a>&gt;&gt;&gt; tensor2 = torch.randn(10, 4, 5)
<a id="__codelineno-506-19" name="__codelineno-506-19" href="#__codelineno-506-19"></a>&gt;&gt;&gt; torch.matmul(tensor1, tensor2).size()
<a id="__codelineno-506-20" name="__codelineno-506-20" href="#__codelineno-506-20"></a>torch.Size([10, 3, 5])
<a id="__codelineno-506-21" name="__codelineno-506-21" href="#__codelineno-506-21"></a>&gt;&gt;&gt; # batched matrix x broadcasted matrix
<a id="__codelineno-506-22" name="__codelineno-506-22" href="#__codelineno-506-22"></a>&gt;&gt;&gt; tensor1 = torch.randn(10, 3, 4)
<a id="__codelineno-506-23" name="__codelineno-506-23" href="#__codelineno-506-23"></a>&gt;&gt;&gt; tensor2 = torch.randn(4, 5)
<a id="__codelineno-506-24" name="__codelineno-506-24" href="#__codelineno-506-24"></a>&gt;&gt;&gt; torch.matmul(tensor1, tensor2).size()
<a id="__codelineno-506-25" name="__codelineno-506-25" href="#__codelineno-506-25"></a>torch.Size([10, 3, 5])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-507-1" name="__codelineno-507-1" href="#__codelineno-507-1"></a>torch.matrix_power(input, n) → Tensor¶
</code></pre></div>
<p>返回平方矩阵乘幂<code>n</code>的矩阵。 对于一批矩阵，将每个单独的矩阵提高到幂<code>n</code>。</p>
<p>如果<code>n</code>为负，则矩阵的逆(如果是可逆的）提高到幂<code>n</code>。 对于一批矩阵，将成批的逆(如果可逆）提高到幂<code>n</code>。 如果<code>n</code>为 0，则返回一个单位矩阵。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>n</strong>  (<em>python：int</em> )–将矩阵提升为</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-508-1" name="__codelineno-508-1" href="#__codelineno-508-1"></a>&gt;&gt;&gt; a = torch.randn(2, 2, 2)
<a id="__codelineno-508-2" name="__codelineno-508-2" href="#__codelineno-508-2"></a>&gt;&gt;&gt; a
<a id="__codelineno-508-3" name="__codelineno-508-3" href="#__codelineno-508-3"></a>tensor([[[-1.9975, -1.9610],
<a id="__codelineno-508-4" name="__codelineno-508-4" href="#__codelineno-508-4"></a>         [ 0.9592, -2.3364]],
<a id="__codelineno-508-5" name="__codelineno-508-5" href="#__codelineno-508-5"></a>
<a id="__codelineno-508-6" name="__codelineno-508-6" href="#__codelineno-508-6"></a>        [[-1.2534, -1.3429],
<a id="__codelineno-508-7" name="__codelineno-508-7" href="#__codelineno-508-7"></a>         [ 0.4153, -1.4664]]])
<a id="__codelineno-508-8" name="__codelineno-508-8" href="#__codelineno-508-8"></a>&gt;&gt;&gt; torch.matrix_power(a, 3)
<a id="__codelineno-508-9" name="__codelineno-508-9" href="#__codelineno-508-9"></a>tensor([[[  3.9392, -23.9916],
<a id="__codelineno-508-10" name="__codelineno-508-10" href="#__codelineno-508-10"></a>         [ 11.7357,  -0.2070]],
<a id="__codelineno-508-11" name="__codelineno-508-11" href="#__codelineno-508-11"></a>
<a id="__codelineno-508-12" name="__codelineno-508-12" href="#__codelineno-508-12"></a>        [[  0.2468,  -6.7168],
<a id="__codelineno-508-13" name="__codelineno-508-13" href="#__codelineno-508-13"></a>         [  2.0774,  -0.8187]]])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-509-1" name="__codelineno-509-1" href="#__codelineno-509-1"></a>torch.matrix_rank(input, tol=None, symmetric=False) → Tensor¶
</code></pre></div>
<p>返回二维张量的数值等级。 默认情况下，使用 SVD 完成计算矩阵等级的方法。 如果<code>symmetric</code>为<code>True</code>，则假定<code>input</code>是对称的，并且通过获得特征值来完成秩的计算。</p>
<p><code>tol</code>是阈值，低于该阈值的奇异值(或当<code>symmetric</code>为<code>True</code>时的特征值）被视为 0。如果未指定<code>tol</code>，则<code>tol</code>设置为<code>S.max() * max(S.size()) * eps</code>，其中 &lt;cite&gt;S&lt;/cite&gt; 是奇异值(或<code>symmetric</code>是<code>True</code>时的特征值），<code>eps</code>是<code>input</code>数据类型的 epsilon 值。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>输入</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–输入二维张量</p>
</li>
<li>
<p><strong>tol</strong>  (<em>python：float</em> <em>，</em> <em>可选</em>）–公差值。 默认值：<code>None</code></p>
</li>
<li>
<p><strong>对称</strong> (<em>bool</em> <em>，</em> <em>可选</em>）–指示<code>input</code>是否对称。 默认值：<code>False</code></p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-510-1" name="__codelineno-510-1" href="#__codelineno-510-1"></a>&gt;&gt;&gt; a = torch.eye(10)
<a id="__codelineno-510-2" name="__codelineno-510-2" href="#__codelineno-510-2"></a>&gt;&gt;&gt; torch.matrix_rank(a)
<a id="__codelineno-510-3" name="__codelineno-510-3" href="#__codelineno-510-3"></a>tensor(10)
<a id="__codelineno-510-4" name="__codelineno-510-4" href="#__codelineno-510-4"></a>&gt;&gt;&gt; b = torch.eye(10)
<a id="__codelineno-510-5" name="__codelineno-510-5" href="#__codelineno-510-5"></a>&gt;&gt;&gt; b[0, 0] = 0
<a id="__codelineno-510-6" name="__codelineno-510-6" href="#__codelineno-510-6"></a>&gt;&gt;&gt; torch.matrix_rank(b)
<a id="__codelineno-510-7" name="__codelineno-510-7" href="#__codelineno-510-7"></a>tensor(9)
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-511-1" name="__codelineno-511-1" href="#__codelineno-511-1"></a>torch.mm(input, mat2, out=None) → Tensor¶
</code></pre></div>
<p>对矩阵<code>input</code>和<code>mat2</code>进行矩阵乘法。</p>
<p>如果<code>input</code>是<img alt="" src="../img/95760d62046dcfa418c3b7ffea4caefc.jpg" />张量，<code>mat2</code>是<img alt="" src="../img/d145985cd9c2b23f68a55b0d5429c2ac.jpg" />张量，<code>out</code>将是<img alt="" src="../img/80b2c8de6d028b93a22dfe571079ee9c.jpg" />张量。</p>
<p>Note</p>
<p>This function does not <a href="notes/broadcasting.html#broadcasting-semantics">broadcast</a>. For broadcasting matrix products, see <a href="#torch.matmul" title="torch.matmul"><code>torch.matmul()</code></a>.</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>输入</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–要相乘的第一个矩阵</p>
</li>
<li>
<p><strong>mat2</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the second matrix to be multiplied</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-512-1" name="__codelineno-512-1" href="#__codelineno-512-1"></a>&gt;&gt;&gt; mat1 = torch.randn(2, 3)
<a id="__codelineno-512-2" name="__codelineno-512-2" href="#__codelineno-512-2"></a>&gt;&gt;&gt; mat2 = torch.randn(3, 3)
<a id="__codelineno-512-3" name="__codelineno-512-3" href="#__codelineno-512-3"></a>&gt;&gt;&gt; torch.mm(mat1, mat2)
<a id="__codelineno-512-4" name="__codelineno-512-4" href="#__codelineno-512-4"></a>tensor([[ 0.4851,  0.5037, -0.3633],
<a id="__codelineno-512-5" name="__codelineno-512-5" href="#__codelineno-512-5"></a>        [-0.0760, -3.6705,  2.4784]])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-513-1" name="__codelineno-513-1" href="#__codelineno-513-1"></a>torch.mv(input, vec, out=None) → Tensor¶
</code></pre></div>
<p>执行矩阵<code>input</code>与向量<code>vec</code>的矩阵向量积。</p>
<p>如果<code>input</code>是<img alt="" src="../img/95760d62046dcfa418c3b7ffea4caefc.jpg" />张量，<code>vec</code>是大小<img alt="" src="../img/03327e7b697db30918e96b2209927929.jpg" />的一维张量，则<code>out</code>将是大小<img alt="" src="../img/5f0051b0454fa75ca446b59b47eff6f6.jpg" />的一维。</p>
<p>Note</p>
<p>This function does not <a href="notes/broadcasting.html#broadcasting-semantics">broadcast</a>.</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>输入</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–要相乘的矩阵</p>
</li>
<li>
<p><strong>vec</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – vector to be multiplied</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-514-1" name="__codelineno-514-1" href="#__codelineno-514-1"></a>&gt;&gt;&gt; mat = torch.randn(2, 3)
<a id="__codelineno-514-2" name="__codelineno-514-2" href="#__codelineno-514-2"></a>&gt;&gt;&gt; vec = torch.randn(3)
<a id="__codelineno-514-3" name="__codelineno-514-3" href="#__codelineno-514-3"></a>&gt;&gt;&gt; torch.mv(mat, vec)
<a id="__codelineno-514-4" name="__codelineno-514-4" href="#__codelineno-514-4"></a>tensor([ 1.0404, -0.6361])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-515-1" name="__codelineno-515-1" href="#__codelineno-515-1"></a>torch.orgqr(input, input2) → Tensor¶
</code></pre></div>
<p>根据 <a href="#torch.geqrf" title="torch.geqrf"><code>torch.geqrf()</code></a> 返回的&lt;cite&gt;(输入，input2）&lt;/cite&gt;元组，计算 QR 分解的正交矩阵 &lt;cite&gt;Q&lt;/cite&gt; 。</p>
<p>这将直接调用基础的 LAPACK 函数&lt;cite&gt;？orgqr&lt;/cite&gt; 。 有关更多详细信息，请参见 orgqr 的 <a href="https://software.intel.com/en-us/mkl-developer-reference-c-orgqr">LAPACK 文档。</a></p>
<p>Parameters</p>
<ul>
<li>
<p><strong>输入</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–来自 <a href="#torch.geqrf" title="torch.geqrf"><code>torch.geqrf()</code></a> 的 &lt;cite&gt;a&lt;/cite&gt; 。</p>
</li>
<li>
<p><strong>input2</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–来自 <a href="#torch.geqrf" title="torch.geqrf"><code>torch.geqrf()</code></a> 的 &lt;cite&gt;tau&lt;/cite&gt; 。</p>
</li>
</ul>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-516-1" name="__codelineno-516-1" href="#__codelineno-516-1"></a>torch.ormqr(input, input2, input3, left=True, transpose=False) → Tensor¶
</code></pre></div>
<p>将&lt;cite&gt;垫&lt;/cite&gt;(由<code>input3</code>赋予）乘以 <a href="#torch.geqrf" title="torch.geqrf"><code>torch.geqrf()</code></a> 表示的 QR 因式分解的正交 &lt;cite&gt;Q&lt;/cite&gt; 矩阵，该矩阵由&lt;cite&gt;(a (tau）&lt;/cite&gt;(由[<code>input</code>，<code>input2</code>给予））。</p>
<p>这将直接调用基础的 LAPACK 函数&lt;cite&gt;？ormqr&lt;/cite&gt; 。 有关更多详细信息，请参见 ormqr 的 <a href="https://software.intel.com/en-us/mkl-developer-reference-c-ormqr">LAPACK 文档。</a></p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the &lt;cite&gt;a&lt;/cite&gt; from <a href="#torch.geqrf" title="torch.geqrf"><code>torch.geqrf()</code></a>.</p>
</li>
<li>
<p><strong>input2</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the &lt;cite&gt;tau&lt;/cite&gt; from <a href="#torch.geqrf" title="torch.geqrf"><code>torch.geqrf()</code></a>.</p>
</li>
<li>
<p><strong>input3</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–要相乘的矩阵。</p>
</li>
</ul>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-517-1" name="__codelineno-517-1" href="#__codelineno-517-1"></a>torch.pinverse(input, rcond=1e-15) → Tensor¶
</code></pre></div>
<p>计算 2D 张量的伪逆(也称为 Moore-Penrose 逆）。 请查看 <a href="https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse">Moore-Penrose 逆</a>了解更多详细信息</p>
<p>Note</p>
<p>使用奇异值分解实现此方法。</p>
<p>Note</p>
<p>在矩阵 <a href="https://epubs.siam.org/doi/10.1137/0117004">[1]</a> 的元素中，伪逆不一定是连续函数。 因此，导数并不总是存在，并且仅以恒定等级存在 <a href="https://www.jstor.org/stable/2156365">[2]</a> 。 但是，由于使用 SVD 结果实现，因此该方法可向后传播，并且可能不稳定。 由于内部使用 SVD，因此双向后退也会变得不稳定。 有关更多详细信息，请参见 <a href="#torch.svd" title="torch.svd"><code>svd()</code></a> 。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>输入</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–大小为<img alt="" src="../img/6217ff594f4af540b27d0cc551ab742c.jpg" />的输入张量，其中<img alt="" src="../img/dcef98688866c0d5a21137cf53bf228d.jpg" />为零或更大批处理尺寸</p>
</li>
<li>
<p><strong>rcond</strong>  (<em>python：float</em> )–一个浮点值，用于确定小的奇异值的截止值。 默认值：1e-15</p>
</li>
</ul>
<p>Returns</p>
<p>尺寸为<img alt="" src="../img/decace38e1e20419be05da9c4eac4b78.jpg" />的<code>input</code>的伪逆。</p>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-518-1" name="__codelineno-518-1" href="#__codelineno-518-1"></a>&gt;&gt;&gt; input = torch.randn(3, 5)
<a id="__codelineno-518-2" name="__codelineno-518-2" href="#__codelineno-518-2"></a>&gt;&gt;&gt; input
<a id="__codelineno-518-3" name="__codelineno-518-3" href="#__codelineno-518-3"></a>tensor([[ 0.5495,  0.0979, -1.4092, -0.1128,  0.4132],
<a id="__codelineno-518-4" name="__codelineno-518-4" href="#__codelineno-518-4"></a>        [-1.1143, -0.3662,  0.3042,  1.6374, -0.9294],
<a id="__codelineno-518-5" name="__codelineno-518-5" href="#__codelineno-518-5"></a>        [-0.3269, -0.5745, -0.0382, -0.5922, -0.6759]])
<a id="__codelineno-518-6" name="__codelineno-518-6" href="#__codelineno-518-6"></a>&gt;&gt;&gt; torch.pinverse(input)
<a id="__codelineno-518-7" name="__codelineno-518-7" href="#__codelineno-518-7"></a>tensor([[ 0.0600, -0.1933, -0.2090],
<a id="__codelineno-518-8" name="__codelineno-518-8" href="#__codelineno-518-8"></a>        [-0.0903, -0.0817, -0.4752],
<a id="__codelineno-518-9" name="__codelineno-518-9" href="#__codelineno-518-9"></a>        [-0.7124, -0.1631, -0.2272],
<a id="__codelineno-518-10" name="__codelineno-518-10" href="#__codelineno-518-10"></a>        [ 0.1356,  0.3933, -0.5023],
<a id="__codelineno-518-11" name="__codelineno-518-11" href="#__codelineno-518-11"></a>        [-0.0308, -0.1725, -0.5216]])
<a id="__codelineno-518-12" name="__codelineno-518-12" href="#__codelineno-518-12"></a>&gt;&gt;&gt; # Batched pinverse example
<a id="__codelineno-518-13" name="__codelineno-518-13" href="#__codelineno-518-13"></a>&gt;&gt;&gt; a = torch.randn(2,6,3)
<a id="__codelineno-518-14" name="__codelineno-518-14" href="#__codelineno-518-14"></a>&gt;&gt;&gt; b = torch.pinverse(a)
<a id="__codelineno-518-15" name="__codelineno-518-15" href="#__codelineno-518-15"></a>&gt;&gt;&gt; torch.matmul(b, a)
<a id="__codelineno-518-16" name="__codelineno-518-16" href="#__codelineno-518-16"></a>tensor([[[ 1.0000e+00,  1.6391e-07, -1.1548e-07],
<a id="__codelineno-518-17" name="__codelineno-518-17" href="#__codelineno-518-17"></a>        [ 8.3121e-08,  1.0000e+00, -2.7567e-07],
<a id="__codelineno-518-18" name="__codelineno-518-18" href="#__codelineno-518-18"></a>        [ 3.5390e-08,  1.4901e-08,  1.0000e+00]],
<a id="__codelineno-518-19" name="__codelineno-518-19" href="#__codelineno-518-19"></a>
<a id="__codelineno-518-20" name="__codelineno-518-20" href="#__codelineno-518-20"></a>        [[ 1.0000e+00, -8.9407e-08,  2.9802e-08],
<a id="__codelineno-518-21" name="__codelineno-518-21" href="#__codelineno-518-21"></a>        [-2.2352e-07,  1.0000e+00,  1.1921e-07],
<a id="__codelineno-518-22" name="__codelineno-518-22" href="#__codelineno-518-22"></a>        [ 0.0000e+00,  8.9407e-08,  1.0000e+00]]])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-519-1" name="__codelineno-519-1" href="#__codelineno-519-1"></a>torch.qr(input, some=True, out=None) -&gt; (Tensor, Tensor)¶
</code></pre></div>
<p>计算矩阵或一批矩阵<code>input</code>的 QR 分解，并返回张量的命名元组(Q，R），使得<img alt="" src="../img/41222a3dad86eb0798f47e735d4eb15b.jpg" />其中<img alt="" src="../img/7f927fea3856ca4796aab74326229f61.jpg" />是正交矩阵或一批正交矩阵，而<img alt="" src="../img/fd6855baddb0a56aeca293dd58a9758d.jpg" />是 上三角矩阵或一批上三角矩阵。</p>
<p>如果<code>some</code>为<code>True</code>，则此函数返回瘦(​​精简）QR 因式分解。 否则，如果<code>some</code>为<code>False</code>，则此函数返回完整的 QR 因式分解。</p>
<p>Note</p>
<p>如果<code>input</code>的元素的幅度较大，则可能会失去精度</p>
<p>Note</p>
<p>尽管它始终可以为您提供有效的分解，但在各个平台上可能不会给您相同的分解-这取决于您的 LAPACK 实现。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>输入</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–大小为<img alt="" src="../img/6217ff594f4af540b27d0cc551ab742c.jpg" />的输入张量，其中 &lt;cite&gt;*&lt;/cite&gt; 为零个或多个批处理尺寸，包括尺寸矩阵 <img alt="" src="../img/5324b7a0d3b2c1767998bc499b6dfb11.jpg" />。</p>
</li>
<li>
<p><strong>一些</strong> (<em>bool</em> <em>，</em> <em>可选</em>）–设置为<code>True</code>可减少 QR 分解，将<code>False</code>进行完全 QR 分解。</p>
</li>
<li>
<p><strong>out</strong> (<em>元组</em> <em>，</em> <em>可选</em>）– &lt;cite&gt;Q&lt;/cite&gt; 和 &lt;cite&gt;R&lt;/cite&gt; 张量的元组 <code>input = torch.matmul(Q, R)</code>。 &lt;cite&gt;Q&lt;/cite&gt; 和 &lt;cite&gt;R&lt;/cite&gt; 的尺寸分别为<img alt="" src="../img/8ebb9d35b764041761ddb4e3436e265d.jpg" />和<img alt="" src="../img/d158d0764e1a57ef7842c747aafc64dc.jpg" />，如果<code>some:</code>为<code>True</code>则为<img alt="" src="../img/c7ea35623a8622e24f04c76bdf3f47c5.jpg" />，否则为<img alt="" src="../img/632a48f07f9deba36cb4ed068721ae09.jpg" />。</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-520-1" name="__codelineno-520-1" href="#__codelineno-520-1"></a>&gt;&gt;&gt; a = torch.tensor([[12., -51, 4], [6, 167, -68], [-4, 24, -41]])
<a id="__codelineno-520-2" name="__codelineno-520-2" href="#__codelineno-520-2"></a>&gt;&gt;&gt; q, r = torch.qr(a)
<a id="__codelineno-520-3" name="__codelineno-520-3" href="#__codelineno-520-3"></a>&gt;&gt;&gt; q
<a id="__codelineno-520-4" name="__codelineno-520-4" href="#__codelineno-520-4"></a>tensor([[-0.8571,  0.3943,  0.3314],
<a id="__codelineno-520-5" name="__codelineno-520-5" href="#__codelineno-520-5"></a>        [-0.4286, -0.9029, -0.0343],
<a id="__codelineno-520-6" name="__codelineno-520-6" href="#__codelineno-520-6"></a>        [ 0.2857, -0.1714,  0.9429]])
<a id="__codelineno-520-7" name="__codelineno-520-7" href="#__codelineno-520-7"></a>&gt;&gt;&gt; r
<a id="__codelineno-520-8" name="__codelineno-520-8" href="#__codelineno-520-8"></a>tensor([[ -14.0000,  -21.0000,   14.0000],
<a id="__codelineno-520-9" name="__codelineno-520-9" href="#__codelineno-520-9"></a>        [   0.0000, -175.0000,   70.0000],
<a id="__codelineno-520-10" name="__codelineno-520-10" href="#__codelineno-520-10"></a>        [   0.0000,    0.0000,  -35.0000]])
<a id="__codelineno-520-11" name="__codelineno-520-11" href="#__codelineno-520-11"></a>&gt;&gt;&gt; torch.mm(q, r).round()
<a id="__codelineno-520-12" name="__codelineno-520-12" href="#__codelineno-520-12"></a>tensor([[  12.,  -51.,    4.],
<a id="__codelineno-520-13" name="__codelineno-520-13" href="#__codelineno-520-13"></a>        [   6.,  167.,  -68.],
<a id="__codelineno-520-14" name="__codelineno-520-14" href="#__codelineno-520-14"></a>        [  -4.,   24.,  -41.]])
<a id="__codelineno-520-15" name="__codelineno-520-15" href="#__codelineno-520-15"></a>&gt;&gt;&gt; torch.mm(q.t(), q).round()
<a id="__codelineno-520-16" name="__codelineno-520-16" href="#__codelineno-520-16"></a>tensor([[ 1.,  0.,  0.],
<a id="__codelineno-520-17" name="__codelineno-520-17" href="#__codelineno-520-17"></a>        [ 0.,  1., -0.],
<a id="__codelineno-520-18" name="__codelineno-520-18" href="#__codelineno-520-18"></a>        [ 0., -0.,  1.]])
<a id="__codelineno-520-19" name="__codelineno-520-19" href="#__codelineno-520-19"></a>&gt;&gt;&gt; a = torch.randn(3, 4, 5)
<a id="__codelineno-520-20" name="__codelineno-520-20" href="#__codelineno-520-20"></a>&gt;&gt;&gt; q, r = torch.qr(a, some=False)
<a id="__codelineno-520-21" name="__codelineno-520-21" href="#__codelineno-520-21"></a>&gt;&gt;&gt; torch.allclose(torch.matmul(q, r), a)
<a id="__codelineno-520-22" name="__codelineno-520-22" href="#__codelineno-520-22"></a>True
<a id="__codelineno-520-23" name="__codelineno-520-23" href="#__codelineno-520-23"></a>&gt;&gt;&gt; torch.allclose(torch.matmul(q.transpose(-2, -1), q), torch.eye(5))
<a id="__codelineno-520-24" name="__codelineno-520-24" href="#__codelineno-520-24"></a>True
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-521-1" name="__codelineno-521-1" href="#__codelineno-521-1"></a>torch.solve(input, A, out=None) -&gt; (Tensor, Tensor)¶
</code></pre></div>
<p>此函数将求解返回到由<img alt="" src="../img/d8a7525921c25fbc9d6af3a53d546be8.jpg" />表示的线性方程组和 A 的 LU 分解，以便将其作为命名元&lt;cite&gt;解决方案 LU&lt;/cite&gt; 。</p>
<p>&lt;cite&gt;LU&lt;/cite&gt; 包含 &lt;cite&gt;L&lt;/cite&gt; 和 &lt;cite&gt;U&lt;/cite&gt; 因素，用于 &lt;cite&gt;A&lt;/cite&gt; 的 LU 分解。</p>
<p>&lt;cite&gt;torch.solve(B，A）&lt;/cite&gt;可以接受 2D 输入 &lt;cite&gt;B，A&lt;/cite&gt; 或一批 2D 矩阵的输入。 如果输入是批次，则返回批次输出&lt;cite&gt;解决方案 LU&lt;/cite&gt; 。</p>
<p>Note</p>
<p>不管原始步幅如何，返回的矩阵&lt;cite&gt;解决方案&lt;/cite&gt;和 &lt;cite&gt;LU&lt;/cite&gt; 都将转置，即，步幅类似 &lt;cite&gt;B.contiguous(）。transpose(-1，-2）。 stride(）&lt;/cite&gt;和 &lt;cite&gt;A.contiguous(）。transpose(-1，-2）.stride(）&lt;/cite&gt;。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>输入</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–大小为<img alt="" src="../img/8ebb9d35b764041761ddb4e3436e265d.jpg" />的输入矩阵<img alt="" src="../img/041cf842180f19a622690f37ed9f70d2.jpg" />，其中<img alt="" src="../img/dcef98688866c0d5a21137cf53bf228d.jpg" />为零或更大的批次尺寸。</p>
</li>
<li>
<p><strong>A</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–大小为<img alt="" src="../img/10ea424983a008dbf72dc86628a2813d.jpg" />的输入方阵，其中<img alt="" src="../img/dcef98688866c0d5a21137cf53bf228d.jpg" />为零或更大的批处理尺寸。</p>
</li>
<li>
<p><strong>输出</strong>(<em>(</em> <a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a> <em>，</em> <a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a> <em>）</em> <em>，</em> <em>可选</em>）–可选输出元组。</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-522-1" name="__codelineno-522-1" href="#__codelineno-522-1"></a>&gt;&gt;&gt; A = torch.tensor([[6.80, -2.11,  5.66,  5.97,  8.23],
<a id="__codelineno-522-2" name="__codelineno-522-2" href="#__codelineno-522-2"></a>                      [-6.05, -3.30,  5.36, -4.44,  1.08],
<a id="__codelineno-522-3" name="__codelineno-522-3" href="#__codelineno-522-3"></a>                      [-0.45,  2.58, -2.70,  0.27,  9.04],
<a id="__codelineno-522-4" name="__codelineno-522-4" href="#__codelineno-522-4"></a>                      [8.32,  2.71,  4.35,  -7.17,  2.14],
<a id="__codelineno-522-5" name="__codelineno-522-5" href="#__codelineno-522-5"></a>                      [-9.67, -5.14, -7.26,  6.08, -6.87]]).t()
<a id="__codelineno-522-6" name="__codelineno-522-6" href="#__codelineno-522-6"></a>&gt;&gt;&gt; B = torch.tensor([[4.02,  6.19, -8.22, -7.57, -3.03],
<a id="__codelineno-522-7" name="__codelineno-522-7" href="#__codelineno-522-7"></a>                      [-1.56,  4.00, -8.67,  1.75,  2.86],
<a id="__codelineno-522-8" name="__codelineno-522-8" href="#__codelineno-522-8"></a>                      [9.81, -4.09, -4.57, -8.61,  8.99]]).t()
<a id="__codelineno-522-9" name="__codelineno-522-9" href="#__codelineno-522-9"></a>&gt;&gt;&gt; X, LU = torch.solve(B, A)
<a id="__codelineno-522-10" name="__codelineno-522-10" href="#__codelineno-522-10"></a>&gt;&gt;&gt; torch.dist(B, torch.mm(A, X))
<a id="__codelineno-522-11" name="__codelineno-522-11" href="#__codelineno-522-11"></a>tensor(1.00000e-06 *
<a id="__codelineno-522-12" name="__codelineno-522-12" href="#__codelineno-522-12"></a>       7.0977)
<a id="__codelineno-522-13" name="__codelineno-522-13" href="#__codelineno-522-13"></a>
<a id="__codelineno-522-14" name="__codelineno-522-14" href="#__codelineno-522-14"></a>&gt;&gt;&gt; # Batched solver example
<a id="__codelineno-522-15" name="__codelineno-522-15" href="#__codelineno-522-15"></a>&gt;&gt;&gt; A = torch.randn(2, 3, 1, 4, 4)
<a id="__codelineno-522-16" name="__codelineno-522-16" href="#__codelineno-522-16"></a>&gt;&gt;&gt; B = torch.randn(2, 3, 1, 4, 6)
<a id="__codelineno-522-17" name="__codelineno-522-17" href="#__codelineno-522-17"></a>&gt;&gt;&gt; X, LU = torch.solve(B, A)
<a id="__codelineno-522-18" name="__codelineno-522-18" href="#__codelineno-522-18"></a>&gt;&gt;&gt; torch.dist(B, A.matmul(X))
<a id="__codelineno-522-19" name="__codelineno-522-19" href="#__codelineno-522-19"></a>tensor(1.00000e-06 *
<a id="__codelineno-522-20" name="__codelineno-522-20" href="#__codelineno-522-20"></a>   3.6386)
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-523-1" name="__codelineno-523-1" href="#__codelineno-523-1"></a>torch.svd(input, some=True, compute_uv=True, out=None) -&gt; (Tensor, Tensor, Tensor)¶
</code></pre></div>
<p>该函数返回一个命名元组<code>(U, S, V)</code>，它是输入实数矩阵或一批实数矩阵<code>input</code>这样<img alt="" src="../img/2a4f9c13d7085a30a536b8751c0e3e4c.jpg" />的奇异值分解。</p>
<p>如果<code>some</code>为<code>True</code>(默认值），则该方法返回简化后的奇异值分解，即，如果<code>input</code>的最后两个维为<code>m</code>和<code>n</code>，则返回 &lt;cite&gt;U&lt;/cite&gt; 和 &lt;cite&gt;V&lt;/cite&gt; 矩阵将仅包含<img alt="" src="../img/cbc4cab078465d304683e6b32e90ce2e.jpg" />正交列。</p>
<p>如果<code>compute_uv</code>为<code>False</code>，则返回的 &lt;cite&gt;U&lt;/cite&gt; 和 &lt;cite&gt;V&lt;/cite&gt; 矩阵将分别为形状为<img alt="" src="../img/b9c3d9c79c4cd656bf4c904908b8c189.jpg" />和<img alt="" src="../img/c64d7c7677227e8b59da7d7cfb2466d8.jpg" />的零矩阵。 <code>some</code>在这里将被忽略。</p>
<p>Note</p>
<p>奇异值以降序返回。 如果<code>input</code>是一批矩阵，则该批中每个矩阵的奇异值将按降序返回。</p>
<p>Note</p>
<p>SVD 在 CPU 上的实现使用 LAPACK 例程&lt;cite&gt;？gesdd&lt;/cite&gt; (分治算法）代替&lt;cite&gt;？gesvd&lt;/cite&gt; 来提高速度。 类似地，GPU 上的 SVD 也使用 MAGMA 例程 &lt;cite&gt;gesdd&lt;/cite&gt; 。</p>
<p>Note</p>
<p>无论原始步幅如何，返回的矩阵 &lt;cite&gt;U&lt;/cite&gt; 都将转置，即步幅为<code>U.contiguous().transpose(-2, -1).stride()</code></p>
<p>Note</p>
<p>向后通过 &lt;cite&gt;U&lt;/cite&gt; 和 &lt;cite&gt;V&lt;/cite&gt; 输出时，需要格外小心。 仅当<code>input</code>具有所有不同的奇异值的完整等级时，此类操作才真正稳定。 否则，由于未正确定义渐变，可能会出现<code>NaN</code>。 另外，请注意，即使原始后退仅出现在 &lt;cite&gt;S&lt;/cite&gt; 上，两次后退通常也会通过 &lt;cite&gt;U&lt;/cite&gt; 和 &lt;cite&gt;V&lt;/cite&gt; 进行额外的后退。</p>
<p>Note</p>
<p>当<code>some</code> = <code>False</code>时，<code>U[..., :, min(m, n):]</code>和<code>V[..., :, min(m, n):]</code>上的梯度将向后忽略，因为这些向量可以是子空间的任意基。</p>
<p>Note</p>
<p>当<code>compute_uv</code> = <code>False</code>时，由于向后操作需要来自正向的 &lt;cite&gt;U&lt;/cite&gt; 和 &lt;cite&gt;V&lt;/cite&gt; ，因此无法执行反向。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>输入</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–大小为<img alt="" src="../img/6217ff594f4af540b27d0cc551ab742c.jpg" />的输入张量，其中 &lt;cite&gt;*&lt;/cite&gt; 是零个或多个由<img alt="" src="../img/5324b7a0d3b2c1767998bc499b6dfb11.jpg" />组成的批量 矩阵。</p>
</li>
<li>
<p><strong>一些</strong> (<em>bool</em> <em>，</em> <em>可选</em>）–控制返回的 &lt;cite&gt;U&lt;/cite&gt; 和 &lt;cite&gt;V&lt;/cite&gt;</p>
</li>
<li>
<p><strong>compute_uv</strong>  (<em>bool</em> <em>，</em> <em>可选</em>）–选择是否计算 &lt;cite&gt;U&lt;/cite&gt; 和 &lt;cite&gt;V&lt;/cite&gt; 或不</p>
</li>
<li>
<p><strong>out</strong> (<em>元组</em> <em>，</em> <em>可选</em>）–张量的输出元组</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-524-1" name="__codelineno-524-1" href="#__codelineno-524-1"></a>&gt;&gt;&gt; a = torch.randn(5, 3)
<a id="__codelineno-524-2" name="__codelineno-524-2" href="#__codelineno-524-2"></a>&gt;&gt;&gt; a
<a id="__codelineno-524-3" name="__codelineno-524-3" href="#__codelineno-524-3"></a>tensor([[ 0.2364, -0.7752,  0.6372],
<a id="__codelineno-524-4" name="__codelineno-524-4" href="#__codelineno-524-4"></a>        [ 1.7201,  0.7394, -0.0504],
<a id="__codelineno-524-5" name="__codelineno-524-5" href="#__codelineno-524-5"></a>        [-0.3371, -1.0584,  0.5296],
<a id="__codelineno-524-6" name="__codelineno-524-6" href="#__codelineno-524-6"></a>        [ 0.3550, -0.4022,  1.5569],
<a id="__codelineno-524-7" name="__codelineno-524-7" href="#__codelineno-524-7"></a>        [ 0.2445, -0.0158,  1.1414]])
<a id="__codelineno-524-8" name="__codelineno-524-8" href="#__codelineno-524-8"></a>&gt;&gt;&gt; u, s, v = torch.svd(a)
<a id="__codelineno-524-9" name="__codelineno-524-9" href="#__codelineno-524-9"></a>&gt;&gt;&gt; u
<a id="__codelineno-524-10" name="__codelineno-524-10" href="#__codelineno-524-10"></a>tensor([[ 0.4027,  0.0287,  0.5434],
<a id="__codelineno-524-11" name="__codelineno-524-11" href="#__codelineno-524-11"></a>        [-0.1946,  0.8833,  0.3679],
<a id="__codelineno-524-12" name="__codelineno-524-12" href="#__codelineno-524-12"></a>        [ 0.4296, -0.2890,  0.5261],
<a id="__codelineno-524-13" name="__codelineno-524-13" href="#__codelineno-524-13"></a>        [ 0.6604,  0.2717, -0.2618],
<a id="__codelineno-524-14" name="__codelineno-524-14" href="#__codelineno-524-14"></a>        [ 0.4234,  0.2481, -0.4733]])
<a id="__codelineno-524-15" name="__codelineno-524-15" href="#__codelineno-524-15"></a>&gt;&gt;&gt; s
<a id="__codelineno-524-16" name="__codelineno-524-16" href="#__codelineno-524-16"></a>tensor([2.3289, 2.0315, 0.7806])
<a id="__codelineno-524-17" name="__codelineno-524-17" href="#__codelineno-524-17"></a>&gt;&gt;&gt; v
<a id="__codelineno-524-18" name="__codelineno-524-18" href="#__codelineno-524-18"></a>tensor([[-0.0199,  0.8766,  0.4809],
<a id="__codelineno-524-19" name="__codelineno-524-19" href="#__codelineno-524-19"></a>        [-0.5080,  0.4054, -0.7600],
<a id="__codelineno-524-20" name="__codelineno-524-20" href="#__codelineno-524-20"></a>        [ 0.8611,  0.2594, -0.4373]])
<a id="__codelineno-524-21" name="__codelineno-524-21" href="#__codelineno-524-21"></a>&gt;&gt;&gt; torch.dist(a, torch.mm(torch.mm(u, torch.diag(s)), v.t()))
<a id="__codelineno-524-22" name="__codelineno-524-22" href="#__codelineno-524-22"></a>tensor(8.6531e-07)
<a id="__codelineno-524-23" name="__codelineno-524-23" href="#__codelineno-524-23"></a>&gt;&gt;&gt; a_big = torch.randn(7, 5, 3)
<a id="__codelineno-524-24" name="__codelineno-524-24" href="#__codelineno-524-24"></a>&gt;&gt;&gt; u, s, v = torch.svd(a_big)
<a id="__codelineno-524-25" name="__codelineno-524-25" href="#__codelineno-524-25"></a>&gt;&gt;&gt; torch.dist(a_big, torch.matmul(torch.matmul(u, torch.diag_embed(s)), v.transpose(-2, -1)))
<a id="__codelineno-524-26" name="__codelineno-524-26" href="#__codelineno-524-26"></a>tensor(2.6503e-06)
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-525-1" name="__codelineno-525-1" href="#__codelineno-525-1"></a>torch.symeig(input, eigenvectors=False, upper=True, out=None) -&gt; (Tensor, Tensor)¶
</code></pre></div>
<p>此函数返回实数对称矩阵<code>input</code>或一批实数对称矩阵的特征值和特征向量，由一个命名元组(特征值，特征向量）表示。</p>
<p>此函数计算<code>input</code>的所有特征值(和向量），使得<img alt="" src="../img/cb16e8778bbe7f58582093ac9fd7dd36.jpg" />。</p>
<p>布尔参数<code>eigenvectors</code>定义特征向量和特征值或仅特征值的计算。</p>
<p>如果为<code>False</code>，则仅计算特征值。 如果为<code>True</code>，则同时计算特征值和特征向量。</p>
<p>由于假定输入矩阵<code>input</code>是对称的，因此默认情况下仅使用上三角部分。</p>
<p>如果<code>upper</code>为<code>False</code>，则使用下部三角形部分。</p>
<p>Note</p>
<p>特征值以升序返回。 如果<code>input</code>是一批矩阵，则该批中每个矩阵的特征值将以升序返回。</p>
<p>Note</p>
<p>无论原始步幅如何，返回的矩阵 &lt;cite&gt;V&lt;/cite&gt; 都将转置，即使用步幅 &lt;cite&gt;V.contiguous(）。transpose(-1，-2）.stride(）&lt;/cite&gt;。</p>
<p>Note</p>
<p>向后通过输出时，需要格外小心。 只有当所有特征值都不同时，这种操作才真正稳定。 否则，可能会出现<code>NaN</code>，因为未正确定义渐变。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>输入</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–大小为<img alt="" src="../img/aa6a866e7977a9ee67a53687003d3821.jpg" />的输入张量，其中 &lt;cite&gt;*&lt;/cite&gt; 为零或更多由对称矩阵组成的批处理尺寸。</p>
</li>
<li>
<p><strong>特征向量</strong>(<em>布尔</em> <em>，</em> <em>可选</em>）–控制是否必须计算特征向量</p>
</li>
<li>
<p><strong>上部</strong>(<em>布尔</em> <em>，</em> <em>可选</em>）–控制是考虑上三角区域还是下三角区域</p>
</li>
<li>
<p><strong>out</strong> (<em>tuple__,</em> <em>optional</em>) – the output tuple of (Tensor, Tensor)</p>
</li>
</ul>
<p>Returns</p>
<p>A namedtuple (eigenvalues, eigenvectors) containing</p>
<blockquote>
<ul>
<li>
<p><strong>特征值</strong>(<em>tensor</em>）：形状<img alt="" src="../img/74dd7138867888df79f198ead03a8f07.jpg" />。 特征值按升序排列。</p>
</li>
<li>
<p><strong>特征向量</strong>(<em>tensor</em>）：形状<img alt="" src="../img/10ea424983a008dbf72dc86628a2813d.jpg" />。 如果<code>eigenvectors=False</code>，则为张量为空。 否则，该张量包含<code>input</code>的正交特征向量。</p>
</li>
</ul>
</blockquote>
<p>Return type</p>
<p>(<a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a>, <a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a>)</p>
<p>Examples:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-526-1" name="__codelineno-526-1" href="#__codelineno-526-1"></a>&gt;&gt;&gt; a = torch.randn(5, 5)
<a id="__codelineno-526-2" name="__codelineno-526-2" href="#__codelineno-526-2"></a>&gt;&gt;&gt; a = a + a.t()  # To make a symmetric
<a id="__codelineno-526-3" name="__codelineno-526-3" href="#__codelineno-526-3"></a>&gt;&gt;&gt; a
<a id="__codelineno-526-4" name="__codelineno-526-4" href="#__codelineno-526-4"></a>tensor([[-5.7827,  4.4559, -0.2344, -1.7123, -1.8330],
<a id="__codelineno-526-5" name="__codelineno-526-5" href="#__codelineno-526-5"></a>        [ 4.4559,  1.4250, -2.8636, -3.2100, -0.1798],
<a id="__codelineno-526-6" name="__codelineno-526-6" href="#__codelineno-526-6"></a>        [-0.2344, -2.8636,  1.7112, -5.5785,  7.1988],
<a id="__codelineno-526-7" name="__codelineno-526-7" href="#__codelineno-526-7"></a>        [-1.7123, -3.2100, -5.5785, -2.6227,  3.1036],
<a id="__codelineno-526-8" name="__codelineno-526-8" href="#__codelineno-526-8"></a>        [-1.8330, -0.1798,  7.1988,  3.1036, -5.1453]])
<a id="__codelineno-526-9" name="__codelineno-526-9" href="#__codelineno-526-9"></a>&gt;&gt;&gt; e, v = torch.symeig(a, eigenvectors=True)
<a id="__codelineno-526-10" name="__codelineno-526-10" href="#__codelineno-526-10"></a>&gt;&gt;&gt; e
<a id="__codelineno-526-11" name="__codelineno-526-11" href="#__codelineno-526-11"></a>tensor([-13.7012,  -7.7497,  -2.3163,   5.2477,   8.1050])
<a id="__codelineno-526-12" name="__codelineno-526-12" href="#__codelineno-526-12"></a>&gt;&gt;&gt; v
<a id="__codelineno-526-13" name="__codelineno-526-13" href="#__codelineno-526-13"></a>tensor([[ 0.1643,  0.9034, -0.0291,  0.3508,  0.1817],
<a id="__codelineno-526-14" name="__codelineno-526-14" href="#__codelineno-526-14"></a>        [-0.2417, -0.3071, -0.5081,  0.6534,  0.4026],
<a id="__codelineno-526-15" name="__codelineno-526-15" href="#__codelineno-526-15"></a>        [-0.5176,  0.1223, -0.0220,  0.3295, -0.7798],
<a id="__codelineno-526-16" name="__codelineno-526-16" href="#__codelineno-526-16"></a>        [-0.4850,  0.2695, -0.5773, -0.5840,  0.1337],
<a id="__codelineno-526-17" name="__codelineno-526-17" href="#__codelineno-526-17"></a>        [ 0.6415, -0.0447, -0.6381, -0.0193, -0.4230]])
<a id="__codelineno-526-18" name="__codelineno-526-18" href="#__codelineno-526-18"></a>&gt;&gt;&gt; a_big = torch.randn(5, 2, 2)
<a id="__codelineno-526-19" name="__codelineno-526-19" href="#__codelineno-526-19"></a>&gt;&gt;&gt; a_big = a_big + a_big.transpose(-2, -1)  # To make a_big symmetric
<a id="__codelineno-526-20" name="__codelineno-526-20" href="#__codelineno-526-20"></a>&gt;&gt;&gt; e, v = a_big.symeig(eigenvectors=True)
<a id="__codelineno-526-21" name="__codelineno-526-21" href="#__codelineno-526-21"></a>&gt;&gt;&gt; torch.allclose(torch.matmul(v, torch.matmul(e.diag_embed(), v.transpose(-2, -1))), a_big)
<a id="__codelineno-526-22" name="__codelineno-526-22" href="#__codelineno-526-22"></a>True
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-527-1" name="__codelineno-527-1" href="#__codelineno-527-1"></a>torch.trapz()¶
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-528-1" name="__codelineno-528-1" href="#__codelineno-528-1"></a>torch.trapz(y, x, *, dim=-1) → Tensor
</code></pre></div>
<p>使用梯形法则估计&lt;cite&gt;暗&lt;/cite&gt;的<img alt="" src="../img/610dd7b4997e45b33ba79e39c75e1b9d.jpg" />。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>y</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–积分函数的值</p>
</li>
<li>
<p><strong>x</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–函数 &lt;cite&gt;y&lt;/cite&gt; 的采样点。 如果 &lt;cite&gt;x&lt;/cite&gt; 不按升序排列，则其减小的时间间隔将对估计的积分产生负面影响(即遵循惯例<img alt="" src="../img/7368567bca3b8caffef0c7639ae1ebd5.jpg" />）。</p>
</li>
<li>
<p><strong>暗淡的</strong> (<em>python：int</em> )–集成所沿的维度。 默认情况下，使用最后一个尺寸。</p>
</li>
</ul>
<p>Returns</p>
<p>一个与输入形状相同的张量，除了删除了&lt;cite&gt;暗淡的&lt;/cite&gt;。 返回的张量的每个元素代表沿着&lt;cite&gt;暗淡&lt;/cite&gt;的估计积分<img alt="" src="../img/610dd7b4997e45b33ba79e39c75e1b9d.jpg" />。</p>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-529-1" name="__codelineno-529-1" href="#__codelineno-529-1"></a>&gt;&gt;&gt; y = torch.randn((2, 3))
<a id="__codelineno-529-2" name="__codelineno-529-2" href="#__codelineno-529-2"></a>&gt;&gt;&gt; y
<a id="__codelineno-529-3" name="__codelineno-529-3" href="#__codelineno-529-3"></a>tensor([[-2.1156,  0.6857, -0.2700],
<a id="__codelineno-529-4" name="__codelineno-529-4" href="#__codelineno-529-4"></a>        [-1.2145,  0.5540,  2.0431]])
<a id="__codelineno-529-5" name="__codelineno-529-5" href="#__codelineno-529-5"></a>&gt;&gt;&gt; x = torch.tensor([[1, 3, 4], [1, 2, 3]])
<a id="__codelineno-529-6" name="__codelineno-529-6" href="#__codelineno-529-6"></a>&gt;&gt;&gt; torch.trapz(y, x)
<a id="__codelineno-529-7" name="__codelineno-529-7" href="#__codelineno-529-7"></a>tensor([-1.2220,  0.9683])
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-530-1" name="__codelineno-530-1" href="#__codelineno-530-1"></a>torch.trapz(y, *, dx=1, dim=-1) → Tensor
</code></pre></div>
<p>如上所述，但是采样点以 &lt;cite&gt;dx&lt;/cite&gt; 的距离均匀间隔。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>y</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – The values of the function to integrate</p>
</li>
<li>
<p><strong>dx</strong>  (<em>python：float</em> )–采样 &lt;cite&gt;y&lt;/cite&gt; 的点之间的距离。</p>
</li>
<li>
<p><strong>dim</strong> (<em>python:int</em>) – The dimension along which to integrate. By default, use the last dimension.</p>
</li>
</ul>
<p>Returns</p>
<p>A Tensor with the same shape as the input, except with &lt;cite&gt;dim&lt;/cite&gt; removed. Each element of the returned tensor represents the estimated integral <img alt="" src="../img/610dd7b4997e45b33ba79e39c75e1b9d.jpg" /> along &lt;cite&gt;dim&lt;/cite&gt;.</p>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-531-1" name="__codelineno-531-1" href="#__codelineno-531-1"></a>torch.triangular_solve(input, A, upper=True, transpose=False, unitriangular=False) -&gt; (Tensor, Tensor)¶
</code></pre></div>
<p>用三角系数矩阵<img alt="" src="../img/8cab287af6acaf0838d1d67381a3716d.jpg" />和多个右侧<img alt="" src="../img/23a2cc697e4e1a8adeb60eb42ab51a6e.jpg" />求解方程组。</p>
<p>特别是，求解<img alt="" src="../img/998c4e0ada5e41efb8c632f644ba86f1.jpg" />并假定<img alt="" src="../img/8cab287af6acaf0838d1d67381a3716d.jpg" />为带有默认关键字参数的上三角。</p>
<p>&lt;cite&gt;torch.triangular_solve(b，A）&lt;/cite&gt;可以接受 2D 输入 &lt;cite&gt;b，A&lt;/cite&gt; 或一批 2D 矩阵的输入。 如果输入为批次，则返回成批输出 &lt;cite&gt;X&lt;/cite&gt;</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>输入</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–尺寸为<img alt="" src="../img/8ebb9d35b764041761ddb4e3436e265d.jpg" />的多个右侧，其中<img alt="" src="../img/dcef98688866c0d5a21137cf53bf228d.jpg" />是零个以上的批次尺寸(<img alt="" src="../img/23a2cc697e4e1a8adeb60eb42ab51a6e.jpg" />）</p>
</li>
<li>
<p><strong>A</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–大小为<img alt="" src="../img/10ea424983a008dbf72dc86628a2813d.jpg" />的输入三角系数矩阵，其中<img alt="" src="../img/dcef98688866c0d5a21137cf53bf228d.jpg" />为零或更大的批处理尺寸</p>
</li>
<li>
<p><strong>上</strong> (<em>bool</em> <em>，</em> <em>可选</em>）–是求解方程的上三角系统(默认）还是下三角系统 方程组。 默认值：<code>True</code>。</p>
</li>
<li>
<p><strong>换位</strong>(<em>布尔</em> <em>，</em> <em>可选</em>）–在将<img alt="" src="../img/8cab287af6acaf0838d1d67381a3716d.jpg" />发送到求解器之前是否应该对其进行换位。 默认值：<code>False</code>。</p>
</li>
<li>
<p><strong>单边形</strong>(<em>布尔</em> <em>，</em> <em>可选</em>）– <img alt="" src="../img/8cab287af6acaf0838d1d67381a3716d.jpg" />是否为单位三角形。 如果为 True，则假定<img alt="" src="../img/8cab287af6acaf0838d1d67381a3716d.jpg" />的对角元素为 1，并且未从<img alt="" src="../img/8cab287af6acaf0838d1d67381a3716d.jpg" />引用。 默认值：<code>False</code>。</p>
</li>
</ul>
<p>Returns</p>
<p>一个命名元组&lt;cite&gt;(解决方案，cloned_coefficient）&lt;/cite&gt;其中 &lt;cite&gt;cloned_coefficient&lt;/cite&gt; 是<img alt="" src="../img/8cab287af6acaf0838d1d67381a3716d.jpg" />的克隆，而&lt;cite&gt;解决方案&lt;/cite&gt;是<img alt="" src="../img/1dc567019b272fda0c5051c472dac2b7.jpg" />到<img alt="" src="../img/998c4e0ada5e41efb8c632f644ba86f1.jpg" />的解决方案(或其他变体） 的方程组，具体取决于关键字参数。）</p>
<p>Examples:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-532-1" name="__codelineno-532-1" href="#__codelineno-532-1"></a>&gt;&gt;&gt; A = torch.randn(2, 2).triu()
<a id="__codelineno-532-2" name="__codelineno-532-2" href="#__codelineno-532-2"></a>&gt;&gt;&gt; A
<a id="__codelineno-532-3" name="__codelineno-532-3" href="#__codelineno-532-3"></a>tensor([[ 1.1527, -1.0753],
<a id="__codelineno-532-4" name="__codelineno-532-4" href="#__codelineno-532-4"></a>        [ 0.0000,  0.7986]])
<a id="__codelineno-532-5" name="__codelineno-532-5" href="#__codelineno-532-5"></a>&gt;&gt;&gt; b = torch.randn(2, 3)
<a id="__codelineno-532-6" name="__codelineno-532-6" href="#__codelineno-532-6"></a>&gt;&gt;&gt; b
<a id="__codelineno-532-7" name="__codelineno-532-7" href="#__codelineno-532-7"></a>tensor([[-0.0210,  2.3513, -1.5492],
<a id="__codelineno-532-8" name="__codelineno-532-8" href="#__codelineno-532-8"></a>        [ 1.5429,  0.7403, -1.0243]])
<a id="__codelineno-532-9" name="__codelineno-532-9" href="#__codelineno-532-9"></a>&gt;&gt;&gt; torch.triangular_solve(b, A)
<a id="__codelineno-532-10" name="__codelineno-532-10" href="#__codelineno-532-10"></a>torch.return_types.triangular_solve(
<a id="__codelineno-532-11" name="__codelineno-532-11" href="#__codelineno-532-11"></a>solution=tensor([[ 1.7841,  2.9046, -2.5405],
<a id="__codelineno-532-12" name="__codelineno-532-12" href="#__codelineno-532-12"></a>        [ 1.9320,  0.9270, -1.2826]]),
<a id="__codelineno-532-13" name="__codelineno-532-13" href="#__codelineno-532-13"></a>cloned_coefficient=tensor([[ 1.1527, -1.0753],
<a id="__codelineno-532-14" name="__codelineno-532-14" href="#__codelineno-532-14"></a>        [ 0.0000,  0.7986]]))
</code></pre></div>
<h2 id="_17">实用工具</h2>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-533-1" name="__codelineno-533-1" href="#__codelineno-533-1"></a>torch.compiled_with_cxx11_abi()¶
</code></pre></div>
<p>返回 PyTorch 是否使用 _GLIBCXX_USE_CXX11_ABI = 1 构建</p>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-534-1" name="__codelineno-534-1" href="#__codelineno-534-1"></a>torch.result_type(tensor1, tensor2) → dtype¶
</code></pre></div>
<p>返回 <a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a> ，这是对提供的输入张量执行算术运算得出的。 有关类型升级逻辑的更多信息，请参见类型升级<a href="tensor_attributes.html#type-promotion-doc">文档</a>。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>张量 1</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a> <em>或</em> <em>数字</em>）–输入张量或数字</p>
</li>
<li>
<p><strong>张量 2</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a> <em>或</em> <em>数字</em>）–输入张量或数字</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-535-1" name="__codelineno-535-1" href="#__codelineno-535-1"></a>&gt;&gt;&gt; torch.result_type(torch.tensor([1, 2], dtype=torch.int), 1.0)
<a id="__codelineno-535-2" name="__codelineno-535-2" href="#__codelineno-535-2"></a>torch.float32
<a id="__codelineno-535-3" name="__codelineno-535-3" href="#__codelineno-535-3"></a>&gt;&gt;&gt; torch.result_type(torch.tensor([1, 2], dtype=torch.uint8), torch.tensor(1))
<a id="__codelineno-535-4" name="__codelineno-535-4" href="#__codelineno-535-4"></a>torch.uint8
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-536-1" name="__codelineno-536-1" href="#__codelineno-536-1"></a>torch.can_cast(from, to) → bool¶
</code></pre></div>
<p>确定在类型提升<a href="tensor_attributes.html#type-promotion-doc">文档</a>中描述的 PyTorch 转换规则下是否允许类型转换。</p>
<p>Parameters</p>
<ul>
<li>
<p>中的<strong> (<em>dpython：type</em> )–原始的 <a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a> 。</strong></p>
</li>
<li>
<p><strong>到</strong> (<em>dpython：type</em> )–目标 <a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a> 。</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-537-1" name="__codelineno-537-1" href="#__codelineno-537-1"></a>&gt;&gt;&gt; torch.can_cast(torch.double, torch.float)
<a id="__codelineno-537-2" name="__codelineno-537-2" href="#__codelineno-537-2"></a>True
<a id="__codelineno-537-3" name="__codelineno-537-3" href="#__codelineno-537-3"></a>&gt;&gt;&gt; torch.can_cast(torch.float, torch.int)
<a id="__codelineno-537-4" name="__codelineno-537-4" href="#__codelineno-537-4"></a>False
</code></pre></div>
<hr />
<div class="highlight"><pre><span></span><code><a id="__codelineno-538-1" name="__codelineno-538-1" href="#__codelineno-538-1"></a>torch.promote_types(type1, type2) → dtype¶
</code></pre></div>
<p>返回尺寸和标量种类最小的 <a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a> ，其大小不小于 &lt;cite&gt;type1&lt;/cite&gt; 或 &lt;cite&gt;type2&lt;/cite&gt; 。 有关类型升级逻辑的更多信息，请参见类型升级<a href="tensor_attributes.html#type-promotion-doc">文档</a>。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>type1</strong>  (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>)–</p>
</li>
<li>
<p><strong>type2</strong>  (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>)–</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-539-1" name="__codelineno-539-1" href="#__codelineno-539-1"></a>&gt;&gt;&gt; torch.promote_types(torch.int32, torch.float32))
<a id="__codelineno-539-2" name="__codelineno-539-2" href="#__codelineno-539-2"></a>torch.float32
<a id="__codelineno-539-3" name="__codelineno-539-3" href="#__codelineno-539-3"></a>&gt;&gt;&gt; torch.promote_types(torch.uint8, torch.long)
<a id="__codelineno-539-4" name="__codelineno-539-4" href="#__codelineno-539-4"></a>torch.long
</code></pre></div>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
    
      
      <nav class="md-footer__inner md-grid" aria-label="Footer" >
        
          
          <a href="../72/" class="md-footer__link md-footer__link--prev" aria-label="Previous: PyTorch Java API">
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
            </div>
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Previous
              </span>
              <div class="md-ellipsis">
                PyTorch Java API
              </div>
            </div>
          </a>
        
        
          
          <a href="../75/" class="md-footer__link md-footer__link--next" aria-label="Next: torch.nn">
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Next
              </span>
              <div class="md-ellipsis">
                torch.nn
              </div>
            </div>
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4Z"/></svg>
            </div>
          </a>
        
      </nav>
    
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../..", "features": ["content.code.copy", "content.action.edit", "content.action.view", "navigation.footer"], "search": "../../assets/javascripts/workers/search.f886a092.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.d7c377c4.min.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>